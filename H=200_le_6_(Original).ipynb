{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "H=200_le-6 (Original).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "cell_type": "code",
      "source": [
        "!pip install JSAnimation >/dev/null"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "0bc420ce-58bb-4121-ddde-aab401b8aa8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 28.4 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=3f8e2817167fe807c8bdf85e8959fd47f6dbc5ba56db03fe00b5f74e68267098\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "80a5f873-2e90-4d34-b8b5-1b61949bcc97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "06ecf0b2-53d0-43bc-ee22-cfdc64a5ac89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "3735144f-2b04-4244-fecd-8163ccaf5124",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "c92fbca7-0c02-4837-9143-1ae6cba448d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-6\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "19fc26b7-7311-4275-ebd5-6a27bed1b6cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.980199\n",
            "resetting env. episode 7.000000, reward total was -19.000000. running mean: -20.960397\n",
            "resetting env. episode 8.000000, reward total was -18.000000. running mean: -20.930793\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.931485\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.922170\n",
            "resetting env. episode 11.000000, reward total was -19.000000. running mean: -20.902949\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.903919\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.904880\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.905831\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.896773\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.887805\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.888927\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.890038\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.881137\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.882326\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.883503\n",
            "resetting env. episode 22.000000, reward total was -19.000000. running mean: -20.864668\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.866021\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.867361\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.858687\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.860100\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.861499\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.862884\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.854255\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.855713\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.857156\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.848584\n",
            "resetting env. episode 33.000000, reward total was -19.000000. running mean: -20.830098\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.821797\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.823579\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -20.805344\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.797290\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.799317\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.791324\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.783411\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.785577\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.777721\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.779944\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.772144\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.774423\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.766679\n",
            "resetting env. episode 47.000000, reward total was -19.000000. running mean: -20.749012\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.741522\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.744107\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.746666\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.749199\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.751707\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.744190\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.736748\n",
            "resetting env. episode 55.000000, reward total was -20.000000. running mean: -20.729380\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.722087\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.724866\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.717617\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.720441\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.713237\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.706104\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.709043\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.711953\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.714833\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.717685\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.710508\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.713403\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.716269\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.709106\n",
            "resetting env. episode 70.000000, reward total was -19.000000. running mean: -20.692015\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.685095\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.678244\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.681462\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.684647\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.687800\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.690922\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.694013\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.687073\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.690202\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -20.673300\n",
            "resetting env. episode 81.000000, reward total was -19.000000. running mean: -20.656567\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.660002\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.653402\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.656868\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.660299\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.663696\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.667059\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.660388\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.653785\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.657247\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.660674\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.664067\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.667427\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.670753\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.674045\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.677305\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.680532\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.683726\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.686889\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.690020\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.693120\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.686189\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.689327\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.692434\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.685509\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.688654\n",
            "resetting env. episode 107.000000, reward total was -19.000000. running mean: -20.671768\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.665050\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.658399\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.661815\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.665197\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.668545\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.671860\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.675141\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.678390\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.671606\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.664890\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.658241\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.661659\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.665042\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.668392\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.671708\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.664991\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.668341\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.661657\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.665041\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -20.648390\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.641906\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -20.625487\n",
            "resetting env. episode 130.000000, reward total was -18.000000. running mean: -20.599232\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.603240\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.597208\n",
            "resetting env. episode 133.000000, reward total was -15.000000. running mean: -20.541236\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.545823\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.550365\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.554861\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.559313\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.563720\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.568082\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.572402\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.566678\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.561011\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.565401\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.569747\n",
            "resetting env. episode 145.000000, reward total was -19.000000. running mean: -20.554049\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.558509\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.562924\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.567294\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.571621\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.565905\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.570246\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.574544\n",
            "resetting env. episode 153.000000, reward total was -19.000000. running mean: -20.558798\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.563210\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.567578\n",
            "resetting env. episode 156.000000, reward total was -19.000000. running mean: -20.551902\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.546383\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.540920\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.545510\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.550055\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.554555\n",
            "resetting env. episode 162.000000, reward total was -19.000000. running mean: -20.539009\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.543619\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.538183\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.542801\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.547373\n",
            "resetting env. episode 167.000000, reward total was -19.000000. running mean: -20.531899\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.536580\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.531215\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.525902\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.520643\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.525437\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.520183\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.524981\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.529731\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.534434\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.539089\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.543698\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.548261\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.542779\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.547351\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.551877\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.546359\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.550895\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.545386\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.549932\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.554433\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.558889\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.563300\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.557667\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.562090\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.566469\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.560805\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.565196\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.569545\n",
            "resetting env. episode 196.000000, reward total was -18.000000. running mean: -20.543849\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.538411\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.543026\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.537596\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.532220\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.526898\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.531629\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.526313\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.521050\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.525839\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.520581\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.525375\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.530121\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.524820\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.529572\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.534276\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.538933\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.543544\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.548109\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.542627\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.547201\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.541729\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.546312\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.550849\n",
            "resetting env. episode 220.000000, reward total was -19.000000. running mean: -20.535340\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.539987\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.534587\n",
            "resetting env. episode 223.000000, reward total was -19.000000. running mean: -20.519241\n",
            "resetting env. episode 224.000000, reward total was -19.000000. running mean: -20.504049\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.509008\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -20.493918\n",
            "resetting env. episode 227.000000, reward total was -18.000000. running mean: -20.468979\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.474289\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.469546\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.464851\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.470202\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.465500\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.470845\n",
            "resetting env. episode 234.000000, reward total was -19.000000. running mean: -20.456137\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.461575\n",
            "resetting env. episode 236.000000, reward total was -17.000000. running mean: -20.426960\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.432690\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.438363\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.433980\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.439640\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -20.425243\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.430991\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.436681\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.442314\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.447891\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.453412\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.448878\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.454389\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.459845\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.465247\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.470594\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.475889\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.481130\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.486318\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.496541\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.501575\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.496559\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.501594\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.506578\n",
            "resetting env. episode 261.000000, reward total was -18.000000. running mean: -20.481512\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.486697\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.491830\n",
            "resetting env. episode 264.000000, reward total was -19.000000. running mean: -20.476912\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.482143\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.477321\n",
            "resetting env. episode 267.000000, reward total was -19.000000. running mean: -20.462548\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.467922\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.473243\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.468511\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.463826\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.469187\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.474496\n",
            "resetting env. episode 274.000000, reward total was -19.000000. running mean: -20.459751\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.465153\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.460502\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.465897\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.471238\n",
            "resetting env. episode 279.000000, reward total was -19.000000. running mean: -20.456525\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.461960\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.467340\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.472667\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.477940\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.483161\n",
            "resetting env. episode 285.000000, reward total was -19.000000. running mean: -20.468329\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.473646\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.468910\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.464220\n",
            "resetting env. episode 289.000000, reward total was -18.000000. running mean: -20.439578\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.445182\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.450731\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.456223\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.451661\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.457144\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.462573\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.467947\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.473268\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.478535\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.473750\n",
            "resetting env. episode 300.000000, reward total was -17.000000. running mean: -20.439012\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.434622\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.430276\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.435973\n",
            "resetting env. episode 304.000000, reward total was -18.000000. running mean: -20.411613\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.407497\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.403422\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.399388\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.405394\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.411340\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.417227\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.423055\n",
            "resetting env. episode 312.000000, reward total was -18.000000. running mean: -20.398824\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.404836\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.410788\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.406680\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.402613\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.408587\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.414501\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.420356\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.416152\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.411991\n",
            "resetting env. episode 322.000000, reward total was -19.000000. running mean: -20.397871\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.393892\n",
            "resetting env. episode 324.000000, reward total was -19.000000. running mean: -20.379953\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.376154\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.382392\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.378568\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.374783\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.381035\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.387224\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.383352\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.389519\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.385623\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.381767\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.387950\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.384070\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.380229\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.386427\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.392563\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.388637\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.384751\n",
            "resetting env. episode 342.000000, reward total was -19.000000. running mean: -20.370903\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.377194\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.383422\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.389588\n",
            "resetting env. episode 346.000000, reward total was -19.000000. running mean: -20.375692\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.371935\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.378216\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.384434\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.390589\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.396684\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.402717\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.408690\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.404603\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.410557\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.416451\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.422287\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.428064\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.433783\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.439445\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.445051\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.450600\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.446094\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.451633\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.457117\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.462546\n",
            "resetting env. episode 367.000000, reward total was -18.000000. running mean: -20.437920\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.433541\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.439206\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.444814\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.450366\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.445862\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.441403\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.446989\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.452519\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.457994\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.463414\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.468780\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.474092\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.479351\n",
            "resetting env. episode 381.000000, reward total was -19.000000. running mean: -20.464558\n",
            "resetting env. episode 382.000000, reward total was -19.000000. running mean: -20.449912\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.455413\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.460859\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.466250\n",
            "resetting env. episode 386.000000, reward total was -19.000000. running mean: -20.451588\n",
            "resetting env. episode 387.000000, reward total was -19.000000. running mean: -20.437072\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.432701\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.428374\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.434091\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.439750\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.445352\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.440899\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.446490\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.452025\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.457504\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.462929\n",
            "resetting env. episode 398.000000, reward total was -19.000000. running mean: -20.448300\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.443817\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.449379\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.434885\n",
            "resetting env. episode 402.000000, reward total was -19.000000. running mean: -20.420536\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.426331\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.432068\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.437747\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.443369\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.438936\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.434546\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.440201\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.435799\n",
            "resetting env. episode 411.000000, reward total was -19.000000. running mean: -20.421441\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.427227\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.422954\n",
            "resetting env. episode 414.000000, reward total was -20.000000. running mean: -20.418725\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.414538\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.420392\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.416188\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.422026\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.417806\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.423628\n",
            "resetting env. episode 421.000000, reward total was -19.000000. running mean: -20.409392\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.405298\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.411245\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.417132\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.422961\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.418731\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.424544\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.420299\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.416096\n",
            "resetting env. episode 430.000000, reward total was -19.000000. running mean: -20.401935\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.407915\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.413836\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.409698\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.405601\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.401545\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.397529\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.403554\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.409519\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.415423\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.411269\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.417157\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.422985\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.418755\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.424568\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.420322\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.426119\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.431857\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.437539\n",
            "resetting env. episode 449.000000, reward total was -19.000000. running mean: -20.423163\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.428932\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.434643\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.440296\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.445893\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.451434\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.456920\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.442351\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.437927\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.443548\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.439112\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.434721\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -20.420374\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.416170\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.412009\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.407889\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.403810\n",
            "resetting env. episode 466.000000, reward total was -19.000000. running mean: -20.389772\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.395874\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.401915\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.407896\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.413817\n",
            "resetting env. episode 471.000000, reward total was -18.000000. running mean: -20.389679\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.395782\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.391824\n",
            "resetting env. episode 474.000000, reward total was -18.000000. running mean: -20.367906\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.374227\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.380485\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.386680\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.392813\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.388885\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.394996\n",
            "resetting env. episode 481.000000, reward total was -18.000000. running mean: -20.371046\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.377336\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.383562\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.389727\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.395829\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.391871\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.387952\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.394073\n",
            "resetting env. episode 489.000000, reward total was -19.000000. running mean: -20.380132\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.386331\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.392467\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.398543\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.394557\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.390612\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.396706\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.402739\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.408711\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.414624\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.420478\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.426273\n",
            "CPU times: user 24min, sys: 11min 2s, total: 35min 2s\n",
            "Wall time: 18min 2s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "fefbfa4e-9bfa-44ca-8c90-4940127a9d6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 3.000000, reward total was -19.000000. running mean: -19.990000\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -19.990100\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.000199\n",
            "resetting env. episode 6.000000, reward total was -19.000000. running mean: -19.990197\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.000295\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.010292\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.010189\n",
            "resetting env. episode 10.000000, reward total was -19.000000. running mean: -20.000087\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.010086\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.019986\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.029786\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.039488\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.039093\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.048702\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.058215\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.067633\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.076957\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.086187\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.085325\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.094472\n",
            "resetting env. episode 23.000000, reward total was -19.000000. running mean: -20.083527\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.092692\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.101765\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.110747\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.119640\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.118443\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.117259\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.126086\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.124826\n",
            "resetting env. episode 32.000000, reward total was -19.000000. running mean: -20.113577\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.122441\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.131217\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.129905\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.138606\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.137220\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -20.125848\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.134589\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.143243\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.151811\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.150293\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.158790\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.167202\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -20.155530\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.153975\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.162435\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.170810\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.169102\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.177411\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.185637\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.183781\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.181943\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.190124\n",
            "resetting env. episode 55.000000, reward total was -19.000000. running mean: -20.178222\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.186440\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.194576\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.202630\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.200604\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.198598\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.206612\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.204546\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.212500\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.220375\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.228171\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.235890\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.243531\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.251095\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.248584\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.246099\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.253638\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.261101\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.268490\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.265805\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.273147\n",
            "resetting env. episode 76.000000, reward total was -19.000000. running mean: -20.260416\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.267812\n",
            "resetting env. episode 78.000000, reward total was -19.000000. running mean: -20.255134\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.262582\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.269956\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.277257\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.284484\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.291639\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.298723\n",
            "resetting env. episode 85.000000, reward total was -19.000000. running mean: -20.285736\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.282878\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.290050\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.297149\n",
            "resetting env. episode 89.000000, reward total was -18.000000. running mean: -20.274178\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.271436\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.278722\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.275934\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.283175\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.290343\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.297440\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.294465\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.301521\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.308506\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.315420\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.322266\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.329044\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.325753\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.332496\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.339171\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.345779\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.352321\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.358798\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.355210\n",
            "resetting env. episode 109.000000, reward total was -19.000000. running mean: -20.341658\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.348241\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.354759\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.361211\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.357599\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.364023\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.360383\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.356779\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.363211\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.369579\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -20.365883\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.372225\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.378502\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.374717\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.370970\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.377260\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.383488\n",
            "resetting env. episode 126.000000, reward total was -19.000000. running mean: -20.369653\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.375956\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.372197\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.368475\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.374790\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.381042\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.387232\n",
            "resetting env. episode 133.000000, reward total was -19.000000. running mean: -20.373360\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.379626\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.375830\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.382071\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.378251\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.374468\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.380723\n",
            "resetting env. episode 140.000000, reward total was -19.000000. running mean: -20.366916\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.373247\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.379515\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.375719\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.371962\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.378243\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.384460\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.390616\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.396709\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.402742\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.408715\n",
            "resetting env. episode 151.000000, reward total was -19.000000. running mean: -20.394628\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.400682\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.406675\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.412608\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.418482\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.424297\n",
            "resetting env. episode 157.000000, reward total was -19.000000. running mean: -20.410054\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.415954\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.411794\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.417676\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.423499\n",
            "resetting env. episode 162.000000, reward total was -19.000000. running mean: -20.409264\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.415172\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.421020\n",
            "resetting env. episode 165.000000, reward total was -19.000000. running mean: -20.406810\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.402742\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.408714\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.414627\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.420481\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.426276\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.432013\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.437693\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.443316\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.438883\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.444494\n",
            "resetting env. episode 176.000000, reward total was -19.000000. running mean: -20.430049\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.425749\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.431491\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.427176\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.432905\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.428576\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.434290\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.429947\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.435647\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.441291\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.446878\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.452409\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.457885\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.463306\n",
            "resetting env. episode 190.000000, reward total was -19.000000. running mean: -20.448673\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.454187\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.459645\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.465048\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.470398\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.465694\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.471037\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.466326\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.471663\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.466947\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -20.462277\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.467654\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.462978\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -20.448348\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.453865\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.459326\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.464733\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.470085\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.465384\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.460731\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.466123\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.461462\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.456847\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.462279\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.457656\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.453080\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.448549\n",
            "resetting env. episode 217.000000, reward total was -19.000000. running mean: -20.434063\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.429723\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.435425\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.431071\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.436760\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.442393\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.447969\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.453489\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.458954\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.464365\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.469721\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.475024\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.480274\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.485471\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.490616\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.485710\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.480853\n",
            "resetting env. episode 234.000000, reward total was -19.000000. running mean: -20.466044\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.471384\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.466670\n",
            "resetting env. episode 237.000000, reward total was -19.000000. running mean: -20.452004\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.457483\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.462909\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.458280\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.463697\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.469060\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.464369\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.469725\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.465028\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.470378\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.465674\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.471017\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.476307\n",
            "resetting env. episode 250.000000, reward total was -19.000000. running mean: -20.461544\n",
            "resetting env. episode 251.000000, reward total was -17.000000. running mean: -20.426929\n",
            "resetting env. episode 252.000000, reward total was -18.000000. running mean: -20.402659\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.398633\n",
            "resetting env. episode 254.000000, reward total was -19.000000. running mean: -20.384647\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.380800\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.386992\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.393122\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.389191\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.385299\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.391446\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.397532\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.403556\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.399521\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.395525\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.401570\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.407555\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.403479\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.409444\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.415350\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.421196\n",
            "resetting env. episode 271.000000, reward total was -19.000000. running mean: -20.406984\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.402914\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.408885\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.404796\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.410748\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.416641\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.422475\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.428250\n",
            "resetting env. episode 279.000000, reward total was -18.000000. running mean: -20.403967\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.409928\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.415828\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.411670\n",
            "resetting env. episode 283.000000, reward total was -18.000000. running mean: -20.387553\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.393678\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.399741\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.405744\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.401686\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.397669\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.393693\n",
            "resetting env. episode 290.000000, reward total was -17.000000. running mean: -20.359756\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.366158\n",
            "resetting env. episode 292.000000, reward total was -18.000000. running mean: -20.342497\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.349072\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.345581\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.352125\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.348604\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.355118\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.361567\n",
            "resetting env. episode 299.000000, reward total was -18.000000. running mean: -20.337951\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.344571\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.351126\n",
            "resetting env. episode 302.000000, reward total was -19.000000. running mean: -20.337615\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.344238\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.350796\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.357288\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.363715\n",
            "resetting env. episode 307.000000, reward total was -19.000000. running mean: -20.350078\n",
            "resetting env. episode 308.000000, reward total was -17.000000. running mean: -20.316577\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.323411\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.330177\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.326876\n",
            "resetting env. episode 312.000000, reward total was -19.000000. running mean: -20.313607\n",
            "resetting env. episode 313.000000, reward total was -19.000000. running mean: -20.300471\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.307466\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.314391\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.311247\n",
            "resetting env. episode 317.000000, reward total was -19.000000. running mean: -20.298135\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.295154\n",
            "resetting env. episode 319.000000, reward total was -19.000000. running mean: -20.282202\n",
            "resetting env. episode 320.000000, reward total was -18.000000. running mean: -20.259380\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.266786\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.274118\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.261377\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.268763\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.266076\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.273415\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.270681\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.277974\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.275194\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.272442\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.279718\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.286921\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.294052\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.301111\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.308100\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.315019\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.311869\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.308750\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.315663\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.322506\n",
            "resetting env. episode 341.000000, reward total was -19.000000. running mean: -20.309281\n",
            "resetting env. episode 342.000000, reward total was -19.000000. running mean: -20.296188\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.303226\n",
            "resetting env. episode 344.000000, reward total was -19.000000. running mean: -20.290194\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.297292\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.294319\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.301376\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.298362\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.305379\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.302325\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.309301\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.316208\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.313046\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.319916\n",
            "resetting env. episode 355.000000, reward total was -18.000000. running mean: -20.296717\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -20.283750\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.290912\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.298003\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.295023\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.292073\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.289152\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.296260\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.293298\n",
            "resetting env. episode 364.000000, reward total was -18.000000. running mean: -20.270365\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.277661\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.284885\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.292036\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.299115\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.286124\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.293263\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.300330\n",
            "resetting env. episode 372.000000, reward total was -18.000000. running mean: -20.277327\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.284554\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.291708\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.298791\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.305803\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.312745\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.309618\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.316522\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.313356\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.320223\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.317021\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.313850\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.320712\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.327505\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.324230\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.330987\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.337678\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.344301\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.340858\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.347449\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.353975\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.360435\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.366831\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.373162\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.379431\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.385636\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.381780\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.387962\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.394083\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.400142\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.406140\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.412079\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.407958\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.413879\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.409740\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.415642\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.421486\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.427271\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.432998\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.438668\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.434282\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.439939\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.445540\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.451084\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.456573\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.462008\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.467387\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.472714\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.477986\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.473207\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.478475\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.483690\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.488853\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.493964\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.499025\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.504034\n",
            "resetting env. episode 428.000000, reward total was -19.000000. running mean: -20.488994\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.494104\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.499163\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.504172\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.509130\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.514039\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.508898\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.513809\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.518671\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.523484\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.518249\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.523067\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.527836\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.522558\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.517332\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.522159\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.526937\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.531668\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.536351\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.530988\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.535678\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.530321\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.535018\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.529668\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.524371\n",
            "resetting env. episode 453.000000, reward total was -19.000000. running mean: -20.509127\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.514036\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.518896\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.523707\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.518470\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.523285\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.518052\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.522872\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.517643\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.522467\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.527242\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.521969\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.516750\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.521582\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.516366\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.521203\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.525991\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.530731\n",
            "resetting env. episode 471.000000, reward total was -19.000000. running mean: -20.515424\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.510269\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.505167\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.500115\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.495114\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.500163\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.505161\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.510109\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.515008\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.519858\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.524660\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.529413\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.534119\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.528778\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.533490\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.538155\n",
            "resetting env. episode 487.000000, reward total was -19.000000. running mean: -20.522774\n",
            "resetting env. episode 488.000000, reward total was -17.000000. running mean: -20.487546\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.492670\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.487744\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.492866\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.497938\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.502958\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.507929\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.502849\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.497821\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.502843\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.507814\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.502736\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.507709\n",
            "CPU times: user 23min 56s, sys: 10min 57s, total: 34min 54s\n",
            "Wall time: 17min 51s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "34abf295-f87b-4a9d-ab38-0da178ac044a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "2e68a21d-a6c8-4a99-94f7-f7f68fb8ebc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.990297\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.980394\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.980590\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.970784\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.971076\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.971366\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.971652\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.971935\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.972216\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.972494\n",
            "resetting env. episode 15.000000, reward total was -18.000000. running mean: -20.942769\n",
            "resetting env. episode 16.000000, reward total was -17.000000. running mean: -20.903341\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.904308\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.895265\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.896312\n",
            "resetting env. episode 20.000000, reward total was -18.000000. running mean: -20.867349\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.868676\n",
            "resetting env. episode 22.000000, reward total was -19.000000. running mean: -20.849989\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.851489\n",
            "resetting env. episode 24.000000, reward total was -19.000000. running mean: -20.832974\n",
            "resetting env. episode 25.000000, reward total was -18.000000. running mean: -20.804644\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.806598\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.798532\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.800547\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.792541\n",
            "resetting env. episode 30.000000, reward total was -19.000000. running mean: -20.774616\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.776869\n",
            "resetting env. episode 32.000000, reward total was -19.000000. running mean: -20.759101\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.761510\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.763895\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -20.746256\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.738793\n",
            "resetting env. episode 37.000000, reward total was -18.000000. running mean: -20.711405\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.704291\n",
            "resetting env. episode 39.000000, reward total was -19.000000. running mean: -20.687248\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.680376\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.683572\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.676736\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.669969\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.663269\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.666637\n",
            "resetting env. episode 46.000000, reward total was -17.000000. running mean: -20.629970\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.633671\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.637334\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.640960\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.644551\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.648105\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.651624\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.645108\n",
            "resetting env. episode 54.000000, reward total was -19.000000. running mean: -20.628657\n",
            "resetting env. episode 55.000000, reward total was -18.000000. running mean: -20.602370\n",
            "resetting env. episode 56.000000, reward total was -18.000000. running mean: -20.576347\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.570583\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.574877\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.569129\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.573437\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.577703\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.571926\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.576207\n",
            "resetting env. episode 64.000000, reward total was -19.000000. running mean: -20.560445\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.554840\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.559292\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.563699\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.568062\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.572381\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.576657\n",
            "resetting env. episode 71.000000, reward total was -19.000000. running mean: -20.560891\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.555282\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.549729\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.544232\n",
            "resetting env. episode 75.000000, reward total was -18.000000. running mean: -20.518790\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.513602\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.518466\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.523281\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.518048\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -20.502868\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.507839\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.512761\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.517633\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.512457\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.517332\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.522159\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.526937\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.531668\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.536351\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.540988\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.535578\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.540222\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.544820\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.549372\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -20.543878\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.548439\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.552955\n",
            "resetting env. episode 98.000000, reward total was -19.000000. running mean: -20.537425\n",
            "resetting env. episode 99.000000, reward total was -17.000000. running mean: -20.502051\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.497030\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.502060\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.507039\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.501969\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.496949\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.501980\n",
            "resetting env. episode 106.000000, reward total was -19.000000. running mean: -20.486960\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.492090\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.487170\n",
            "resetting env. episode 109.000000, reward total was -19.000000. running mean: -20.472298\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.467575\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.472899\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -20.458170\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.463588\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.468953\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.464263\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.469620\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.474924\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.470175\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.475473\n",
            "resetting env. episode 120.000000, reward total was -17.000000. running mean: -20.440718\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.436311\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.431948\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.437629\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.443252\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.448820\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.454332\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.459788\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.465190\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -20.450539\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -20.436033\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.441673\n",
            "resetting env. episode 132.000000, reward total was -18.000000. running mean: -20.417256\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.413084\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.418953\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.424763\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.430516\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.426210\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.421948\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.427729\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.433452\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.439117\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.444726\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.450279\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.445776\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.451318\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.446805\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.442337\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.447913\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.453434\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.458900\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.464311\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.469668\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.474971\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.470221\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.465519\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.470864\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.476155\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.481394\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.486580\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.481714\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.486897\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.482028\n",
            "resetting env. episode 163.000000, reward total was -19.000000. running mean: -20.467208\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.472536\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.467810\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.473132\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.478401\n",
            "resetting env. episode 168.000000, reward total was -19.000000. running mean: -20.463617\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.458981\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.454391\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.459847\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.455249\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.460696\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.466089\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.461428\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.466814\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.462146\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.467524\n",
            "resetting env. episode 179.000000, reward total was -19.000000. running mean: -20.452849\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.458321\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.463737\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.469100\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.474409\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.469665\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.474968\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.480219\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -20.465416\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.470762\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.476055\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.481294\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.476481\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.481716\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.486899\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.492030\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.497110\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.502139\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.507117\n",
            "resetting env. episode 198.000000, reward total was -18.000000. running mean: -20.482046\n",
            "resetting env. episode 199.000000, reward total was -19.000000. running mean: -20.467226\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.472553\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.477828\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.483050\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.488219\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.493337\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.488404\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.493520\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.498584\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.503599\n",
            "resetting env. episode 209.000000, reward total was -19.000000. running mean: -20.488563\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.493677\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.488740\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.483853\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.479014\n",
            "resetting env. episode 214.000000, reward total was -19.000000. running mean: -20.464224\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.459582\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.464986\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.470336\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.475633\n",
            "resetting env. episode 219.000000, reward total was -19.000000. running mean: -20.460876\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.466268\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.471605\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.476889\n",
            "resetting env. episode 223.000000, reward total was -19.000000. running mean: -20.462120\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.467499\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.472824\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -20.458096\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.463515\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.468880\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.464191\n",
            "resetting env. episode 230.000000, reward total was -18.000000. running mean: -20.439549\n",
            "resetting env. episode 231.000000, reward total was -19.000000. running mean: -20.425153\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.430902\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.426593\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.422327\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.428104\n",
            "resetting env. episode 236.000000, reward total was -18.000000. running mean: -20.403823\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.409784\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.405687\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.411630\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.407513\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.413438\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.419304\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.415111\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.410960\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.416850\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.422682\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.428455\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.424170\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.429929\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.435629\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.431273\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.436960\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.442591\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.448165\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.453683\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.459146\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.464555\n",
            "resetting env. episode 258.000000, reward total was -17.000000. running mean: -20.429909\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.435610\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -20.421254\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.427041\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.432771\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.438443\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.444059\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.439618\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.435222\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.440870\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.446461\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.441997\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.437577\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.443201\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.448769\n",
            "resetting env. episode 273.000000, reward total was -19.000000. running mean: -20.434281\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.439938\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.445539\n",
            "resetting env. episode 276.000000, reward total was -19.000000. running mean: -20.431084\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.426773\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.422505\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.428280\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.423997\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.429757\n",
            "resetting env. episode 282.000000, reward total was -19.000000. running mean: -20.415460\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.411305\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.417192\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.423020\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.428790\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.434502\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.430157\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.425855\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.431597\n",
            "resetting env. episode 291.000000, reward total was -19.000000. running mean: -20.417281\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.423108\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.428877\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.434588\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.430242\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.425940\n",
            "resetting env. episode 297.000000, reward total was -19.000000. running mean: -20.411681\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.417564\n",
            "resetting env. episode 299.000000, reward total was -19.000000. running mean: -20.403388\n",
            "resetting env. episode 300.000000, reward total was -18.000000. running mean: -20.379354\n",
            "resetting env. episode 301.000000, reward total was -18.000000. running mean: -20.355561\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.362005\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.368385\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.364701\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.371054\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.377344\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.383570\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.379734\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.385937\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.392078\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.398157\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.394175\n",
            "resetting env. episode 313.000000, reward total was -17.000000. running mean: -20.360234\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.366631\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.372965\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.379235\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.375443\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.381689\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.387872\n",
            "resetting env. episode 320.000000, reward total was -19.000000. running mean: -20.373993\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.380253\n",
            "resetting env. episode 322.000000, reward total was -19.000000. running mean: -20.366450\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.352786\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.349258\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.355766\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.352208\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.358686\n",
            "resetting env. episode 328.000000, reward total was -19.000000. running mean: -20.345099\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.351648\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.348131\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.354650\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.361104\n",
            "resetting env. episode 333.000000, reward total was -19.000000. running mean: -20.347493\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.344018\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.350578\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.357072\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.353501\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.349966\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.346466\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.353002\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.359472\n",
            "resetting env. episode 342.000000, reward total was -19.000000. running mean: -20.345877\n",
            "resetting env. episode 343.000000, reward total was -19.000000. running mean: -20.332418\n",
            "resetting env. episode 344.000000, reward total was -17.000000. running mean: -20.299094\n",
            "resetting env. episode 345.000000, reward total was -19.000000. running mean: -20.286103\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.293242\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.290310\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.287407\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.294532\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.301587\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.308571\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.305486\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.302431\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.309406\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.316312\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.323149\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.329918\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.336619\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.343252\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.339820\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.346422\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.342957\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.339528\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.346133\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.352671\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.359145\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.365553\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.361898\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.358279\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.364696\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.361049\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.357438\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.353864\n",
            "resetting env. episode 374.000000, reward total was -18.000000. running mean: -20.330325\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.337022\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.343652\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.350215\n",
            "resetting env. episode 378.000000, reward total was -16.000000. running mean: -20.306713\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.313646\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.320510\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.327304\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.334031\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.330691\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.337384\n",
            "resetting env. episode 385.000000, reward total was -20.000000. running mean: -20.334010\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.330670\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.327364\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.334090\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.330749\n",
            "resetting env. episode 390.000000, reward total was -18.000000. running mean: -20.307442\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.304367\n",
            "resetting env. episode 392.000000, reward total was -19.000000. running mean: -20.291323\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.298410\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -20.285426\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.282572\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.279746\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.276949\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.284179\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.291337\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.298424\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.285440\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.292585\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.289660\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.296763\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.293795\n",
            "resetting env. episode 406.000000, reward total was -18.000000. running mean: -20.270857\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.268149\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.275467\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.272713\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.269985\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.277286\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.274513\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.271768\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.279050\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.276259\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.283497\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.290662\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.297755\n",
            "resetting env. episode 419.000000, reward total was -18.000000. running mean: -20.274778\n",
            "resetting env. episode 420.000000, reward total was -19.000000. running mean: -20.262030\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.269410\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.276716\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.273948\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.281209\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.288397\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.295513\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.302558\n",
            "resetting env. episode 428.000000, reward total was -20.000000. running mean: -20.299532\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.296537\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.303571\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.310536\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.317430\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.324256\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.331014\n",
            "resetting env. episode 435.000000, reward total was -18.000000. running mean: -20.307703\n",
            "resetting env. episode 436.000000, reward total was -19.000000. running mean: -20.294626\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.301680\n",
            "resetting env. episode 438.000000, reward total was -17.000000. running mean: -20.268663\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.275977\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.283217\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.290385\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.297481\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.304506\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.311461\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.318346\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.315163\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.322011\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -20.308791\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.305703\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.312646\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.319520\n",
            "resetting env. episode 452.000000, reward total was -19.000000. running mean: -20.306325\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.313261\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.320129\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.326927\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.323658\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.320422\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.317217\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.324045\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.330805\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.337497\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.344122\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.340681\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.347274\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.343801\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.340363\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.336959\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.343590\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.350154\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.346652\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.353186\n",
            "resetting env. episode 472.000000, reward total was -19.000000. running mean: -20.339654\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.346257\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.352795\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.359267\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.355674\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.352117\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.358596\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.355010\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.361460\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.367846\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.374167\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.380425\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.386621\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.382755\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.388927\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.395038\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.401088\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.407077\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.403006\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.398976\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.404986\n",
            "resetting env. episode 493.000000, reward total was -18.000000. running mean: -20.380936\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.387127\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.393256\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.399323\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.395330\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.401377\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.407363\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -20.393289\n",
            "resetting env. episode 501.000000, reward total was -19.000000. running mean: -20.379356\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.385563\n",
            "resetting env. episode 503.000000, reward total was -19.000000. running mean: -20.371707\n",
            "resetting env. episode 504.000000, reward total was -19.000000. running mean: -20.357990\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -20.364410\n",
            "resetting env. episode 506.000000, reward total was -21.000000. running mean: -20.370766\n",
            "resetting env. episode 507.000000, reward total was -21.000000. running mean: -20.377059\n",
            "resetting env. episode 508.000000, reward total was -20.000000. running mean: -20.373288\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.379555\n",
            "resetting env. episode 510.000000, reward total was -20.000000. running mean: -20.375760\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -20.372002\n",
            "resetting env. episode 512.000000, reward total was -20.000000. running mean: -20.368282\n",
            "resetting env. episode 513.000000, reward total was -21.000000. running mean: -20.374599\n",
            "resetting env. episode 514.000000, reward total was -21.000000. running mean: -20.380853\n",
            "resetting env. episode 515.000000, reward total was -18.000000. running mean: -20.357045\n",
            "resetting env. episode 516.000000, reward total was -19.000000. running mean: -20.343474\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.350039\n",
            "resetting env. episode 518.000000, reward total was -20.000000. running mean: -20.346539\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.353074\n",
            "resetting env. episode 520.000000, reward total was -20.000000. running mean: -20.349543\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.356047\n",
            "resetting env. episode 522.000000, reward total was -20.000000. running mean: -20.352487\n",
            "resetting env. episode 523.000000, reward total was -21.000000. running mean: -20.358962\n",
            "resetting env. episode 524.000000, reward total was -21.000000. running mean: -20.365372\n",
            "resetting env. episode 525.000000, reward total was -19.000000. running mean: -20.351719\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -20.358202\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.364620\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.370973\n",
            "resetting env. episode 529.000000, reward total was -20.000000. running mean: -20.367264\n",
            "resetting env. episode 530.000000, reward total was -20.000000. running mean: -20.363591\n",
            "resetting env. episode 531.000000, reward total was -21.000000. running mean: -20.369955\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.376256\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.382493\n",
            "resetting env. episode 534.000000, reward total was -20.000000. running mean: -20.378668\n",
            "resetting env. episode 535.000000, reward total was -21.000000. running mean: -20.384881\n",
            "resetting env. episode 536.000000, reward total was -19.000000. running mean: -20.371033\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.377322\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.383549\n",
            "resetting env. episode 539.000000, reward total was -19.000000. running mean: -20.369713\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.376016\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.382256\n",
            "resetting env. episode 542.000000, reward total was -20.000000. running mean: -20.378434\n",
            "resetting env. episode 543.000000, reward total was -21.000000. running mean: -20.384649\n",
            "resetting env. episode 544.000000, reward total was -19.000000. running mean: -20.370803\n",
            "resetting env. episode 545.000000, reward total was -21.000000. running mean: -20.377095\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -20.383324\n",
            "resetting env. episode 547.000000, reward total was -19.000000. running mean: -20.369491\n",
            "resetting env. episode 548.000000, reward total was -20.000000. running mean: -20.365796\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.372138\n",
            "resetting env. episode 550.000000, reward total was -21.000000. running mean: -20.378416\n",
            "resetting env. episode 551.000000, reward total was -21.000000. running mean: -20.384632\n",
            "resetting env. episode 552.000000, reward total was -20.000000. running mean: -20.380786\n",
            "resetting env. episode 553.000000, reward total was -21.000000. running mean: -20.386978\n",
            "resetting env. episode 554.000000, reward total was -20.000000. running mean: -20.383108\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -20.389277\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.385384\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.391531\n",
            "resetting env. episode 558.000000, reward total was -20.000000. running mean: -20.387615\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.393739\n",
            "resetting env. episode 560.000000, reward total was -21.000000. running mean: -20.399802\n",
            "resetting env. episode 561.000000, reward total was -20.000000. running mean: -20.395804\n",
            "resetting env. episode 562.000000, reward total was -20.000000. running mean: -20.391846\n",
            "resetting env. episode 563.000000, reward total was -19.000000. running mean: -20.377927\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -20.384148\n",
            "resetting env. episode 565.000000, reward total was -20.000000. running mean: -20.380306\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.386503\n",
            "resetting env. episode 567.000000, reward total was -20.000000. running mean: -20.382638\n",
            "resetting env. episode 568.000000, reward total was -21.000000. running mean: -20.388812\n",
            "resetting env. episode 569.000000, reward total was -20.000000. running mean: -20.384924\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.391075\n",
            "resetting env. episode 571.000000, reward total was -21.000000. running mean: -20.397164\n",
            "resetting env. episode 572.000000, reward total was -20.000000. running mean: -20.393192\n",
            "resetting env. episode 573.000000, reward total was -21.000000. running mean: -20.399260\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.405268\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.411215\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -20.417103\n",
            "resetting env. episode 577.000000, reward total was -20.000000. running mean: -20.412932\n",
            "resetting env. episode 578.000000, reward total was -18.000000. running mean: -20.388802\n",
            "resetting env. episode 579.000000, reward total was -20.000000. running mean: -20.384914\n",
            "resetting env. episode 580.000000, reward total was -21.000000. running mean: -20.391065\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -20.397155\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.403183\n",
            "resetting env. episode 583.000000, reward total was -21.000000. running mean: -20.409151\n",
            "resetting env. episode 584.000000, reward total was -20.000000. running mean: -20.405060\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.411009\n",
            "resetting env. episode 586.000000, reward total was -20.000000. running mean: -20.406899\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.412830\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.418702\n",
            "resetting env. episode 589.000000, reward total was -21.000000. running mean: -20.424515\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.430270\n",
            "resetting env. episode 591.000000, reward total was -19.000000. running mean: -20.415967\n",
            "resetting env. episode 592.000000, reward total was -21.000000. running mean: -20.421807\n",
            "resetting env. episode 593.000000, reward total was -20.000000. running mean: -20.417589\n",
            "resetting env. episode 594.000000, reward total was -21.000000. running mean: -20.423413\n",
            "resetting env. episode 595.000000, reward total was -20.000000. running mean: -20.419179\n",
            "resetting env. episode 596.000000, reward total was -21.000000. running mean: -20.424987\n",
            "resetting env. episode 597.000000, reward total was -21.000000. running mean: -20.430738\n",
            "resetting env. episode 598.000000, reward total was -21.000000. running mean: -20.436430\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.442066\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.447645\n",
            "resetting env. episode 601.000000, reward total was -19.000000. running mean: -20.433169\n",
            "resetting env. episode 602.000000, reward total was -20.000000. running mean: -20.428837\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -20.434549\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.440203\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.445801\n",
            "resetting env. episode 606.000000, reward total was -21.000000. running mean: -20.451343\n",
            "resetting env. episode 607.000000, reward total was -20.000000. running mean: -20.446830\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -20.452361\n",
            "resetting env. episode 609.000000, reward total was -21.000000. running mean: -20.457838\n",
            "resetting env. episode 610.000000, reward total was -20.000000. running mean: -20.453259\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -20.458727\n",
            "resetting env. episode 612.000000, reward total was -19.000000. running mean: -20.444140\n",
            "resetting env. episode 613.000000, reward total was -20.000000. running mean: -20.439698\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.445301\n",
            "resetting env. episode 615.000000, reward total was -20.000000. running mean: -20.440848\n",
            "resetting env. episode 616.000000, reward total was -20.000000. running mean: -20.436440\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.442075\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.447655\n",
            "resetting env. episode 619.000000, reward total was -19.000000. running mean: -20.433178\n",
            "resetting env. episode 620.000000, reward total was -19.000000. running mean: -20.418846\n",
            "resetting env. episode 621.000000, reward total was -20.000000. running mean: -20.414658\n",
            "resetting env. episode 622.000000, reward total was -18.000000. running mean: -20.390511\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.396606\n",
            "resetting env. episode 624.000000, reward total was -21.000000. running mean: -20.402640\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.408614\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.414527\n",
            "resetting env. episode 627.000000, reward total was -21.000000. running mean: -20.420382\n",
            "resetting env. episode 628.000000, reward total was -20.000000. running mean: -20.416178\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.422017\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.427796\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -20.433518\n",
            "resetting env. episode 632.000000, reward total was -20.000000. running mean: -20.429183\n",
            "resetting env. episode 633.000000, reward total was -21.000000. running mean: -20.434891\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -20.440543\n",
            "resetting env. episode 635.000000, reward total was -20.000000. running mean: -20.436137\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.441776\n",
            "resetting env. episode 637.000000, reward total was -20.000000. running mean: -20.437358\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -20.442984\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.448555\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -20.454069\n",
            "resetting env. episode 641.000000, reward total was -21.000000. running mean: -20.459528\n",
            "resetting env. episode 642.000000, reward total was -21.000000. running mean: -20.464933\n",
            "resetting env. episode 643.000000, reward total was -21.000000. running mean: -20.470284\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.475581\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.480825\n",
            "resetting env. episode 646.000000, reward total was -20.000000. running mean: -20.476017\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.481257\n",
            "resetting env. episode 648.000000, reward total was -19.000000. running mean: -20.466444\n",
            "resetting env. episode 649.000000, reward total was -21.000000. running mean: -20.471780\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.477062\n",
            "resetting env. episode 651.000000, reward total was -18.000000. running mean: -20.452291\n",
            "resetting env. episode 652.000000, reward total was -20.000000. running mean: -20.447768\n",
            "resetting env. episode 653.000000, reward total was -20.000000. running mean: -20.443291\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -20.448858\n",
            "resetting env. episode 655.000000, reward total was -20.000000. running mean: -20.444369\n",
            "resetting env. episode 656.000000, reward total was -18.000000. running mean: -20.419925\n",
            "resetting env. episode 657.000000, reward total was -21.000000. running mean: -20.425726\n",
            "resetting env. episode 658.000000, reward total was -21.000000. running mean: -20.431469\n",
            "resetting env. episode 659.000000, reward total was -19.000000. running mean: -20.417154\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -20.422983\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -20.428753\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -20.424465\n",
            "resetting env. episode 663.000000, reward total was -18.000000. running mean: -20.400221\n",
            "resetting env. episode 664.000000, reward total was -19.000000. running mean: -20.386218\n",
            "resetting env. episode 665.000000, reward total was -21.000000. running mean: -20.392356\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -20.398433\n",
            "resetting env. episode 667.000000, reward total was -21.000000. running mean: -20.404448\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.410404\n",
            "resetting env. episode 669.000000, reward total was -20.000000. running mean: -20.406300\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.412237\n",
            "resetting env. episode 671.000000, reward total was -21.000000. running mean: -20.418115\n",
            "resetting env. episode 672.000000, reward total was -20.000000. running mean: -20.413933\n",
            "resetting env. episode 673.000000, reward total was -21.000000. running mean: -20.419794\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -20.425596\n",
            "resetting env. episode 675.000000, reward total was -20.000000. running mean: -20.421340\n",
            "resetting env. episode 676.000000, reward total was -20.000000. running mean: -20.417127\n",
            "resetting env. episode 677.000000, reward total was -21.000000. running mean: -20.422955\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -20.428726\n",
            "resetting env. episode 679.000000, reward total was -20.000000. running mean: -20.424439\n",
            "resetting env. episode 680.000000, reward total was -21.000000. running mean: -20.430194\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -20.435892\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -20.441533\n",
            "resetting env. episode 683.000000, reward total was -20.000000. running mean: -20.437118\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -20.432747\n",
            "resetting env. episode 685.000000, reward total was -21.000000. running mean: -20.438419\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -20.444035\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -20.439595\n",
            "resetting env. episode 688.000000, reward total was -21.000000. running mean: -20.445199\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.450747\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -20.446239\n",
            "resetting env. episode 691.000000, reward total was -20.000000. running mean: -20.441777\n",
            "resetting env. episode 692.000000, reward total was -20.000000. running mean: -20.437359\n",
            "resetting env. episode 693.000000, reward total was -21.000000. running mean: -20.442986\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.448556\n",
            "resetting env. episode 695.000000, reward total was -21.000000. running mean: -20.454070\n",
            "resetting env. episode 696.000000, reward total was -20.000000. running mean: -20.449530\n",
            "resetting env. episode 697.000000, reward total was -21.000000. running mean: -20.455034\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -20.460484\n",
            "resetting env. episode 699.000000, reward total was -19.000000. running mean: -20.445879\n",
            "resetting env. episode 700.000000, reward total was -21.000000. running mean: -20.451420\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.456906\n",
            "resetting env. episode 702.000000, reward total was -21.000000. running mean: -20.462337\n",
            "resetting env. episode 703.000000, reward total was -20.000000. running mean: -20.457714\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -20.453137\n",
            "resetting env. episode 705.000000, reward total was -19.000000. running mean: -20.438605\n",
            "resetting env. episode 706.000000, reward total was -21.000000. running mean: -20.444219\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -20.449777\n",
            "resetting env. episode 708.000000, reward total was -19.000000. running mean: -20.435279\n",
            "resetting env. episode 709.000000, reward total was -19.000000. running mean: -20.420926\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -20.426717\n",
            "resetting env. episode 711.000000, reward total was -19.000000. running mean: -20.412450\n",
            "resetting env. episode 712.000000, reward total was -20.000000. running mean: -20.408325\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.414242\n",
            "resetting env. episode 714.000000, reward total was -21.000000. running mean: -20.420100\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.425899\n",
            "resetting env. episode 716.000000, reward total was -20.000000. running mean: -20.421640\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -20.427423\n",
            "resetting env. episode 718.000000, reward total was -21.000000. running mean: -20.433149\n",
            "resetting env. episode 719.000000, reward total was -20.000000. running mean: -20.428818\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -20.434529\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.440184\n",
            "resetting env. episode 722.000000, reward total was -21.000000. running mean: -20.445782\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.451325\n",
            "resetting env. episode 724.000000, reward total was -20.000000. running mean: -20.446811\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -20.452343\n",
            "resetting env. episode 726.000000, reward total was -21.000000. running mean: -20.457820\n",
            "resetting env. episode 727.000000, reward total was -18.000000. running mean: -20.433242\n",
            "resetting env. episode 728.000000, reward total was -21.000000. running mean: -20.438909\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -20.444520\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -20.450075\n",
            "resetting env. episode 731.000000, reward total was -21.000000. running mean: -20.455574\n",
            "resetting env. episode 732.000000, reward total was -20.000000. running mean: -20.451018\n",
            "resetting env. episode 733.000000, reward total was -19.000000. running mean: -20.436508\n",
            "resetting env. episode 734.000000, reward total was -20.000000. running mean: -20.432143\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -20.437822\n",
            "resetting env. episode 736.000000, reward total was -19.000000. running mean: -20.423443\n",
            "resetting env. episode 737.000000, reward total was -21.000000. running mean: -20.429209\n",
            "resetting env. episode 738.000000, reward total was -19.000000. running mean: -20.414917\n",
            "resetting env. episode 739.000000, reward total was -20.000000. running mean: -20.410768\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -20.416660\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.422493\n",
            "resetting env. episode 742.000000, reward total was -21.000000. running mean: -20.428269\n",
            "resetting env. episode 743.000000, reward total was -20.000000. running mean: -20.423986\n",
            "resetting env. episode 744.000000, reward total was -20.000000. running mean: -20.419746\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -20.425549\n",
            "resetting env. episode 746.000000, reward total was -21.000000. running mean: -20.431293\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.436980\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -20.442610\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -20.448184\n",
            "resetting env. episode 750.000000, reward total was -20.000000. running mean: -20.443702\n",
            "resetting env. episode 751.000000, reward total was -19.000000. running mean: -20.429265\n",
            "resetting env. episode 752.000000, reward total was -21.000000. running mean: -20.434973\n",
            "resetting env. episode 753.000000, reward total was -19.000000. running mean: -20.420623\n",
            "resetting env. episode 754.000000, reward total was -21.000000. running mean: -20.426417\n",
            "resetting env. episode 755.000000, reward total was -21.000000. running mean: -20.432153\n",
            "resetting env. episode 756.000000, reward total was -21.000000. running mean: -20.437831\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.443453\n",
            "resetting env. episode 758.000000, reward total was -21.000000. running mean: -20.449018\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -20.454528\n",
            "resetting env. episode 760.000000, reward total was -21.000000. running mean: -20.459983\n",
            "resetting env. episode 761.000000, reward total was -20.000000. running mean: -20.455383\n",
            "resetting env. episode 762.000000, reward total was -21.000000. running mean: -20.460829\n",
            "resetting env. episode 763.000000, reward total was -21.000000. running mean: -20.466221\n",
            "resetting env. episode 764.000000, reward total was -21.000000. running mean: -20.471559\n",
            "resetting env. episode 765.000000, reward total was -21.000000. running mean: -20.476843\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -20.482075\n",
            "resetting env. episode 767.000000, reward total was -20.000000. running mean: -20.477254\n",
            "resetting env. episode 768.000000, reward total was -17.000000. running mean: -20.442481\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.448056\n",
            "resetting env. episode 770.000000, reward total was -20.000000. running mean: -20.443576\n",
            "resetting env. episode 771.000000, reward total was -20.000000. running mean: -20.439140\n",
            "resetting env. episode 772.000000, reward total was -21.000000. running mean: -20.444749\n",
            "resetting env. episode 773.000000, reward total was -19.000000. running mean: -20.430301\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.435998\n",
            "resetting env. episode 775.000000, reward total was -19.000000. running mean: -20.421638\n",
            "resetting env. episode 776.000000, reward total was -20.000000. running mean: -20.417422\n",
            "resetting env. episode 777.000000, reward total was -19.000000. running mean: -20.403248\n",
            "resetting env. episode 778.000000, reward total was -20.000000. running mean: -20.399215\n",
            "resetting env. episode 779.000000, reward total was -21.000000. running mean: -20.405223\n",
            "resetting env. episode 780.000000, reward total was -21.000000. running mean: -20.411171\n",
            "resetting env. episode 781.000000, reward total was -21.000000. running mean: -20.417059\n",
            "resetting env. episode 782.000000, reward total was -21.000000. running mean: -20.422888\n",
            "resetting env. episode 783.000000, reward total was -21.000000. running mean: -20.428660\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -20.434373\n",
            "resetting env. episode 785.000000, reward total was -19.000000. running mean: -20.420029\n",
            "resetting env. episode 786.000000, reward total was -19.000000. running mean: -20.405829\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -20.411771\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -20.417653\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -20.423476\n",
            "resetting env. episode 790.000000, reward total was -20.000000. running mean: -20.419242\n",
            "resetting env. episode 791.000000, reward total was -21.000000. running mean: -20.425049\n",
            "resetting env. episode 792.000000, reward total was -21.000000. running mean: -20.430799\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -20.436491\n",
            "resetting env. episode 794.000000, reward total was -19.000000. running mean: -20.422126\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.427905\n",
            "resetting env. episode 796.000000, reward total was -19.000000. running mean: -20.413626\n",
            "resetting env. episode 797.000000, reward total was -20.000000. running mean: -20.409489\n",
            "resetting env. episode 798.000000, reward total was -20.000000. running mean: -20.405394\n",
            "resetting env. episode 799.000000, reward total was -20.000000. running mean: -20.401340\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.407327\n",
            "resetting env. episode 801.000000, reward total was -20.000000. running mean: -20.403254\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.409221\n",
            "resetting env. episode 803.000000, reward total was -20.000000. running mean: -20.405129\n",
            "resetting env. episode 804.000000, reward total was -21.000000. running mean: -20.411078\n",
            "resetting env. episode 805.000000, reward total was -19.000000. running mean: -20.396967\n",
            "resetting env. episode 806.000000, reward total was -19.000000. running mean: -20.382997\n",
            "resetting env. episode 807.000000, reward total was -21.000000. running mean: -20.389167\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.395276\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -20.401323\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -20.407310\n",
            "resetting env. episode 811.000000, reward total was -19.000000. running mean: -20.393237\n",
            "resetting env. episode 812.000000, reward total was -20.000000. running mean: -20.389304\n",
            "resetting env. episode 813.000000, reward total was -21.000000. running mean: -20.395411\n",
            "resetting env. episode 814.000000, reward total was -19.000000. running mean: -20.381457\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -20.387643\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.393766\n",
            "resetting env. episode 817.000000, reward total was -19.000000. running mean: -20.379828\n",
            "resetting env. episode 818.000000, reward total was -21.000000. running mean: -20.386030\n",
            "resetting env. episode 819.000000, reward total was -20.000000. running mean: -20.382170\n",
            "resetting env. episode 820.000000, reward total was -20.000000. running mean: -20.378348\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -20.384565\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.390719\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.396812\n",
            "resetting env. episode 824.000000, reward total was -21.000000. running mean: -20.402844\n",
            "resetting env. episode 825.000000, reward total was -20.000000. running mean: -20.398815\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -20.404827\n",
            "resetting env. episode 827.000000, reward total was -21.000000. running mean: -20.410779\n",
            "resetting env. episode 828.000000, reward total was -21.000000. running mean: -20.416671\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -20.422504\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -20.428279\n",
            "resetting env. episode 831.000000, reward total was -21.000000. running mean: -20.433997\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -20.439657\n",
            "resetting env. episode 833.000000, reward total was -21.000000. running mean: -20.445260\n",
            "resetting env. episode 834.000000, reward total was -21.000000. running mean: -20.450807\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.456299\n",
            "resetting env. episode 836.000000, reward total was -21.000000. running mean: -20.461736\n",
            "resetting env. episode 837.000000, reward total was -19.000000. running mean: -20.447119\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.452648\n",
            "resetting env. episode 839.000000, reward total was -20.000000. running mean: -20.448121\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -20.453640\n",
            "resetting env. episode 841.000000, reward total was -19.000000. running mean: -20.439104\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -20.444713\n",
            "resetting env. episode 843.000000, reward total was -20.000000. running mean: -20.440266\n",
            "resetting env. episode 844.000000, reward total was -20.000000. running mean: -20.435863\n",
            "resetting env. episode 845.000000, reward total was -21.000000. running mean: -20.441504\n",
            "resetting env. episode 846.000000, reward total was -19.000000. running mean: -20.427089\n",
            "resetting env. episode 847.000000, reward total was -21.000000. running mean: -20.432818\n",
            "resetting env. episode 848.000000, reward total was -19.000000. running mean: -20.418490\n",
            "resetting env. episode 849.000000, reward total was -17.000000. running mean: -20.384305\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -20.390462\n",
            "resetting env. episode 851.000000, reward total was -21.000000. running mean: -20.396558\n",
            "resetting env. episode 852.000000, reward total was -21.000000. running mean: -20.402592\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -20.408566\n",
            "resetting env. episode 854.000000, reward total was -21.000000. running mean: -20.414480\n",
            "resetting env. episode 855.000000, reward total was -20.000000. running mean: -20.410336\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -20.416232\n",
            "resetting env. episode 857.000000, reward total was -20.000000. running mean: -20.412070\n",
            "resetting env. episode 858.000000, reward total was -21.000000. running mean: -20.417949\n",
            "resetting env. episode 859.000000, reward total was -21.000000. running mean: -20.423770\n",
            "resetting env. episode 860.000000, reward total was -20.000000. running mean: -20.419532\n",
            "resetting env. episode 861.000000, reward total was -21.000000. running mean: -20.425337\n",
            "resetting env. episode 862.000000, reward total was -20.000000. running mean: -20.421083\n",
            "resetting env. episode 863.000000, reward total was -20.000000. running mean: -20.416872\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.422704\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -20.428477\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -20.434192\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -20.439850\n",
            "resetting env. episode 868.000000, reward total was -20.000000. running mean: -20.435452\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -20.441097\n",
            "resetting env. episode 870.000000, reward total was -21.000000. running mean: -20.446686\n",
            "resetting env. episode 871.000000, reward total was -19.000000. running mean: -20.432219\n",
            "resetting env. episode 872.000000, reward total was -20.000000. running mean: -20.427897\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.433618\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -20.439282\n",
            "resetting env. episode 875.000000, reward total was -20.000000. running mean: -20.434889\n",
            "resetting env. episode 876.000000, reward total was -21.000000. running mean: -20.440540\n",
            "resetting env. episode 877.000000, reward total was -20.000000. running mean: -20.436135\n",
            "resetting env. episode 878.000000, reward total was -17.000000. running mean: -20.401773\n",
            "resetting env. episode 879.000000, reward total was -21.000000. running mean: -20.407756\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -20.413678\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.419541\n",
            "resetting env. episode 882.000000, reward total was -20.000000. running mean: -20.415346\n",
            "resetting env. episode 883.000000, reward total was -21.000000. running mean: -20.421192\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -20.426981\n",
            "resetting env. episode 885.000000, reward total was -18.000000. running mean: -20.402711\n",
            "resetting env. episode 886.000000, reward total was -20.000000. running mean: -20.398684\n",
            "resetting env. episode 887.000000, reward total was -21.000000. running mean: -20.404697\n",
            "resetting env. episode 888.000000, reward total was -20.000000. running mean: -20.400650\n",
            "resetting env. episode 889.000000, reward total was -21.000000. running mean: -20.406643\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -20.412577\n",
            "resetting env. episode 891.000000, reward total was -20.000000. running mean: -20.408451\n",
            "resetting env. episode 892.000000, reward total was -17.000000. running mean: -20.374367\n",
            "resetting env. episode 893.000000, reward total was -21.000000. running mean: -20.380623\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -20.386817\n",
            "resetting env. episode 895.000000, reward total was -19.000000. running mean: -20.372949\n",
            "resetting env. episode 896.000000, reward total was -19.000000. running mean: -20.359219\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.365627\n",
            "resetting env. episode 898.000000, reward total was -20.000000. running mean: -20.361971\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -20.368351\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -20.374667\n",
            "resetting env. episode 901.000000, reward total was -20.000000. running mean: -20.370921\n",
            "resetting env. episode 902.000000, reward total was -20.000000. running mean: -20.367211\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -20.373539\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -20.379804\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.386006\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -20.392146\n",
            "resetting env. episode 907.000000, reward total was -20.000000. running mean: -20.388224\n",
            "resetting env. episode 908.000000, reward total was -20.000000. running mean: -20.384342\n",
            "resetting env. episode 909.000000, reward total was -20.000000. running mean: -20.380499\n",
            "resetting env. episode 910.000000, reward total was -21.000000. running mean: -20.386694\n",
            "resetting env. episode 911.000000, reward total was -21.000000. running mean: -20.392827\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -20.398899\n",
            "resetting env. episode 913.000000, reward total was -21.000000. running mean: -20.404910\n",
            "resetting env. episode 914.000000, reward total was -20.000000. running mean: -20.400860\n",
            "resetting env. episode 915.000000, reward total was -21.000000. running mean: -20.406852\n",
            "resetting env. episode 916.000000, reward total was -20.000000. running mean: -20.402783\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -20.408756\n",
            "resetting env. episode 918.000000, reward total was -21.000000. running mean: -20.414668\n",
            "resetting env. episode 919.000000, reward total was -21.000000. running mean: -20.420521\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -20.426316\n",
            "resetting env. episode 921.000000, reward total was -21.000000. running mean: -20.432053\n",
            "resetting env. episode 922.000000, reward total was -20.000000. running mean: -20.427732\n",
            "resetting env. episode 923.000000, reward total was -20.000000. running mean: -20.423455\n",
            "resetting env. episode 924.000000, reward total was -20.000000. running mean: -20.419221\n",
            "resetting env. episode 925.000000, reward total was -21.000000. running mean: -20.425028\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -20.430778\n",
            "resetting env. episode 927.000000, reward total was -20.000000. running mean: -20.426470\n",
            "resetting env. episode 928.000000, reward total was -19.000000. running mean: -20.412206\n",
            "resetting env. episode 929.000000, reward total was -20.000000. running mean: -20.408083\n",
            "resetting env. episode 930.000000, reward total was -19.000000. running mean: -20.394003\n",
            "resetting env. episode 931.000000, reward total was -20.000000. running mean: -20.390063\n",
            "resetting env. episode 932.000000, reward total was -21.000000. running mean: -20.396162\n",
            "resetting env. episode 933.000000, reward total was -19.000000. running mean: -20.382200\n",
            "resetting env. episode 934.000000, reward total was -21.000000. running mean: -20.388378\n",
            "resetting env. episode 935.000000, reward total was -21.000000. running mean: -20.394495\n",
            "resetting env. episode 936.000000, reward total was -21.000000. running mean: -20.400550\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -20.396544\n",
            "resetting env. episode 938.000000, reward total was -21.000000. running mean: -20.402579\n",
            "resetting env. episode 939.000000, reward total was -21.000000. running mean: -20.408553\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -20.414467\n",
            "resetting env. episode 941.000000, reward total was -20.000000. running mean: -20.410323\n",
            "resetting env. episode 942.000000, reward total was -21.000000. running mean: -20.416219\n",
            "resetting env. episode 943.000000, reward total was -21.000000. running mean: -20.422057\n",
            "resetting env. episode 944.000000, reward total was -20.000000. running mean: -20.417837\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -20.423658\n",
            "resetting env. episode 946.000000, reward total was -21.000000. running mean: -20.429422\n",
            "resetting env. episode 947.000000, reward total was -20.000000. running mean: -20.425128\n",
            "resetting env. episode 948.000000, reward total was -21.000000. running mean: -20.430876\n",
            "resetting env. episode 949.000000, reward total was -20.000000. running mean: -20.426568\n",
            "resetting env. episode 950.000000, reward total was -21.000000. running mean: -20.432302\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.437979\n",
            "resetting env. episode 952.000000, reward total was -21.000000. running mean: -20.443599\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -20.449163\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -20.454671\n",
            "resetting env. episode 955.000000, reward total was -21.000000. running mean: -20.460125\n",
            "resetting env. episode 956.000000, reward total was -18.000000. running mean: -20.435523\n",
            "resetting env. episode 957.000000, reward total was -21.000000. running mean: -20.441168\n",
            "resetting env. episode 958.000000, reward total was -20.000000. running mean: -20.436757\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -20.442389\n",
            "resetting env. episode 960.000000, reward total was -19.000000. running mean: -20.427965\n",
            "resetting env. episode 961.000000, reward total was -21.000000. running mean: -20.433685\n",
            "resetting env. episode 962.000000, reward total was -21.000000. running mean: -20.439349\n",
            "resetting env. episode 963.000000, reward total was -20.000000. running mean: -20.434955\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -20.440606\n",
            "resetting env. episode 965.000000, reward total was -21.000000. running mean: -20.446199\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.451737\n",
            "resetting env. episode 967.000000, reward total was -21.000000. running mean: -20.457220\n",
            "resetting env. episode 968.000000, reward total was -20.000000. running mean: -20.452648\n",
            "resetting env. episode 969.000000, reward total was -20.000000. running mean: -20.448121\n",
            "resetting env. episode 970.000000, reward total was -20.000000. running mean: -20.443640\n",
            "resetting env. episode 971.000000, reward total was -21.000000. running mean: -20.449204\n",
            "resetting env. episode 972.000000, reward total was -20.000000. running mean: -20.444712\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -20.450265\n",
            "resetting env. episode 974.000000, reward total was -21.000000. running mean: -20.455762\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.461204\n",
            "resetting env. episode 976.000000, reward total was -20.000000. running mean: -20.456592\n",
            "resetting env. episode 977.000000, reward total was -19.000000. running mean: -20.442026\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.447606\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.453130\n",
            "resetting env. episode 980.000000, reward total was -20.000000. running mean: -20.448599\n",
            "resetting env. episode 981.000000, reward total was -21.000000. running mean: -20.454113\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.459572\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.464976\n",
            "resetting env. episode 984.000000, reward total was -20.000000. running mean: -20.460326\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.465723\n",
            "resetting env. episode 986.000000, reward total was -21.000000. running mean: -20.471066\n",
            "resetting env. episode 987.000000, reward total was -20.000000. running mean: -20.466355\n",
            "resetting env. episode 988.000000, reward total was -20.000000. running mean: -20.461692\n",
            "resetting env. episode 989.000000, reward total was -19.000000. running mean: -20.447075\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -20.452604\n",
            "resetting env. episode 991.000000, reward total was -19.000000. running mean: -20.438078\n",
            "resetting env. episode 992.000000, reward total was -18.000000. running mean: -20.413697\n",
            "resetting env. episode 993.000000, reward total was -19.000000. running mean: -20.399560\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.405564\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.411509\n",
            "resetting env. episode 996.000000, reward total was -20.000000. running mean: -20.407394\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -20.413320\n",
            "resetting env. episode 998.000000, reward total was -20.000000. running mean: -20.409187\n",
            "resetting env. episode 999.000000, reward total was -20.000000. running mean: -20.405095\n",
            "resetting env. episode 1000.000000, reward total was -20.000000. running mean: -20.401044\n",
            "resetting env. episode 1001.000000, reward total was -21.000000. running mean: -20.407033\n",
            "resetting env. episode 1002.000000, reward total was -21.000000. running mean: -20.412963\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -20.418833\n",
            "resetting env. episode 1004.000000, reward total was -20.000000. running mean: -20.414645\n",
            "resetting env. episode 1005.000000, reward total was -20.000000. running mean: -20.410499\n",
            "resetting env. episode 1006.000000, reward total was -20.000000. running mean: -20.406394\n",
            "resetting env. episode 1007.000000, reward total was -21.000000. running mean: -20.412330\n",
            "resetting env. episode 1008.000000, reward total was -20.000000. running mean: -20.408206\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -20.414124\n",
            "resetting env. episode 1010.000000, reward total was -19.000000. running mean: -20.399983\n",
            "resetting env. episode 1011.000000, reward total was -20.000000. running mean: -20.395983\n",
            "resetting env. episode 1012.000000, reward total was -20.000000. running mean: -20.392023\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -20.398103\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -20.404122\n",
            "resetting env. episode 1015.000000, reward total was -20.000000. running mean: -20.400081\n",
            "resetting env. episode 1016.000000, reward total was -21.000000. running mean: -20.406080\n",
            "resetting env. episode 1017.000000, reward total was -20.000000. running mean: -20.402019\n",
            "resetting env. episode 1018.000000, reward total was -21.000000. running mean: -20.407999\n",
            "resetting env. episode 1019.000000, reward total was -21.000000. running mean: -20.413919\n",
            "resetting env. episode 1020.000000, reward total was -19.000000. running mean: -20.399780\n",
            "resetting env. episode 1021.000000, reward total was -21.000000. running mean: -20.405782\n",
            "resetting env. episode 1022.000000, reward total was -20.000000. running mean: -20.401724\n",
            "resetting env. episode 1023.000000, reward total was -20.000000. running mean: -20.397707\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.403730\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -20.409693\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -20.415596\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -20.421440\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -20.427225\n",
            "resetting env. episode 1029.000000, reward total was -20.000000. running mean: -20.422953\n",
            "resetting env. episode 1030.000000, reward total was -20.000000. running mean: -20.418724\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.424536\n",
            "resetting env. episode 1032.000000, reward total was -21.000000. running mean: -20.430291\n",
            "resetting env. episode 1033.000000, reward total was -20.000000. running mean: -20.425988\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -20.431728\n",
            "resetting env. episode 1035.000000, reward total was -21.000000. running mean: -20.437411\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -20.443037\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.448606\n",
            "resetting env. episode 1038.000000, reward total was -21.000000. running mean: -20.454120\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.459579\n",
            "resetting env. episode 1040.000000, reward total was -20.000000. running mean: -20.454983\n",
            "resetting env. episode 1041.000000, reward total was -21.000000. running mean: -20.460434\n",
            "resetting env. episode 1042.000000, reward total was -20.000000. running mean: -20.455829\n",
            "resetting env. episode 1043.000000, reward total was -20.000000. running mean: -20.451271\n",
            "resetting env. episode 1044.000000, reward total was -20.000000. running mean: -20.446758\n",
            "resetting env. episode 1045.000000, reward total was -20.000000. running mean: -20.442291\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -20.447868\n",
            "resetting env. episode 1047.000000, reward total was -21.000000. running mean: -20.453389\n",
            "resetting env. episode 1048.000000, reward total was -21.000000. running mean: -20.458855\n",
            "resetting env. episode 1049.000000, reward total was -19.000000. running mean: -20.444267\n",
            "resetting env. episode 1050.000000, reward total was -20.000000. running mean: -20.439824\n",
            "resetting env. episode 1051.000000, reward total was -20.000000. running mean: -20.435426\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -20.431071\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -20.436761\n",
            "resetting env. episode 1054.000000, reward total was -20.000000. running mean: -20.432393\n",
            "resetting env. episode 1055.000000, reward total was -21.000000. running mean: -20.438069\n",
            "resetting env. episode 1056.000000, reward total was -21.000000. running mean: -20.443689\n",
            "resetting env. episode 1057.000000, reward total was -20.000000. running mean: -20.439252\n",
            "resetting env. episode 1058.000000, reward total was -21.000000. running mean: -20.444859\n",
            "resetting env. episode 1059.000000, reward total was -21.000000. running mean: -20.450411\n",
            "resetting env. episode 1060.000000, reward total was -19.000000. running mean: -20.435906\n",
            "resetting env. episode 1061.000000, reward total was -21.000000. running mean: -20.441547\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -20.447132\n",
            "resetting env. episode 1063.000000, reward total was -21.000000. running mean: -20.452661\n",
            "resetting env. episode 1064.000000, reward total was -19.000000. running mean: -20.438134\n",
            "resetting env. episode 1065.000000, reward total was -18.000000. running mean: -20.413753\n",
            "resetting env. episode 1066.000000, reward total was -21.000000. running mean: -20.419615\n",
            "resetting env. episode 1067.000000, reward total was -21.000000. running mean: -20.425419\n",
            "resetting env. episode 1068.000000, reward total was -21.000000. running mean: -20.431165\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -20.436853\n",
            "resetting env. episode 1070.000000, reward total was -18.000000. running mean: -20.412485\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -20.418360\n",
            "resetting env. episode 1072.000000, reward total was -20.000000. running mean: -20.414176\n",
            "resetting env. episode 1073.000000, reward total was -19.000000. running mean: -20.400034\n",
            "resetting env. episode 1074.000000, reward total was -19.000000. running mean: -20.386034\n",
            "resetting env. episode 1075.000000, reward total was -21.000000. running mean: -20.392174\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -20.398252\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -20.404269\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -20.410227\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.416124\n",
            "resetting env. episode 1080.000000, reward total was -21.000000. running mean: -20.421963\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -20.427744\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -20.433466\n",
            "resetting env. episode 1083.000000, reward total was -21.000000. running mean: -20.439131\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -20.444740\n",
            "resetting env. episode 1085.000000, reward total was -20.000000. running mean: -20.440293\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -20.445890\n",
            "resetting env. episode 1087.000000, reward total was -20.000000. running mean: -20.441431\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -20.447017\n",
            "resetting env. episode 1089.000000, reward total was -21.000000. running mean: -20.452546\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -20.458021\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.463441\n",
            "resetting env. episode 1092.000000, reward total was -20.000000. running mean: -20.458806\n",
            "resetting env. episode 1093.000000, reward total was -19.000000. running mean: -20.444218\n",
            "resetting env. episode 1094.000000, reward total was -21.000000. running mean: -20.449776\n",
            "resetting env. episode 1095.000000, reward total was -19.000000. running mean: -20.435278\n",
            "resetting env. episode 1096.000000, reward total was -20.000000. running mean: -20.430926\n",
            "resetting env. episode 1097.000000, reward total was -20.000000. running mean: -20.426616\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -20.432350\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -20.438027\n",
            "resetting env. episode 1100.000000, reward total was -21.000000. running mean: -20.443646\n",
            "resetting env. episode 1101.000000, reward total was -20.000000. running mean: -20.439210\n",
            "resetting env. episode 1102.000000, reward total was -20.000000. running mean: -20.434818\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -20.440470\n",
            "resetting env. episode 1104.000000, reward total was -20.000000. running mean: -20.436065\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.441704\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -20.447287\n",
            "resetting env. episode 1107.000000, reward total was -20.000000. running mean: -20.442814\n",
            "resetting env. episode 1108.000000, reward total was -21.000000. running mean: -20.448386\n",
            "resetting env. episode 1109.000000, reward total was -20.000000. running mean: -20.443902\n",
            "resetting env. episode 1110.000000, reward total was -21.000000. running mean: -20.449463\n",
            "resetting env. episode 1111.000000, reward total was -20.000000. running mean: -20.444969\n",
            "resetting env. episode 1112.000000, reward total was -20.000000. running mean: -20.440519\n",
            "resetting env. episode 1113.000000, reward total was -21.000000. running mean: -20.446114\n",
            "resetting env. episode 1114.000000, reward total was -20.000000. running mean: -20.441653\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -20.447236\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.452764\n",
            "resetting env. episode 1117.000000, reward total was -20.000000. running mean: -20.448236\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -20.453754\n",
            "resetting env. episode 1119.000000, reward total was -21.000000. running mean: -20.459216\n",
            "resetting env. episode 1120.000000, reward total was -21.000000. running mean: -20.464624\n",
            "resetting env. episode 1121.000000, reward total was -21.000000. running mean: -20.469978\n",
            "resetting env. episode 1122.000000, reward total was -19.000000. running mean: -20.455278\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -20.460725\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -20.466118\n",
            "resetting env. episode 1125.000000, reward total was -20.000000. running mean: -20.461457\n",
            "resetting env. episode 1126.000000, reward total was -21.000000. running mean: -20.466842\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -20.472174\n",
            "resetting env. episode 1128.000000, reward total was -21.000000. running mean: -20.477452\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -20.482678\n",
            "resetting env. episode 1130.000000, reward total was -21.000000. running mean: -20.487851\n",
            "resetting env. episode 1131.000000, reward total was -20.000000. running mean: -20.482972\n",
            "resetting env. episode 1132.000000, reward total was -20.000000. running mean: -20.478143\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -20.483361\n",
            "resetting env. episode 1134.000000, reward total was -21.000000. running mean: -20.488528\n",
            "resetting env. episode 1135.000000, reward total was -21.000000. running mean: -20.493642\n",
            "resetting env. episode 1136.000000, reward total was -21.000000. running mean: -20.498706\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -20.503719\n",
            "resetting env. episode 1138.000000, reward total was -20.000000. running mean: -20.498682\n",
            "resetting env. episode 1139.000000, reward total was -20.000000. running mean: -20.493695\n",
            "resetting env. episode 1140.000000, reward total was -21.000000. running mean: -20.498758\n",
            "resetting env. episode 1141.000000, reward total was -21.000000. running mean: -20.503770\n",
            "resetting env. episode 1142.000000, reward total was -21.000000. running mean: -20.508733\n",
            "resetting env. episode 1143.000000, reward total was -21.000000. running mean: -20.513645\n",
            "resetting env. episode 1144.000000, reward total was -20.000000. running mean: -20.508509\n",
            "resetting env. episode 1145.000000, reward total was -19.000000. running mean: -20.493424\n",
            "resetting env. episode 1146.000000, reward total was -21.000000. running mean: -20.498490\n",
            "resetting env. episode 1147.000000, reward total was -17.000000. running mean: -20.463505\n",
            "resetting env. episode 1148.000000, reward total was -21.000000. running mean: -20.468870\n",
            "resetting env. episode 1149.000000, reward total was -19.000000. running mean: -20.454181\n",
            "resetting env. episode 1150.000000, reward total was -21.000000. running mean: -20.459639\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -20.465043\n",
            "resetting env. episode 1152.000000, reward total was -20.000000. running mean: -20.460392\n",
            "resetting env. episode 1153.000000, reward total was -21.000000. running mean: -20.465788\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -20.471130\n",
            "resetting env. episode 1155.000000, reward total was -21.000000. running mean: -20.476419\n",
            "resetting env. episode 1156.000000, reward total was -20.000000. running mean: -20.471655\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -20.476938\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.482169\n",
            "resetting env. episode 1159.000000, reward total was -19.000000. running mean: -20.467347\n",
            "resetting env. episode 1160.000000, reward total was -21.000000. running mean: -20.472674\n",
            "resetting env. episode 1161.000000, reward total was -20.000000. running mean: -20.467947\n",
            "resetting env. episode 1162.000000, reward total was -19.000000. running mean: -20.453268\n",
            "resetting env. episode 1163.000000, reward total was -20.000000. running mean: -20.448735\n",
            "resetting env. episode 1164.000000, reward total was -20.000000. running mean: -20.444248\n",
            "resetting env. episode 1165.000000, reward total was -20.000000. running mean: -20.439805\n",
            "resetting env. episode 1166.000000, reward total was -21.000000. running mean: -20.445407\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -20.450953\n",
            "resetting env. episode 1168.000000, reward total was -20.000000. running mean: -20.446443\n",
            "resetting env. episode 1169.000000, reward total was -20.000000. running mean: -20.441979\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -20.447559\n",
            "resetting env. episode 1171.000000, reward total was -21.000000. running mean: -20.453084\n",
            "resetting env. episode 1172.000000, reward total was -21.000000. running mean: -20.458553\n",
            "resetting env. episode 1173.000000, reward total was -19.000000. running mean: -20.443967\n",
            "resetting env. episode 1174.000000, reward total was -21.000000. running mean: -20.449528\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -20.455032\n",
            "resetting env. episode 1176.000000, reward total was -20.000000. running mean: -20.450482\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -20.455977\n",
            "resetting env. episode 1178.000000, reward total was -20.000000. running mean: -20.451417\n",
            "resetting env. episode 1179.000000, reward total was -20.000000. running mean: -20.446903\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -20.452434\n",
            "resetting env. episode 1181.000000, reward total was -21.000000. running mean: -20.457910\n",
            "resetting env. episode 1182.000000, reward total was -21.000000. running mean: -20.463331\n",
            "resetting env. episode 1183.000000, reward total was -20.000000. running mean: -20.458697\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -20.464111\n",
            "resetting env. episode 1185.000000, reward total was -20.000000. running mean: -20.459469\n",
            "resetting env. episode 1186.000000, reward total was -20.000000. running mean: -20.454875\n",
            "resetting env. episode 1187.000000, reward total was -21.000000. running mean: -20.460326\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -20.465723\n",
            "resetting env. episode 1189.000000, reward total was -18.000000. running mean: -20.441065\n",
            "resetting env. episode 1190.000000, reward total was -21.000000. running mean: -20.446655\n",
            "resetting env. episode 1191.000000, reward total was -21.000000. running mean: -20.452188\n",
            "resetting env. episode 1192.000000, reward total was -19.000000. running mean: -20.437666\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -20.443290\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -20.448857\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.454368\n",
            "resetting env. episode 1196.000000, reward total was -20.000000. running mean: -20.449825\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -20.455326\n",
            "resetting env. episode 1198.000000, reward total was -21.000000. running mean: -20.460773\n",
            "resetting env. episode 1199.000000, reward total was -20.000000. running mean: -20.456165\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -20.461604\n",
            "resetting env. episode 1201.000000, reward total was -21.000000. running mean: -20.466988\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.472318\n",
            "resetting env. episode 1203.000000, reward total was -21.000000. running mean: -20.477595\n",
            "resetting env. episode 1204.000000, reward total was -20.000000. running mean: -20.472819\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -20.478090\n",
            "resetting env. episode 1206.000000, reward total was -20.000000. running mean: -20.473310\n",
            "resetting env. episode 1207.000000, reward total was -19.000000. running mean: -20.458576\n",
            "resetting env. episode 1208.000000, reward total was -21.000000. running mean: -20.463991\n",
            "resetting env. episode 1209.000000, reward total was -19.000000. running mean: -20.449351\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.454857\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -20.460309\n",
            "resetting env. episode 1212.000000, reward total was -21.000000. running mean: -20.465706\n",
            "resetting env. episode 1213.000000, reward total was -21.000000. running mean: -20.471049\n",
            "resetting env. episode 1214.000000, reward total was -21.000000. running mean: -20.476338\n",
            "resetting env. episode 1215.000000, reward total was -20.000000. running mean: -20.471575\n",
            "resetting env. episode 1216.000000, reward total was -20.000000. running mean: -20.466859\n",
            "resetting env. episode 1217.000000, reward total was -20.000000. running mean: -20.462190\n",
            "resetting env. episode 1218.000000, reward total was -20.000000. running mean: -20.457568\n",
            "resetting env. episode 1219.000000, reward total was -21.000000. running mean: -20.462993\n",
            "resetting env. episode 1220.000000, reward total was -20.000000. running mean: -20.458363\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -20.463779\n",
            "resetting env. episode 1222.000000, reward total was -21.000000. running mean: -20.469141\n",
            "resetting env. episode 1223.000000, reward total was -21.000000. running mean: -20.474450\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -20.479706\n",
            "resetting env. episode 1225.000000, reward total was -19.000000. running mean: -20.464908\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -20.470259\n",
            "resetting env. episode 1227.000000, reward total was -20.000000. running mean: -20.465557\n",
            "resetting env. episode 1228.000000, reward total was -21.000000. running mean: -20.470901\n",
            "resetting env. episode 1229.000000, reward total was -21.000000. running mean: -20.476192\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -20.481430\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -20.486616\n",
            "resetting env. episode 1232.000000, reward total was -19.000000. running mean: -20.471750\n",
            "resetting env. episode 1233.000000, reward total was -21.000000. running mean: -20.477032\n",
            "resetting env. episode 1234.000000, reward total was -20.000000. running mean: -20.472262\n",
            "resetting env. episode 1235.000000, reward total was -20.000000. running mean: -20.467539\n",
            "resetting env. episode 1236.000000, reward total was -20.000000. running mean: -20.462864\n",
            "resetting env. episode 1237.000000, reward total was -21.000000. running mean: -20.468235\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -20.473553\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -20.478817\n",
            "resetting env. episode 1240.000000, reward total was -19.000000. running mean: -20.464029\n",
            "resetting env. episode 1241.000000, reward total was -21.000000. running mean: -20.469389\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -20.474695\n",
            "resetting env. episode 1243.000000, reward total was -21.000000. running mean: -20.479948\n",
            "resetting env. episode 1244.000000, reward total was -21.000000. running mean: -20.485149\n",
            "resetting env. episode 1245.000000, reward total was -21.000000. running mean: -20.490297\n",
            "resetting env. episode 1246.000000, reward total was -21.000000. running mean: -20.495394\n",
            "resetting env. episode 1247.000000, reward total was -21.000000. running mean: -20.500440\n",
            "resetting env. episode 1248.000000, reward total was -21.000000. running mean: -20.505436\n",
            "resetting env. episode 1249.000000, reward total was -20.000000. running mean: -20.500382\n",
            "resetting env. episode 1250.000000, reward total was -19.000000. running mean: -20.485378\n",
            "resetting env. episode 1251.000000, reward total was -21.000000. running mean: -20.490524\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -20.495619\n",
            "resetting env. episode 1253.000000, reward total was -21.000000. running mean: -20.500662\n",
            "resetting env. episode 1254.000000, reward total was -20.000000. running mean: -20.495656\n",
            "resetting env. episode 1255.000000, reward total was -20.000000. running mean: -20.490699\n",
            "resetting env. episode 1256.000000, reward total was -20.000000. running mean: -20.485792\n",
            "resetting env. episode 1257.000000, reward total was -21.000000. running mean: -20.490934\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -20.496025\n",
            "resetting env. episode 1259.000000, reward total was -20.000000. running mean: -20.491065\n",
            "resetting env. episode 1260.000000, reward total was -21.000000. running mean: -20.496154\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -20.501193\n",
            "resetting env. episode 1262.000000, reward total was -20.000000. running mean: -20.496181\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -20.501219\n",
            "resetting env. episode 1264.000000, reward total was -20.000000. running mean: -20.496207\n",
            "resetting env. episode 1265.000000, reward total was -18.000000. running mean: -20.471245\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -20.476532\n",
            "resetting env. episode 1267.000000, reward total was -21.000000. running mean: -20.481767\n",
            "resetting env. episode 1268.000000, reward total was -21.000000. running mean: -20.486949\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -20.492080\n",
            "resetting env. episode 1270.000000, reward total was -21.000000. running mean: -20.497159\n",
            "resetting env. episode 1271.000000, reward total was -21.000000. running mean: -20.502187\n",
            "resetting env. episode 1272.000000, reward total was -19.000000. running mean: -20.487165\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -20.492294\n",
            "resetting env. episode 1274.000000, reward total was -20.000000. running mean: -20.487371\n",
            "resetting env. episode 1275.000000, reward total was -20.000000. running mean: -20.482497\n",
            "resetting env. episode 1276.000000, reward total was -21.000000. running mean: -20.487672\n",
            "resetting env. episode 1277.000000, reward total was -20.000000. running mean: -20.482795\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -20.487967\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -20.493088\n",
            "resetting env. episode 1280.000000, reward total was -21.000000. running mean: -20.498157\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -20.503175\n",
            "resetting env. episode 1282.000000, reward total was -20.000000. running mean: -20.498144\n",
            "resetting env. episode 1283.000000, reward total was -20.000000. running mean: -20.493162\n",
            "resetting env. episode 1284.000000, reward total was -18.000000. running mean: -20.468231\n",
            "resetting env. episode 1285.000000, reward total was -21.000000. running mean: -20.473548\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -20.478813\n",
            "resetting env. episode 1287.000000, reward total was -21.000000. running mean: -20.484025\n",
            "resetting env. episode 1288.000000, reward total was -20.000000. running mean: -20.479184\n",
            "resetting env. episode 1289.000000, reward total was -20.000000. running mean: -20.474393\n",
            "resetting env. episode 1290.000000, reward total was -21.000000. running mean: -20.479649\n",
            "resetting env. episode 1291.000000, reward total was -19.000000. running mean: -20.464852\n",
            "resetting env. episode 1292.000000, reward total was -21.000000. running mean: -20.470204\n",
            "resetting env. episode 1293.000000, reward total was -20.000000. running mean: -20.465502\n",
            "resetting env. episode 1294.000000, reward total was -19.000000. running mean: -20.450847\n",
            "resetting env. episode 1295.000000, reward total was -20.000000. running mean: -20.446338\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -20.451875\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -20.457356\n",
            "resetting env. episode 1298.000000, reward total was -19.000000. running mean: -20.442782\n",
            "resetting env. episode 1299.000000, reward total was -21.000000. running mean: -20.448355\n",
            "resetting env. episode 1300.000000, reward total was -21.000000. running mean: -20.453871\n",
            "resetting env. episode 1301.000000, reward total was -21.000000. running mean: -20.459332\n",
            "resetting env. episode 1302.000000, reward total was -20.000000. running mean: -20.454739\n",
            "resetting env. episode 1303.000000, reward total was -21.000000. running mean: -20.460192\n",
            "resetting env. episode 1304.000000, reward total was -20.000000. running mean: -20.455590\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -20.461034\n",
            "resetting env. episode 1306.000000, reward total was -21.000000. running mean: -20.466423\n",
            "resetting env. episode 1307.000000, reward total was -21.000000. running mean: -20.471759\n",
            "resetting env. episode 1308.000000, reward total was -21.000000. running mean: -20.477042\n",
            "resetting env. episode 1309.000000, reward total was -20.000000. running mean: -20.472271\n",
            "resetting env. episode 1310.000000, reward total was -21.000000. running mean: -20.477549\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -20.482773\n",
            "resetting env. episode 1312.000000, reward total was -21.000000. running mean: -20.487945\n",
            "resetting env. episode 1313.000000, reward total was -20.000000. running mean: -20.483066\n",
            "resetting env. episode 1314.000000, reward total was -18.000000. running mean: -20.458235\n",
            "resetting env. episode 1315.000000, reward total was -21.000000. running mean: -20.463653\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -20.469016\n",
            "resetting env. episode 1317.000000, reward total was -20.000000. running mean: -20.464326\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -20.469683\n",
            "resetting env. episode 1319.000000, reward total was -20.000000. running mean: -20.464986\n",
            "resetting env. episode 1320.000000, reward total was -21.000000. running mean: -20.470336\n",
            "resetting env. episode 1321.000000, reward total was -18.000000. running mean: -20.445633\n",
            "resetting env. episode 1322.000000, reward total was -21.000000. running mean: -20.451176\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -20.456665\n",
            "resetting env. episode 1324.000000, reward total was -20.000000. running mean: -20.452098\n",
            "resetting env. episode 1325.000000, reward total was -20.000000. running mean: -20.447577\n",
            "resetting env. episode 1326.000000, reward total was -19.000000. running mean: -20.433101\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -20.438770\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -20.444383\n",
            "resetting env. episode 1329.000000, reward total was -20.000000. running mean: -20.439939\n",
            "resetting env. episode 1330.000000, reward total was -20.000000. running mean: -20.435539\n",
            "resetting env. episode 1331.000000, reward total was -21.000000. running mean: -20.441184\n",
            "resetting env. episode 1332.000000, reward total was -21.000000. running mean: -20.446772\n",
            "resetting env. episode 1333.000000, reward total was -19.000000. running mean: -20.432304\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -20.437981\n",
            "resetting env. episode 1335.000000, reward total was -20.000000. running mean: -20.433602\n",
            "resetting env. episode 1336.000000, reward total was -20.000000. running mean: -20.429266\n",
            "resetting env. episode 1337.000000, reward total was -21.000000. running mean: -20.434973\n",
            "resetting env. episode 1338.000000, reward total was -21.000000. running mean: -20.440623\n",
            "resetting env. episode 1339.000000, reward total was -20.000000. running mean: -20.436217\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -20.441855\n",
            "resetting env. episode 1341.000000, reward total was -21.000000. running mean: -20.447436\n",
            "resetting env. episode 1342.000000, reward total was -21.000000. running mean: -20.452962\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -20.458432\n",
            "resetting env. episode 1344.000000, reward total was -20.000000. running mean: -20.453848\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.459309\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -20.464716\n",
            "resetting env. episode 1347.000000, reward total was -19.000000. running mean: -20.450069\n",
            "resetting env. episode 1348.000000, reward total was -21.000000. running mean: -20.455569\n",
            "resetting env. episode 1349.000000, reward total was -20.000000. running mean: -20.451013\n",
            "resetting env. episode 1350.000000, reward total was -20.000000. running mean: -20.446503\n",
            "resetting env. episode 1351.000000, reward total was -18.000000. running mean: -20.422038\n",
            "resetting env. episode 1352.000000, reward total was -21.000000. running mean: -20.427817\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -20.433539\n",
            "resetting env. episode 1354.000000, reward total was -21.000000. running mean: -20.439204\n",
            "resetting env. episode 1355.000000, reward total was -20.000000. running mean: -20.434812\n",
            "resetting env. episode 1356.000000, reward total was -21.000000. running mean: -20.440464\n",
            "resetting env. episode 1357.000000, reward total was -21.000000. running mean: -20.446059\n",
            "resetting env. episode 1358.000000, reward total was -21.000000. running mean: -20.451598\n",
            "resetting env. episode 1359.000000, reward total was -21.000000. running mean: -20.457082\n",
            "resetting env. episode 1360.000000, reward total was -19.000000. running mean: -20.442512\n",
            "resetting env. episode 1361.000000, reward total was -21.000000. running mean: -20.448086\n",
            "resetting env. episode 1362.000000, reward total was -20.000000. running mean: -20.443606\n",
            "resetting env. episode 1363.000000, reward total was -21.000000. running mean: -20.449169\n",
            "resetting env. episode 1364.000000, reward total was -19.000000. running mean: -20.434678\n",
            "resetting env. episode 1365.000000, reward total was -20.000000. running mean: -20.430331\n",
            "resetting env. episode 1366.000000, reward total was -21.000000. running mean: -20.436028\n",
            "resetting env. episode 1367.000000, reward total was -21.000000. running mean: -20.441667\n",
            "resetting env. episode 1368.000000, reward total was -21.000000. running mean: -20.447251\n",
            "resetting env. episode 1369.000000, reward total was -21.000000. running mean: -20.452778\n",
            "resetting env. episode 1370.000000, reward total was -21.000000. running mean: -20.458250\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -20.463668\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -20.469031\n",
            "resetting env. episode 1373.000000, reward total was -21.000000. running mean: -20.474341\n",
            "resetting env. episode 1374.000000, reward total was -20.000000. running mean: -20.469598\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -20.474902\n",
            "resetting env. episode 1376.000000, reward total was -20.000000. running mean: -20.470153\n",
            "resetting env. episode 1377.000000, reward total was -21.000000. running mean: -20.475451\n",
            "resetting env. episode 1378.000000, reward total was -21.000000. running mean: -20.480697\n",
            "resetting env. episode 1379.000000, reward total was -21.000000. running mean: -20.485890\n",
            "resetting env. episode 1380.000000, reward total was -20.000000. running mean: -20.481031\n",
            "resetting env. episode 1381.000000, reward total was -19.000000. running mean: -20.466220\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -20.471558\n",
            "resetting env. episode 1383.000000, reward total was -21.000000. running mean: -20.476843\n",
            "resetting env. episode 1384.000000, reward total was -21.000000. running mean: -20.482074\n",
            "resetting env. episode 1385.000000, reward total was -20.000000. running mean: -20.477253\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -20.482481\n",
            "resetting env. episode 1387.000000, reward total was -21.000000. running mean: -20.487656\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -20.492780\n",
            "resetting env. episode 1389.000000, reward total was -21.000000. running mean: -20.497852\n",
            "resetting env. episode 1390.000000, reward total was -19.000000. running mean: -20.482873\n",
            "resetting env. episode 1391.000000, reward total was -21.000000. running mean: -20.488044\n",
            "resetting env. episode 1392.000000, reward total was -19.000000. running mean: -20.473164\n",
            "resetting env. episode 1393.000000, reward total was -20.000000. running mean: -20.468432\n",
            "resetting env. episode 1394.000000, reward total was -20.000000. running mean: -20.463748\n",
            "resetting env. episode 1395.000000, reward total was -21.000000. running mean: -20.469111\n",
            "resetting env. episode 1396.000000, reward total was -20.000000. running mean: -20.464419\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -20.469775\n",
            "resetting env. episode 1398.000000, reward total was -20.000000. running mean: -20.465078\n",
            "resetting env. episode 1399.000000, reward total was -21.000000. running mean: -20.470427\n",
            "resetting env. episode 1400.000000, reward total was -21.000000. running mean: -20.475722\n",
            "resetting env. episode 1401.000000, reward total was -21.000000. running mean: -20.480965\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -20.486156\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -20.491294\n",
            "resetting env. episode 1404.000000, reward total was -20.000000. running mean: -20.486381\n",
            "resetting env. episode 1405.000000, reward total was -21.000000. running mean: -20.491517\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -20.496602\n",
            "resetting env. episode 1407.000000, reward total was -20.000000. running mean: -20.491636\n",
            "resetting env. episode 1408.000000, reward total was -21.000000. running mean: -20.496720\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -20.501753\n",
            "resetting env. episode 1410.000000, reward total was -20.000000. running mean: -20.496735\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -20.501768\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -20.506750\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -20.511682\n",
            "resetting env. episode 1414.000000, reward total was -21.000000. running mean: -20.516566\n",
            "resetting env. episode 1415.000000, reward total was -21.000000. running mean: -20.521400\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -20.526186\n",
            "resetting env. episode 1417.000000, reward total was -19.000000. running mean: -20.510924\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -20.515815\n",
            "resetting env. episode 1419.000000, reward total was -19.000000. running mean: -20.500657\n",
            "resetting env. episode 1420.000000, reward total was -21.000000. running mean: -20.505650\n",
            "resetting env. episode 1421.000000, reward total was -21.000000. running mean: -20.510594\n",
            "resetting env. episode 1422.000000, reward total was -20.000000. running mean: -20.505488\n",
            "resetting env. episode 1423.000000, reward total was -21.000000. running mean: -20.510433\n",
            "resetting env. episode 1424.000000, reward total was -21.000000. running mean: -20.515329\n",
            "resetting env. episode 1425.000000, reward total was -21.000000. running mean: -20.520175\n",
            "resetting env. episode 1426.000000, reward total was -20.000000. running mean: -20.514974\n",
            "resetting env. episode 1427.000000, reward total was -20.000000. running mean: -20.509824\n",
            "resetting env. episode 1428.000000, reward total was -19.000000. running mean: -20.494726\n",
            "resetting env. episode 1429.000000, reward total was -20.000000. running mean: -20.489778\n",
            "resetting env. episode 1430.000000, reward total was -18.000000. running mean: -20.464880\n",
            "resetting env. episode 1431.000000, reward total was -21.000000. running mean: -20.470232\n",
            "resetting env. episode 1432.000000, reward total was -21.000000. running mean: -20.475529\n",
            "resetting env. episode 1433.000000, reward total was -19.000000. running mean: -20.460774\n",
            "resetting env. episode 1434.000000, reward total was -21.000000. running mean: -20.466166\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -20.471505\n",
            "resetting env. episode 1436.000000, reward total was -20.000000. running mean: -20.466790\n",
            "resetting env. episode 1437.000000, reward total was -18.000000. running mean: -20.442122\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.447701\n",
            "resetting env. episode 1439.000000, reward total was -21.000000. running mean: -20.453224\n",
            "resetting env. episode 1440.000000, reward total was -21.000000. running mean: -20.458691\n",
            "resetting env. episode 1441.000000, reward total was -20.000000. running mean: -20.454104\n",
            "resetting env. episode 1442.000000, reward total was -20.000000. running mean: -20.449563\n",
            "resetting env. episode 1443.000000, reward total was -21.000000. running mean: -20.455068\n",
            "resetting env. episode 1444.000000, reward total was -19.000000. running mean: -20.440517\n",
            "resetting env. episode 1445.000000, reward total was -20.000000. running mean: -20.436112\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -20.441751\n",
            "resetting env. episode 1447.000000, reward total was -21.000000. running mean: -20.447333\n",
            "resetting env. episode 1448.000000, reward total was -21.000000. running mean: -20.452860\n",
            "resetting env. episode 1449.000000, reward total was -21.000000. running mean: -20.458331\n",
            "resetting env. episode 1450.000000, reward total was -21.000000. running mean: -20.463748\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -20.469110\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -20.474419\n",
            "resetting env. episode 1453.000000, reward total was -21.000000. running mean: -20.479675\n",
            "resetting env. episode 1454.000000, reward total was -21.000000. running mean: -20.484878\n",
            "resetting env. episode 1455.000000, reward total was -20.000000. running mean: -20.480030\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -20.485229\n",
            "resetting env. episode 1457.000000, reward total was -20.000000. running mean: -20.480377\n",
            "resetting env. episode 1458.000000, reward total was -21.000000. running mean: -20.485573\n",
            "resetting env. episode 1459.000000, reward total was -19.000000. running mean: -20.470718\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -20.476010\n",
            "resetting env. episode 1461.000000, reward total was -19.000000. running mean: -20.461250\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -20.466638\n",
            "resetting env. episode 1463.000000, reward total was -20.000000. running mean: -20.461971\n",
            "resetting env. episode 1464.000000, reward total was -21.000000. running mean: -20.467352\n",
            "resetting env. episode 1465.000000, reward total was -20.000000. running mean: -20.462678\n",
            "resetting env. episode 1466.000000, reward total was -21.000000. running mean: -20.468051\n",
            "resetting env. episode 1467.000000, reward total was -20.000000. running mean: -20.463371\n",
            "resetting env. episode 1468.000000, reward total was -20.000000. running mean: -20.458737\n",
            "resetting env. episode 1469.000000, reward total was -21.000000. running mean: -20.464150\n",
            "resetting env. episode 1470.000000, reward total was -20.000000. running mean: -20.459508\n",
            "resetting env. episode 1471.000000, reward total was -21.000000. running mean: -20.464913\n",
            "resetting env. episode 1472.000000, reward total was -21.000000. running mean: -20.470264\n",
            "resetting env. episode 1473.000000, reward total was -21.000000. running mean: -20.475561\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -20.480806\n",
            "resetting env. episode 1475.000000, reward total was -20.000000. running mean: -20.475998\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -20.481238\n",
            "resetting env. episode 1477.000000, reward total was -21.000000. running mean: -20.486425\n",
            "resetting env. episode 1478.000000, reward total was -18.000000. running mean: -20.461561\n",
            "resetting env. episode 1479.000000, reward total was -19.000000. running mean: -20.446946\n",
            "resetting env. episode 1480.000000, reward total was -21.000000. running mean: -20.452476\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -20.457951\n",
            "resetting env. episode 1482.000000, reward total was -18.000000. running mean: -20.433372\n",
            "resetting env. episode 1483.000000, reward total was -21.000000. running mean: -20.439038\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -20.444648\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -20.450201\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -20.455699\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -20.461142\n",
            "resetting env. episode 1488.000000, reward total was -21.000000. running mean: -20.466531\n",
            "resetting env. episode 1489.000000, reward total was -21.000000. running mean: -20.471866\n",
            "resetting env. episode 1490.000000, reward total was -20.000000. running mean: -20.467147\n",
            "resetting env. episode 1491.000000, reward total was -20.000000. running mean: -20.462475\n",
            "resetting env. episode 1492.000000, reward total was -20.000000. running mean: -20.457851\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -20.463272\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -20.468639\n",
            "resetting env. episode 1495.000000, reward total was -20.000000. running mean: -20.463953\n",
            "resetting env. episode 1496.000000, reward total was -20.000000. running mean: -20.459313\n",
            "resetting env. episode 1497.000000, reward total was -20.000000. running mean: -20.454720\n",
            "resetting env. episode 1498.000000, reward total was -21.000000. running mean: -20.460173\n",
            "resetting env. episode 1499.000000, reward total was -18.000000. running mean: -20.435571\n",
            "resetting env. episode 1500.000000, reward total was -19.000000. running mean: -20.421216\n",
            "CPU times: user 1h 11min 34s, sys: 32min 44s, total: 1h 44min 19s\n",
            "Wall time: 53min 31s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "a316f7d2-f7dd-42fb-be09-0675546e2722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}