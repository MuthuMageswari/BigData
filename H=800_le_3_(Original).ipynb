{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "outputs": [],
      "source": [
        "!pip install gym >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "outputs": [],
      "source": [
        "!pip install JSAnimation >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "6dba303c-27c1-4925-c151-af05f3a89ae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 39.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=faa5a765a55b8c8eff7cffec6a685e015b526ffb4e5ec0bb3a624c6e1031af9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtT2GyK_6edc",
        "outputId": "ac86f072-77cb-479a-aced-1d5a91ed6d2b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRE6WmXQJ1Z0",
        "outputId": "f68f5eb8-cbe4-4dc5-e60f-65137b6da061"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl_9d4HFJ31W",
        "outputId": "6391dbcc-2e23-44b7-963a-13d6b36b24c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trwRXI-h6eeI",
        "outputId": "8376c6f0-dcd1-44f9-9c4e-b2daba32259d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode finished without success, accumulated reward = -19.0\n"
          ]
        }
      ],
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 800 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ka_5Vl9Orm",
        "outputId": "70f3240c-945f-4482-d6ed-acc9529d248a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "resetting env. episode 1.000000, reward total was -18.000000. running mean: -18.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -18.020000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -18.049800\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -18.079302\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -18.108509\n",
            "resetting env. episode 6.000000, reward total was -19.000000. running mean: -18.117424\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -18.136250\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -18.154887\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -18.173338\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -18.191605\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -18.219689\n",
            "resetting env. episode 12.000000, reward total was -19.000000. running mean: -18.227492\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -18.255217\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -18.272665\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -18.299938\n",
            "resetting env. episode 16.000000, reward total was -19.000000. running mean: -18.306939\n",
            "resetting env. episode 17.000000, reward total was -19.000000. running mean: -18.313869\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -18.340731\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -18.357323\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -18.383750\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -18.399913\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -18.425914\n",
            "resetting env. episode 23.000000, reward total was -19.000000. running mean: -18.431654\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -18.447338\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -18.472865\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -18.498136\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -18.513155\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -18.528023\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -18.552743\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -18.567215\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -18.581543\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -18.595728\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -18.609770\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -18.623673\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -18.647436\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -18.670962\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -18.694252\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -18.717310\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -18.740136\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -18.762735\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -18.785108\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -18.807257\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -18.829184\n",
            "resetting env. episode 44.000000, reward total was -18.000000. running mean: -18.820892\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -18.832683\n",
            "resetting env. episode 46.000000, reward total was -19.000000. running mean: -18.834356\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -18.846013\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -18.857553\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -18.868977\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -18.890287\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -18.911385\n",
            "resetting env. episode 52.000000, reward total was -18.000000. running mean: -18.902271\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -18.923248\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -18.944016\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -18.964575\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -18.984930\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -19.005080\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -19.015030\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -19.024879\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.044630\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -19.054184\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -19.063642\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.083006\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.102176\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -19.121154\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.139943\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -19.148543\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -19.157058\n",
            "resetting env. episode 69.000000, reward total was -19.000000. running mean: -19.155487\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.173932\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -19.192193\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -19.210271\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.228168\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -19.245887\n",
            "resetting env. episode 75.000000, reward total was -19.000000. running mean: -19.243428\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.260993\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -19.268384\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -19.275700\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -19.292943\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -19.300013\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -19.307013\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -19.313943\n",
            "resetting env. episode 83.000000, reward total was -19.000000. running mean: -19.310804\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.327696\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.344419\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -19.350974\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.367465\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.383790\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -19.399952\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -19.415953\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -19.431793\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -19.447475\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -19.463000\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -19.478370\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -19.493587\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -19.508651\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -19.513564\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -19.518429\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.533244\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -19.547912\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -19.562433\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -19.566808\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -19.581140\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -19.585329\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -19.599476\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.613481\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -19.627346\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -19.641073\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.654662\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.668115\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -19.681434\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -19.674620\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -19.677874\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -19.691095\n",
            "resetting env. episode 115.000000, reward total was -19.000000. running mean: -19.684184\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -19.697342\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.710369\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -19.723265\n",
            "resetting env. episode 119.000000, reward total was -18.000000. running mean: -19.706032\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -19.708972\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -19.711882\n",
            "resetting env. episode 122.000000, reward total was -19.000000. running mean: -19.704763\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -19.707716\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -19.710639\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -19.713532\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -19.726397\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -19.729133\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -19.741842\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -19.754423\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -19.766879\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -19.779210\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -19.791418\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -19.803504\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -19.815469\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -19.827314\n",
            "resetting env. episode 136.000000, reward total was -19.000000. running mean: -19.819041\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -19.830851\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -19.832542\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -19.844217\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -19.855775\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -19.857217\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -19.868645\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -19.869958\n",
            "resetting env. episode 144.000000, reward total was -18.000000. running mean: -19.851259\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -19.862746\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -19.864119\n",
            "resetting env. episode 147.000000, reward total was -19.000000. running mean: -19.855477\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -19.856923\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -19.868353\n",
            "resetting env. episode 150.000000, reward total was -19.000000. running mean: -19.859670\n",
            "resetting env. episode 151.000000, reward total was -18.000000. running mean: -19.841073\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -19.842662\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -19.854236\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -19.855693\n",
            "resetting env. episode 155.000000, reward total was -17.000000. running mean: -19.827137\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -19.838865\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -19.840477\n",
            "resetting env. episode 158.000000, reward total was -19.000000. running mean: -19.832072\n",
            "resetting env. episode 159.000000, reward total was -19.000000. running mean: -19.823751\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -19.835514\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -19.847158\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -19.858687\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -19.860100\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -19.871499\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -19.872784\n",
            "resetting env. episode 166.000000, reward total was -19.000000. running mean: -19.864056\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -19.875416\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -19.886661\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -19.887795\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -19.898917\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -19.909928\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -19.920828\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -19.931620\n",
            "resetting env. episode 174.000000, reward total was -19.000000. running mean: -19.922304\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -19.933081\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -19.943750\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -19.954313\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -19.954769\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -19.965222\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -19.975570\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -19.985814\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -19.995956\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.005996\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.005936\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.005877\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.005818\n",
            "resetting env. episode 187.000000, reward total was -18.000000. running mean: -19.985760\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -19.995902\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.005943\n",
            "resetting env. episode 190.000000, reward total was -19.000000. running mean: -19.995884\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -19.995925\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.005966\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.015906\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.015747\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.025590\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.035334\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.044980\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.044530\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.044085\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.053644\n",
            "resetting env. episode 201.000000, reward total was -18.000000. running mean: -20.033108\n",
            "resetting env. episode 202.000000, reward total was -17.000000. running mean: -20.002777\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.012749\n",
            "resetting env. episode 204.000000, reward total was -18.000000. running mean: -19.992622\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -19.982695\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -19.992868\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.002940\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.012910\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.022781\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.032553\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.032228\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.041906\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.051487\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.060972\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.070362\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.069658\n",
            "resetting env. episode 217.000000, reward total was -19.000000. running mean: -20.058962\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.068372\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.077688\n",
            "resetting env. episode 220.000000, reward total was -19.000000. running mean: -20.066912\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.066242\n",
            "resetting env. episode 222.000000, reward total was -19.000000. running mean: -20.055580\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.065024\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.064374\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.063730\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.063093\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.072462\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.071737\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.071020\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.080310\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.079507\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.088712\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.097824\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.106846\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.115778\n",
            "resetting env. episode 236.000000, reward total was -19.000000. running mean: -20.104620\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.113574\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.122438\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.131214\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.139902\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.148503\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.157018\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.165447\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.173793\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.172055\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.170334\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.178631\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.186845\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.184976\n",
            "resetting env. episode 250.000000, reward total was -19.000000. running mean: -20.173127\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.181395\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.179581\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.187785\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.195908\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.203949\n",
            "resetting env. episode 256.000000, reward total was -19.000000. running mean: -20.191909\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.189990\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.188090\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.196209\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.194247\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.202305\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.200282\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.208279\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.216196\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.214034\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.221894\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.219675\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.217478\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.215303\n",
            "resetting env. episode 270.000000, reward total was -18.000000. running mean: -20.193150\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.191219\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.189306\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.197413\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.205439\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.213385\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.221251\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.229039\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.226748\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.224481\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.232236\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.239913\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.247514\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.255039\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.262489\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.269864\n",
            "resetting env. episode 286.000000, reward total was -18.000000. running mean: -20.247165\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.244694\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.242247\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.249824\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.247326\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.254853\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.262304\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.269681\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.276984\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.274215\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.281472\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.288658\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.285771\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.292913\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.299984\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.296984\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.304015\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.300974\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.307965\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.304885\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.301836\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.308818\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.315730\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.322572\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.329347\n",
            "resetting env. episode 311.000000, reward total was -20.000000. running mean: -20.326053\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.332793\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.329465\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.326170\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.332908\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.329579\n",
            "resetting env. episode 317.000000, reward total was -19.000000. running mean: -20.316283\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.313121\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.319989\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.326790\n",
            "resetting env. episode 321.000000, reward total was -19.000000. running mean: -20.313522\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.320386\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.327183\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.333911\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.340572\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.337166\n",
            "resetting env. episode 327.000000, reward total was -18.000000. running mean: -20.313794\n",
            "resetting env. episode 328.000000, reward total was -19.000000. running mean: -20.300656\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.297650\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.304673\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.311626\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.318510\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.325325\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.332072\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.338751\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.335364\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.342010\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.328590\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.315304\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.322151\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.328929\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.335640\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.342284\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.348861\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.355372\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.361819\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.368200\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.374518\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.370773\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.377065\n",
            "resetting env. episode 351.000000, reward total was -18.000000. running mean: -20.353295\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.359762\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.366164\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.362503\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.368878\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.375189\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.381437\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.377623\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.373846\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.380108\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.376307\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.382544\n",
            "resetting env. episode 363.000000, reward total was -16.000000. running mean: -20.338718\n",
            "resetting env. episode 364.000000, reward total was -19.000000. running mean: -20.325331\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.332078\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.338757\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.345369\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.351916\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.348397\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.344913\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.351464\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.357949\n",
            "resetting env. episode 373.000000, reward total was -19.000000. running mean: -20.344369\n",
            "resetting env. episode 374.000000, reward total was -19.000000. running mean: -20.330926\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.337616\n",
            "resetting env. episode 376.000000, reward total was -18.000000. running mean: -20.314240\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.321098\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.317887\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.324708\n",
            "resetting env. episode 380.000000, reward total was -19.000000. running mean: -20.311461\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.308346\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.305263\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.302210\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.299188\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.306196\n",
            "resetting env. episode 386.000000, reward total was -19.000000. running mean: -20.293134\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.300203\n",
            "resetting env. episode 388.000000, reward total was -19.000000. running mean: -20.287201\n",
            "resetting env. episode 389.000000, reward total was -20.000000. running mean: -20.284329\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.281486\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.288671\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.295784\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.302826\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.309798\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.306700\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.313633\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.320497\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.317292\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.314119\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.310978\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.307868\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.304789\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.301741\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.298724\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.305737\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.302679\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.299652\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.296656\n",
            "resetting env. episode 409.000000, reward total was -19.000000. running mean: -20.283689\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.280852\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.278044\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.275263\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.282511\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.289686\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.296789\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.293821\n",
            "resetting env. episode 417.000000, reward total was -19.000000. running mean: -20.280883\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.288074\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.295193\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.292241\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.289319\n",
            "resetting env. episode 422.000000, reward total was -18.000000. running mean: -20.266426\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.273761\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.281024\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.268214\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.275531\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.282776\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.289948\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.287049\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.284178\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.281337\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.278523\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.275738\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.272981\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.270251\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.277548\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.284773\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.281925\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.279106\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.286315\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.293452\n",
            "resetting env. episode 442.000000, reward total was -19.000000. running mean: -20.280517\n",
            "resetting env. episode 443.000000, reward total was -19.000000. running mean: -20.267712\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.275035\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.272284\n",
            "resetting env. episode 446.000000, reward total was -19.000000. running mean: -20.259562\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.256966\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.264396\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.271752\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.279035\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.286245\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.293382\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.290448\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.287544\n",
            "resetting env. episode 455.000000, reward total was -18.000000. running mean: -20.264668\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.262022\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.259401\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.266807\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.274139\n",
            "resetting env. episode 460.000000, reward total was -19.000000. running mean: -20.261398\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.258784\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.256196\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.263634\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.270998\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.268288\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.265605\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -20.252949\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.250419\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.247915\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.245436\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.252982\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.260452\n",
            "resetting env. episode 473.000000, reward total was -17.000000. running mean: -20.227847\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.235569\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.243213\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.240781\n",
            "resetting env. episode 477.000000, reward total was -19.000000. running mean: -20.228373\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.236090\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.243729\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.251291\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.258778\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.256191\n",
            "resetting env. episode 483.000000, reward total was -19.000000. running mean: -20.243629\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.241192\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.238781\n",
            "resetting env. episode 486.000000, reward total was -18.000000. running mean: -20.216393\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.214229\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.222087\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.219866\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.227667\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.235390\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.233036\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.240706\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.238299\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.235916\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.223557\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.231321\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.239008\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.236618\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.244252\n",
            "CPU times: user 23min 12s, sys: 10min 21s, total: 33min 34s\n",
            "Wall time: 17min 17s\n"
          ]
        }
      ],
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cHYCDYwhlVLV",
        "outputId": "ba1ae3ba-a1ff-4798-a4b8-9581afedfebb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.980199\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.980397\n",
            "resetting env. episode 6.000000, reward total was -19.000000. running mean: -20.960593\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.950987\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.941477\n",
            "resetting env. episode 9.000000, reward total was -19.000000. running mean: -20.922062\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.912842\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.913713\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.914576\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.905431\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.906376\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.907312\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.898239\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.889257\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.890364\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.891461\n",
            "resetting env. episode 20.000000, reward total was -19.000000. running mean: -20.872546\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.873821\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.865082\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.866432\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.867767\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.869090\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.860399\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.851795\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.843277\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.844844\n",
            "resetting env. episode 30.000000, reward total was -17.000000. running mean: -20.806396\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.808332\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.800248\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.792246\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.784323\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.776480\n",
            "resetting env. episode 36.000000, reward total was -16.000000. running mean: -20.728715\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.731428\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.734114\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.736773\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.729405\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.722111\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.724890\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.727641\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.720365\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.723161\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.725929\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.718670\n",
            "resetting env. episode 48.000000, reward total was -19.000000. running mean: -20.701483\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.704468\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.707424\n",
            "resetting env. episode 51.000000, reward total was -18.000000. running mean: -20.680350\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.683546\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.686711\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.689844\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.692945\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.696016\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.699055\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.692065\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -20.675144\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.678393\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.671609\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.674893\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.678144\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.671362\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.664649\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.658002\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.661422\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.654808\n",
            "resetting env. episode 69.000000, reward total was -19.000000. running mean: -20.638260\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.631877\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.635559\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -20.629203\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.632911\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.636582\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.640216\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.643814\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.647376\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.640902\n",
            "resetting env. episode 79.000000, reward total was -19.000000. running mean: -20.624493\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -20.608248\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.612166\n",
            "resetting env. episode 82.000000, reward total was -18.000000. running mean: -20.586044\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.580183\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.574382\n",
            "resetting env. episode 85.000000, reward total was -19.000000. running mean: -20.558638\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.553051\n",
            "resetting env. episode 87.000000, reward total was -19.000000. running mean: -20.537521\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.542146\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.546724\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.551257\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.555744\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.550187\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.554685\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.559138\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.563547\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.567911\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.572232\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.576510\n",
            "resetting env. episode 99.000000, reward total was -18.000000. running mean: -20.550745\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.555237\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.549685\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.544188\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.538746\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.533359\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.528025\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.532745\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.537418\n",
            "resetting env. episode 108.000000, reward total was -19.000000. running mean: -20.522043\n",
            "resetting env. episode 109.000000, reward total was -19.000000. running mean: -20.506823\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.511755\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.516637\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.511471\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.506356\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.501293\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.506280\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.501217\n",
            "resetting env. episode 117.000000, reward total was -20.000000. running mean: -20.496205\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.501243\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.506230\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.511168\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.516056\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.510896\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.515787\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.520629\n",
            "resetting env. episode 125.000000, reward total was -19.000000. running mean: -20.505423\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.510368\n",
            "resetting env. episode 127.000000, reward total was -18.000000. running mean: -20.485265\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.490412\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.495508\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.490553\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.485647\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.490791\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.495883\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.500924\n",
            "resetting env. episode 135.000000, reward total was -17.000000. running mean: -20.465915\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.461256\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.456643\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.462077\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.457456\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.462881\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.468253\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.463570\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.458934\n",
            "resetting env. episode 144.000000, reward total was -17.000000. running mean: -20.424345\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.430102\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.425801\n",
            "resetting env. episode 147.000000, reward total was -18.000000. running mean: -20.401543\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.397527\n",
            "resetting env. episode 149.000000, reward total was -19.000000. running mean: -20.383552\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.389716\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.395819\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.401861\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.407842\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.403764\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.409726\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.405629\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.411573\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.417457\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.423282\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.419050\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.424859\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.430611\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.426304\n",
            "resetting env. episode 164.000000, reward total was -19.000000. running mean: -20.412041\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.407921\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.403842\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.409803\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.405705\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.411648\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.407532\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.413456\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.409322\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.415229\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.421076\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.416866\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.422697\n",
            "resetting env. episode 177.000000, reward total was -18.000000. running mean: -20.398470\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.404485\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.410440\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.406336\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.412273\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.408150\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.414068\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.419928\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.425728\n",
            "resetting env. episode 186.000000, reward total was -20.000000. running mean: -20.421471\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.427256\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.432984\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.418654\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.424468\n",
            "resetting env. episode 191.000000, reward total was -19.000000. running mean: -20.410223\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.406121\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.412059\n",
            "resetting env. episode 194.000000, reward total was -16.000000. running mean: -20.367939\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.374259\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.370517\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.376812\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.383044\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.389213\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.395321\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.401368\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.397354\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.403381\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.399347\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.405353\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.411300\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.407187\n",
            "resetting env. episode 208.000000, reward total was -17.000000. running mean: -20.373115\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.369384\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.375690\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.381933\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.388114\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.394233\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.390290\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.396387\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.402423\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.398399\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.394415\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.400471\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.406466\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.412402\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.408278\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.414195\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.420053\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.425852\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.431594\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.427278\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.433005\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.438675\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.434288\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.439945\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.445546\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.441091\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.446680\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.452213\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.447691\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.453214\n",
            "resetting env. episode 238.000000, reward total was -19.000000. running mean: -20.438682\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.444295\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.449852\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.445353\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.450900\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.446391\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.451927\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.457408\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.462834\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.458205\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.453623\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.459087\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.464496\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.469851\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.475153\n",
            "resetting env. episode 253.000000, reward total was -19.000000. running mean: -20.460401\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.455797\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.461239\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.466627\n",
            "resetting env. episode 257.000000, reward total was -19.000000. running mean: -20.451960\n",
            "resetting env. episode 258.000000, reward total was -19.000000. running mean: -20.437441\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.443066\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.448636\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.444149\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.449708\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.445211\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.440759\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.446351\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.451888\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.457369\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.452795\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.458267\n",
            "resetting env. episode 270.000000, reward total was -18.000000. running mean: -20.433684\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.429348\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.435054\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.430704\n",
            "resetting env. episode 274.000000, reward total was -19.000000. running mean: -20.416397\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.412233\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.418110\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.423929\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.429690\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.435393\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.441039\n",
            "resetting env. episode 281.000000, reward total was -19.000000. running mean: -20.426629\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.422362\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.418139\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.423957\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.429718\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.435421\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.441066\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.446656\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.442189\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.447767\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.453290\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.458757\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.454169\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.449627\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.455131\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.460580\n",
            "resetting env. episode 297.000000, reward total was -19.000000. running mean: -20.445974\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.451514\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.446999\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.452529\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.458004\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.453424\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.448890\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.434401\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.440057\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.445656\n",
            "resetting env. episode 307.000000, reward total was -19.000000. running mean: -20.431200\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.436888\n",
            "resetting env. episode 309.000000, reward total was -20.000000. running mean: -20.432519\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.438194\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.443812\n",
            "resetting env. episode 312.000000, reward total was -19.000000. running mean: -20.429373\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.435080\n",
            "resetting env. episode 314.000000, reward total was -19.000000. running mean: -20.420729\n",
            "resetting env. episode 315.000000, reward total was -19.000000. running mean: -20.406522\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.402456\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.408432\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.414348\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.410204\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.416102\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.421941\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.427722\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.433444\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.439110\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.444719\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.450272\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.445769\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.441311\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.446898\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.442429\n",
            "resetting env. episode 331.000000, reward total was -18.000000. running mean: -20.418005\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.423825\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.429587\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.435291\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.440938\n",
            "resetting env. episode 336.000000, reward total was -19.000000. running mean: -20.426528\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.432263\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.437941\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.423561\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.429326\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.435032\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.430682\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.426375\n",
            "resetting env. episode 344.000000, reward total was -19.000000. running mean: -20.412111\n",
            "resetting env. episode 345.000000, reward total was -19.000000. running mean: -20.397990\n",
            "resetting env. episode 346.000000, reward total was -18.000000. running mean: -20.374010\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.370270\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.376568\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.382802\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.388974\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.385084\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.391233\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.397321\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.393348\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.399414\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.405420\n",
            "resetting env. episode 357.000000, reward total was -19.000000. running mean: -20.391366\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.397452\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.393478\n",
            "resetting env. episode 360.000000, reward total was -18.000000. running mean: -20.369543\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.375848\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.382089\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.388268\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.394385\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.400442\n",
            "resetting env. episode 366.000000, reward total was -18.000000. running mean: -20.376437\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.372673\n",
            "resetting env. episode 368.000000, reward total was -19.000000. running mean: -20.358946\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.365357\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.371703\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.377986\n",
            "resetting env. episode 372.000000, reward total was -18.000000. running mean: -20.354206\n",
            "resetting env. episode 373.000000, reward total was -18.000000. running mean: -20.330664\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.337357\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.343984\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.340544\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.347139\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.353667\n",
            "resetting env. episode 379.000000, reward total was -19.000000. running mean: -20.340131\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.346729\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.343262\n",
            "resetting env. episode 382.000000, reward total was -19.000000. running mean: -20.329829\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.326531\n",
            "resetting env. episode 384.000000, reward total was -19.000000. running mean: -20.313266\n",
            "resetting env. episode 385.000000, reward total was -19.000000. running mean: -20.300133\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.307132\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.304060\n",
            "resetting env. episode 388.000000, reward total was -19.000000. running mean: -20.291020\n",
            "resetting env. episode 389.000000, reward total was -19.000000. running mean: -20.278110\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.275329\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -20.262575\n",
            "resetting env. episode 392.000000, reward total was -19.000000. running mean: -20.249949\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.257450\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.254876\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.252327\n",
            "resetting env. episode 396.000000, reward total was -17.000000. running mean: -20.219803\n",
            "resetting env. episode 397.000000, reward total was -19.000000. running mean: -20.207605\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.215529\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.223374\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.231140\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.228829\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.236541\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.234175\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.231834\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.229515\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.237220\n",
            "resetting env. episode 407.000000, reward total was -18.000000. running mean: -20.214848\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.222699\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.230472\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.238168\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.235786\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.243428\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.230994\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.238684\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.246297\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.243834\n",
            "resetting env. episode 417.000000, reward total was -20.000000. running mean: -20.241396\n",
            "resetting env. episode 418.000000, reward total was -19.000000. running mean: -20.228982\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.226692\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.234425\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.242081\n",
            "resetting env. episode 422.000000, reward total was -19.000000. running mean: -20.229660\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.227363\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.225090\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.222839\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.220610\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.218404\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.226220\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.233958\n",
            "resetting env. episode 430.000000, reward total was -20.000000. running mean: -20.231619\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.229302\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.237009\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.234639\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.232293\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.239970\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.247570\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.255094\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.262544\n",
            "resetting env. episode 439.000000, reward total was -19.000000. running mean: -20.249918\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.247419\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.254945\n",
            "resetting env. episode 442.000000, reward total was -14.000000. running mean: -20.192395\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.200471\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.208467\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.216382\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.224218\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.231976\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.229656\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.237360\n",
            "resetting env. episode 450.000000, reward total was -17.000000. running mean: -20.204986\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.192936\n",
            "resetting env. episode 452.000000, reward total was -19.000000. running mean: -20.181007\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.179197\n",
            "resetting env. episode 454.000000, reward total was -20.000000. running mean: -20.177405\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.175631\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.183874\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.182036\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.180215\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.188413\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.196529\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.194564\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.202618\n",
            "resetting env. episode 463.000000, reward total was -19.000000. running mean: -20.190592\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.198686\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.206699\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.214632\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.212486\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.220361\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.218157\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.225976\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.233716\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.241379\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.238965\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.246575\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.244110\n",
            "resetting env. episode 476.000000, reward total was -19.000000. running mean: -20.231669\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.239352\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.246958\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.254489\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.261944\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.269324\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.266631\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.273965\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.281225\n",
            "resetting env. episode 485.000000, reward total was -19.000000. running mean: -20.268413\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.275729\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.282972\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.280142\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.277340\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.274567\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.271821\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.279103\n",
            "resetting env. episode 493.000000, reward total was -19.000000. running mean: -20.266312\n",
            "resetting env. episode 494.000000, reward total was -18.000000. running mean: -20.243649\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.251213\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.248700\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.256213\n",
            "resetting env. episode 498.000000, reward total was -20.000000. running mean: -20.253651\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.261115\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.258504\n",
            "CPU times: user 23min 17s, sys: 10min 20s, total: 33min 37s\n",
            "Wall time: 17min 11s\n"
          ]
        }
      ],
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8fheN9DRlWXQ",
        "outputId": "96c62b47-e521-4307-99b5-a110cc604085"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode finished without success, accumulated reward = -7.0\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "play_game(env, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9AxOcQhIsKow",
        "outputId": "3d3d46d2-181d-4805-c814-58b5c332e2a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "resetting env. episode 1.000000, reward total was -18.000000. running mean: -18.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -18.020000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -18.039800\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -18.069402\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -18.098708\n",
            "resetting env. episode 6.000000, reward total was -18.000000. running mean: -18.097721\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -18.126744\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -18.145476\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -18.174021\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -18.202281\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -18.230258\n",
            "resetting env. episode 12.000000, reward total was -19.000000. running mean: -18.237956\n",
            "resetting env. episode 13.000000, reward total was -18.000000. running mean: -18.235576\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -18.263221\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -18.270588\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -18.287882\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -18.305004\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -18.321954\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -18.338734\n",
            "resetting env. episode 20.000000, reward total was -19.000000. running mean: -18.345347\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -18.371893\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -18.398174\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -18.424193\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -18.449951\n",
            "resetting env. episode 25.000000, reward total was -18.000000. running mean: -18.445451\n",
            "resetting env. episode 26.000000, reward total was -19.000000. running mean: -18.450997\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -18.466487\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -18.481822\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -18.507004\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -18.521934\n",
            "resetting env. episode 31.000000, reward total was -19.000000. running mean: -18.526714\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -18.541447\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -18.566033\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -18.590372\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -18.594469\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -18.608524\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -18.632439\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -18.646114\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -18.659653\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -18.673057\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -18.686326\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -18.699463\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -18.722468\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -18.735243\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -18.757891\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -18.770312\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -18.792609\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -18.814683\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -18.826536\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -18.838271\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -18.859888\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -18.871289\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -18.872576\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -18.893850\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -18.914912\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -18.935763\n",
            "resetting env. episode 57.000000, reward total was -19.000000. running mean: -18.936405\n",
            "resetting env. episode 58.000000, reward total was -19.000000. running mean: -18.937041\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -18.957671\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -18.978094\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -18.998313\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.018330\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.038147\n",
            "resetting env. episode 64.000000, reward total was -19.000000. running mean: -19.037765\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -19.057388\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.076814\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -19.096046\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.115085\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -19.123934\n",
            "resetting env. episode 70.000000, reward total was -19.000000. running mean: -19.122695\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -19.141468\n",
            "resetting env. episode 72.000000, reward total was -19.000000. running mean: -19.140053\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.158653\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -19.167066\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -19.185396\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.203542\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -19.221506\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.239291\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -19.256898\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -19.274329\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -19.291586\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.308670\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -19.315583\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -19.322428\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -19.339203\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -19.345811\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.362353\n",
            "resetting env. episode 88.000000, reward total was -19.000000. running mean: -19.358730\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -19.365142\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -19.381491\n",
            "resetting env. episode 91.000000, reward total was -18.000000. running mean: -19.367676\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -19.383999\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -19.390159\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -19.396258\n",
            "resetting env. episode 95.000000, reward total was -19.000000. running mean: -19.392295\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -19.398372\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -19.414388\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -19.420244\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -19.416042\n",
            "resetting env. episode 100.000000, reward total was -19.000000. running mean: -19.411882\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -19.427763\n",
            "resetting env. episode 102.000000, reward total was -19.000000. running mean: -19.423485\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -19.419250\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -19.425058\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -19.430807\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.446499\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -19.462034\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -19.477414\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.492640\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.507713\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -19.522636\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.537410\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -19.542036\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -19.556615\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -19.571049\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -19.585339\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.599485\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -19.613490\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -19.627356\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -19.641082\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -19.654671\n",
            "resetting env. episode 122.000000, reward total was -19.000000. running mean: -19.648124\n",
            "resetting env. episode 123.000000, reward total was -20.000000. running mean: -19.651643\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -19.655127\n",
            "resetting env. episode 125.000000, reward total was -19.000000. running mean: -19.648576\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -19.662090\n",
            "resetting env. episode 127.000000, reward total was -18.000000. running mean: -19.645469\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -19.649014\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -19.652524\n",
            "resetting env. episode 130.000000, reward total was -19.000000. running mean: -19.645999\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -19.649539\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -19.663043\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -19.676413\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -19.689649\n",
            "resetting env. episode 135.000000, reward total was -19.000000. running mean: -19.682752\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -19.695925\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -19.708966\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -19.711876\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -19.724757\n",
            "resetting env. episode 140.000000, reward total was -18.000000. running mean: -19.707510\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -19.710435\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -19.713330\n",
            "resetting env. episode 143.000000, reward total was -19.000000. running mean: -19.706197\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -19.709135\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -19.722044\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -19.724823\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -19.727575\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -19.730299\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -19.732996\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -19.745666\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -19.758210\n",
            "resetting env. episode 152.000000, reward total was -19.000000. running mean: -19.750627\n",
            "resetting env. episode 153.000000, reward total was -20.000000. running mean: -19.753121\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -19.755590\n",
            "resetting env. episode 155.000000, reward total was -15.000000. running mean: -19.708034\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -19.720954\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -19.723744\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -19.736507\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -19.749142\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -19.751650\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -19.764134\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -19.766492\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -19.778827\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -19.791039\n",
            "resetting env. episode 165.000000, reward total was -18.000000. running mean: -19.773129\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -19.775398\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -19.787644\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -19.789767\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -19.801869\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -19.793851\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -19.795912\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -19.797953\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -19.789974\n",
            "resetting env. episode 174.000000, reward total was -19.000000. running mean: -19.782074\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -19.784253\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -19.786411\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -19.798546\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -19.810561\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -19.812455\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -19.814331\n",
            "resetting env. episode 181.000000, reward total was -19.000000. running mean: -19.806188\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -19.818126\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -19.829944\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -19.841645\n",
            "resetting env. episode 185.000000, reward total was -17.000000. running mean: -19.813229\n",
            "resetting env. episode 186.000000, reward total was -19.000000. running mean: -19.805096\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -19.807045\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -19.808975\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -19.820885\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -19.832676\n",
            "resetting env. episode 191.000000, reward total was -19.000000. running mean: -19.824349\n",
            "resetting env. episode 192.000000, reward total was -17.000000. running mean: -19.796106\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -19.788145\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -19.790263\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -19.802361\n",
            "resetting env. episode 196.000000, reward total was -18.000000. running mean: -19.784337\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -19.786494\n",
            "resetting env. episode 198.000000, reward total was -19.000000. running mean: -19.778629\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -19.790843\n",
            "resetting env. episode 200.000000, reward total was -20.000000. running mean: -19.792934\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -19.795005\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -19.807055\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -19.808984\n",
            "resetting env. episode 204.000000, reward total was -17.000000. running mean: -19.780894\n",
            "resetting env. episode 205.000000, reward total was -19.000000. running mean: -19.773085\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -19.775355\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -19.787601\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -19.799725\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -19.811728\n",
            "resetting env. episode 210.000000, reward total was -19.000000. running mean: -19.803611\n",
            "resetting env. episode 211.000000, reward total was -18.000000. running mean: -19.785574\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -19.797719\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -19.799741\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -19.811744\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -19.813627\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -19.825490\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -19.837235\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -19.838863\n",
            "resetting env. episode 219.000000, reward total was -19.000000. running mean: -19.830474\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -19.832170\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -19.843848\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -19.855410\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -19.856855\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -19.858287\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -19.859704\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -19.861107\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -19.872496\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -19.883771\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -19.894933\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -19.895984\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -19.897024\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -19.898054\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -19.899073\n",
            "resetting env. episode 234.000000, reward total was -19.000000. running mean: -19.890083\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -19.901182\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -19.912170\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -19.923048\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -19.923818\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -19.924580\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -19.935334\n",
            "resetting env. episode 241.000000, reward total was -19.000000. running mean: -19.925980\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -19.936721\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -19.937353\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -19.947980\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -19.958500\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -19.968915\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -19.979226\n",
            "resetting env. episode 248.000000, reward total was -18.000000. running mean: -19.959434\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -19.959839\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -19.970241\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -19.980539\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -19.980733\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -19.990926\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -19.991017\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -19.991106\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.001195\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.011183\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.011072\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.010961\n",
            "resetting env. episode 260.000000, reward total was -19.000000. running mean: -20.000851\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.010843\n",
            "resetting env. episode 262.000000, reward total was -19.000000. running mean: -20.000734\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.010727\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.020620\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.030413\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.040109\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.049708\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.059211\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.058619\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.068033\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.067353\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.076679\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.085912\n",
            "resetting env. episode 274.000000, reward total was -19.000000. running mean: -20.075053\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.074303\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.083560\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -20.072724\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.081997\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.091177\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.090265\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.099362\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.108369\n",
            "resetting env. episode 283.000000, reward total was -19.000000. running mean: -20.097285\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.106312\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.115249\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.124097\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.122856\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.131627\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.140311\n",
            "resetting env. episode 290.000000, reward total was -17.000000. running mean: -20.108908\n",
            "resetting env. episode 291.000000, reward total was -17.000000. running mean: -20.077819\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.077040\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.086270\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.085407\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.094553\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.093608\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.102672\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.111645\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.110528\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.119423\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.118229\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.127047\n",
            "resetting env. episode 303.000000, reward total was -17.000000. running mean: -20.095776\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.094818\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -20.083870\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.083032\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.092201\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.101279\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.110266\n",
            "resetting env. episode 310.000000, reward total was -18.000000. running mean: -20.089164\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.098272\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.097289\n",
            "resetting env. episode 313.000000, reward total was -19.000000. running mean: -20.086316\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.095453\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.104499\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.113454\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.122319\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.131096\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.139785\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.148387\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.146903\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.145434\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.153980\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.162440\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.170816\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.169108\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.177417\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.175642\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.183886\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.192047\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.200127\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.198125\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.196144\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.194183\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.192241\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.200318\n",
            "resetting env. episode 337.000000, reward total was -19.000000. running mean: -20.188315\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.196432\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.184468\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.192623\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.190697\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.198790\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.196802\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.194834\n",
            "resetting env. episode 345.000000, reward total was -19.000000. running mean: -20.182886\n",
            "resetting env. episode 346.000000, reward total was -19.000000. running mean: -20.171057\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.179346\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.177553\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.185777\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.183919\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.172080\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.170359\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.168656\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.166969\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.175300\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.183547\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.181711\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.189894\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.187995\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.196115\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.194154\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.202212\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.210190\n",
            "resetting env. episode 364.000000, reward total was -21.000000. running mean: -20.218088\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.225908\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.233648\n",
            "resetting env. episode 367.000000, reward total was -18.000000. running mean: -20.211312\n",
            "resetting env. episode 368.000000, reward total was -18.000000. running mean: -20.189199\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.197307\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.195334\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.193380\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.201447\n",
            "resetting env. episode 373.000000, reward total was -19.000000. running mean: -20.189432\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.197538\n",
            "resetting env. episode 375.000000, reward total was -18.000000. running mean: -20.175563\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.183807\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.191969\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.200049\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.208049\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.215968\n",
            "resetting env. episode 381.000000, reward total was -19.000000. running mean: -20.203808\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.211770\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.209653\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.217556\n",
            "resetting env. episode 385.000000, reward total was -18.000000. running mean: -20.195381\n",
            "resetting env. episode 386.000000, reward total was -17.000000. running mean: -20.163427\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.171793\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.180075\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.188274\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.196391\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -20.184427\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.182583\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.190757\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.188850\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.196961\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.204991\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.212942\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.210812\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.218704\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.206517\n",
            "resetting env. episode 401.000000, reward total was -19.000000. running mean: -20.194452\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.202507\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.200482\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.198477\n",
            "resetting env. episode 405.000000, reward total was -19.000000. running mean: -20.186493\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.194628\n",
            "resetting env. episode 407.000000, reward total was -18.000000. running mean: -20.172681\n",
            "resetting env. episode 408.000000, reward total was -19.000000. running mean: -20.160955\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.159345\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.167752\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.176074\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.174313\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.182570\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.190744\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.188837\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.196949\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.204979\n",
            "resetting env. episode 418.000000, reward total was -20.000000. running mean: -20.202929\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.210900\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.208791\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.216703\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.224536\n",
            "resetting env. episode 423.000000, reward total was -19.000000. running mean: -20.212291\n",
            "resetting env. episode 424.000000, reward total was -20.000000. running mean: -20.210168\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.218066\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.225886\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -20.213627\n",
            "resetting env. episode 428.000000, reward total was -16.000000. running mean: -20.171490\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.179776\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.187978\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.196098\n",
            "resetting env. episode 432.000000, reward total was -19.000000. running mean: -20.184137\n",
            "resetting env. episode 433.000000, reward total was -19.000000. running mean: -20.172296\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.180573\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.188767\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.186879\n",
            "resetting env. episode 437.000000, reward total was -17.000000. running mean: -20.155010\n",
            "resetting env. episode 438.000000, reward total was -18.000000. running mean: -20.133460\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.132126\n",
            "resetting env. episode 440.000000, reward total was -19.000000. running mean: -20.120805\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.119596\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.128400\n",
            "resetting env. episode 443.000000, reward total was -19.000000. running mean: -20.117116\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.115945\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.124786\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.123538\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.132303\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -20.120980\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.119770\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -20.108572\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.117486\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.126312\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.125048\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.133798\n",
            "resetting env. episode 455.000000, reward total was -19.000000. running mean: -20.122460\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.131235\n",
            "resetting env. episode 457.000000, reward total was -18.000000. running mean: -20.109923\n",
            "resetting env. episode 458.000000, reward total was -19.000000. running mean: -20.098824\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.097836\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.106857\n",
            "resetting env. episode 461.000000, reward total was -18.000000. running mean: -20.085789\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.094931\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.103981\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.112942\n",
            "resetting env. episode 465.000000, reward total was -19.000000. running mean: -20.101812\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.100794\n",
            "resetting env. episode 467.000000, reward total was -20.000000. running mean: -20.099786\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.098788\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.097800\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.106822\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.105754\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.114697\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.123550\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.132314\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.140991\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.139581\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.148185\n",
            "resetting env. episode 478.000000, reward total was -17.000000. running mean: -20.116703\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.125536\n",
            "resetting env. episode 480.000000, reward total was -17.000000. running mean: -20.094281\n",
            "resetting env. episode 481.000000, reward total was -19.000000. running mean: -20.083338\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.082505\n",
            "resetting env. episode 483.000000, reward total was -20.000000. running mean: -20.081680\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.080863\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.090054\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.099154\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.098162\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.107181\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.116109\n",
            "resetting env. episode 490.000000, reward total was -19.000000. running mean: -20.104948\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.103898\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.112859\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.111731\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.110613\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.119507\n",
            "resetting env. episode 496.000000, reward total was -17.000000. running mean: -20.088312\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.087429\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -20.076555\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.085789\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.084931\n",
            "resetting env. episode 501.000000, reward total was -20.000000. running mean: -20.084082\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.093241\n",
            "resetting env. episode 503.000000, reward total was -21.000000. running mean: -20.102309\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -20.111286\n",
            "resetting env. episode 505.000000, reward total was -20.000000. running mean: -20.110173\n",
            "resetting env. episode 506.000000, reward total was -21.000000. running mean: -20.119071\n",
            "resetting env. episode 507.000000, reward total was -21.000000. running mean: -20.127880\n",
            "resetting env. episode 508.000000, reward total was -20.000000. running mean: -20.126602\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.135336\n",
            "resetting env. episode 510.000000, reward total was -19.000000. running mean: -20.123982\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -20.122742\n",
            "resetting env. episode 512.000000, reward total was -20.000000. running mean: -20.121515\n",
            "resetting env. episode 513.000000, reward total was -20.000000. running mean: -20.120300\n",
            "resetting env. episode 514.000000, reward total was -20.000000. running mean: -20.119097\n",
            "resetting env. episode 515.000000, reward total was -21.000000. running mean: -20.127906\n",
            "resetting env. episode 516.000000, reward total was -21.000000. running mean: -20.136627\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.145261\n",
            "resetting env. episode 518.000000, reward total was -20.000000. running mean: -20.143808\n",
            "resetting env. episode 519.000000, reward total was -19.000000. running mean: -20.132370\n",
            "resetting env. episode 520.000000, reward total was -20.000000. running mean: -20.131046\n",
            "resetting env. episode 521.000000, reward total was -20.000000. running mean: -20.129736\n",
            "resetting env. episode 522.000000, reward total was -21.000000. running mean: -20.138438\n",
            "resetting env. episode 523.000000, reward total was -20.000000. running mean: -20.137054\n",
            "resetting env. episode 524.000000, reward total was -20.000000. running mean: -20.135683\n",
            "resetting env. episode 525.000000, reward total was -21.000000. running mean: -20.144327\n",
            "resetting env. episode 526.000000, reward total was -19.000000. running mean: -20.132883\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.141554\n",
            "resetting env. episode 528.000000, reward total was -20.000000. running mean: -20.140139\n",
            "resetting env. episode 529.000000, reward total was -21.000000. running mean: -20.148738\n",
            "resetting env. episode 530.000000, reward total was -21.000000. running mean: -20.157250\n",
            "resetting env. episode 531.000000, reward total was -20.000000. running mean: -20.155678\n",
            "resetting env. episode 532.000000, reward total was -21.000000. running mean: -20.164121\n",
            "resetting env. episode 533.000000, reward total was -19.000000. running mean: -20.152480\n",
            "resetting env. episode 534.000000, reward total was -19.000000. running mean: -20.140955\n",
            "resetting env. episode 535.000000, reward total was -20.000000. running mean: -20.139545\n",
            "resetting env. episode 536.000000, reward total was -20.000000. running mean: -20.138150\n",
            "resetting env. episode 537.000000, reward total was -20.000000. running mean: -20.136768\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.145401\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -20.143947\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.152507\n",
            "resetting env. episode 541.000000, reward total was -20.000000. running mean: -20.150982\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.159472\n",
            "resetting env. episode 543.000000, reward total was -19.000000. running mean: -20.147878\n",
            "resetting env. episode 544.000000, reward total was -20.000000. running mean: -20.146399\n",
            "resetting env. episode 545.000000, reward total was -21.000000. running mean: -20.154935\n",
            "resetting env. episode 546.000000, reward total was -19.000000. running mean: -20.143386\n",
            "resetting env. episode 547.000000, reward total was -21.000000. running mean: -20.151952\n",
            "resetting env. episode 548.000000, reward total was -18.000000. running mean: -20.130432\n",
            "resetting env. episode 549.000000, reward total was -21.000000. running mean: -20.139128\n",
            "resetting env. episode 550.000000, reward total was -20.000000. running mean: -20.137737\n",
            "resetting env. episode 551.000000, reward total was -20.000000. running mean: -20.136359\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.144996\n",
            "resetting env. episode 553.000000, reward total was -21.000000. running mean: -20.153546\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -20.162010\n",
            "resetting env. episode 555.000000, reward total was -20.000000. running mean: -20.160390\n",
            "resetting env. episode 556.000000, reward total was -19.000000. running mean: -20.148786\n",
            "resetting env. episode 557.000000, reward total was -21.000000. running mean: -20.157298\n",
            "resetting env. episode 558.000000, reward total was -20.000000. running mean: -20.155725\n",
            "resetting env. episode 559.000000, reward total was -20.000000. running mean: -20.154168\n",
            "resetting env. episode 560.000000, reward total was -19.000000. running mean: -20.142626\n",
            "resetting env. episode 561.000000, reward total was -20.000000. running mean: -20.141200\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -20.149788\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.158290\n",
            "resetting env. episode 564.000000, reward total was -20.000000. running mean: -20.156707\n",
            "resetting env. episode 565.000000, reward total was -20.000000. running mean: -20.155140\n",
            "resetting env. episode 566.000000, reward total was -20.000000. running mean: -20.153589\n",
            "resetting env. episode 567.000000, reward total was -19.000000. running mean: -20.142053\n",
            "resetting env. episode 568.000000, reward total was -20.000000. running mean: -20.140632\n",
            "resetting env. episode 569.000000, reward total was -21.000000. running mean: -20.149226\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.157734\n",
            "resetting env. episode 571.000000, reward total was -21.000000. running mean: -20.166157\n",
            "resetting env. episode 572.000000, reward total was -20.000000. running mean: -20.164495\n",
            "resetting env. episode 573.000000, reward total was -21.000000. running mean: -20.172850\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.181122\n",
            "resetting env. episode 575.000000, reward total was -18.000000. running mean: -20.159310\n",
            "resetting env. episode 576.000000, reward total was -20.000000. running mean: -20.157717\n",
            "resetting env. episode 577.000000, reward total was -19.000000. running mean: -20.146140\n",
            "resetting env. episode 578.000000, reward total was -20.000000. running mean: -20.144679\n",
            "resetting env. episode 579.000000, reward total was -21.000000. running mean: -20.153232\n",
            "resetting env. episode 580.000000, reward total was -21.000000. running mean: -20.161700\n",
            "resetting env. episode 581.000000, reward total was -19.000000. running mean: -20.150083\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.158582\n",
            "resetting env. episode 583.000000, reward total was -20.000000. running mean: -20.156996\n",
            "resetting env. episode 584.000000, reward total was -20.000000. running mean: -20.155426\n",
            "resetting env. episode 585.000000, reward total was -20.000000. running mean: -20.153872\n",
            "resetting env. episode 586.000000, reward total was -20.000000. running mean: -20.152333\n",
            "resetting env. episode 587.000000, reward total was -20.000000. running mean: -20.150810\n",
            "resetting env. episode 588.000000, reward total was -21.000000. running mean: -20.159302\n",
            "resetting env. episode 589.000000, reward total was -21.000000. running mean: -20.167708\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.176031\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.184271\n",
            "resetting env. episode 592.000000, reward total was -20.000000. running mean: -20.182428\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -20.190604\n",
            "resetting env. episode 594.000000, reward total was -21.000000. running mean: -20.198698\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.206711\n",
            "resetting env. episode 596.000000, reward total was -20.000000. running mean: -20.204644\n",
            "resetting env. episode 597.000000, reward total was -21.000000. running mean: -20.212598\n",
            "resetting env. episode 598.000000, reward total was -21.000000. running mean: -20.220472\n",
            "resetting env. episode 599.000000, reward total was -20.000000. running mean: -20.218267\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.226084\n",
            "resetting env. episode 601.000000, reward total was -21.000000. running mean: -20.233823\n",
            "resetting env. episode 602.000000, reward total was -17.000000. running mean: -20.201485\n",
            "resetting env. episode 603.000000, reward total was -21.000000. running mean: -20.209470\n",
            "resetting env. episode 604.000000, reward total was -20.000000. running mean: -20.207376\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.215302\n",
            "resetting env. episode 606.000000, reward total was -19.000000. running mean: -20.203149\n",
            "resetting env. episode 607.000000, reward total was -21.000000. running mean: -20.211117\n",
            "resetting env. episode 608.000000, reward total was -20.000000. running mean: -20.209006\n",
            "resetting env. episode 609.000000, reward total was -19.000000. running mean: -20.196916\n",
            "resetting env. episode 610.000000, reward total was -17.000000. running mean: -20.164947\n",
            "resetting env. episode 611.000000, reward total was -19.000000. running mean: -20.153297\n",
            "resetting env. episode 612.000000, reward total was -20.000000. running mean: -20.151764\n",
            "resetting env. episode 613.000000, reward total was -20.000000. running mean: -20.150247\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.158744\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -20.167157\n",
            "resetting env. episode 616.000000, reward total was -19.000000. running mean: -20.155485\n",
            "resetting env. episode 617.000000, reward total was -21.000000. running mean: -20.163930\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.172291\n",
            "resetting env. episode 619.000000, reward total was -21.000000. running mean: -20.180568\n",
            "resetting env. episode 620.000000, reward total was -21.000000. running mean: -20.188763\n",
            "resetting env. episode 621.000000, reward total was -20.000000. running mean: -20.186875\n",
            "resetting env. episode 622.000000, reward total was -20.000000. running mean: -20.185006\n",
            "resetting env. episode 623.000000, reward total was -21.000000. running mean: -20.193156\n",
            "resetting env. episode 624.000000, reward total was -20.000000. running mean: -20.191225\n",
            "resetting env. episode 625.000000, reward total was -20.000000. running mean: -20.189312\n",
            "resetting env. episode 626.000000, reward total was -21.000000. running mean: -20.197419\n",
            "resetting env. episode 627.000000, reward total was -21.000000. running mean: -20.205445\n",
            "resetting env. episode 628.000000, reward total was -20.000000. running mean: -20.203391\n",
            "resetting env. episode 629.000000, reward total was -20.000000. running mean: -20.201357\n",
            "resetting env. episode 630.000000, reward total was -20.000000. running mean: -20.199343\n",
            "resetting env. episode 631.000000, reward total was -19.000000. running mean: -20.187350\n",
            "resetting env. episode 632.000000, reward total was -21.000000. running mean: -20.195476\n",
            "resetting env. episode 633.000000, reward total was -20.000000. running mean: -20.193521\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -20.201586\n",
            "resetting env. episode 635.000000, reward total was -20.000000. running mean: -20.199570\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.207575\n",
            "resetting env. episode 637.000000, reward total was -20.000000. running mean: -20.205499\n",
            "resetting env. episode 638.000000, reward total was -21.000000. running mean: -20.213444\n",
            "resetting env. episode 639.000000, reward total was -19.000000. running mean: -20.201309\n",
            "resetting env. episode 640.000000, reward total was -20.000000. running mean: -20.199296\n",
            "resetting env. episode 641.000000, reward total was -20.000000. running mean: -20.197303\n",
            "resetting env. episode 642.000000, reward total was -20.000000. running mean: -20.195330\n",
            "resetting env. episode 643.000000, reward total was -21.000000. running mean: -20.203377\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.211343\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.219230\n",
            "resetting env. episode 646.000000, reward total was -21.000000. running mean: -20.227038\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.234767\n",
            "resetting env. episode 648.000000, reward total was -18.000000. running mean: -20.212419\n",
            "resetting env. episode 649.000000, reward total was -20.000000. running mean: -20.210295\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.218192\n",
            "resetting env. episode 651.000000, reward total was -19.000000. running mean: -20.206010\n",
            "resetting env. episode 652.000000, reward total was -19.000000. running mean: -20.193950\n",
            "resetting env. episode 653.000000, reward total was -21.000000. running mean: -20.202011\n",
            "resetting env. episode 654.000000, reward total was -19.000000. running mean: -20.189991\n",
            "resetting env. episode 655.000000, reward total was -21.000000. running mean: -20.198091\n",
            "resetting env. episode 656.000000, reward total was -20.000000. running mean: -20.196110\n",
            "resetting env. episode 657.000000, reward total was -21.000000. running mean: -20.204149\n",
            "resetting env. episode 658.000000, reward total was -19.000000. running mean: -20.192107\n",
            "resetting env. episode 659.000000, reward total was -20.000000. running mean: -20.190186\n",
            "resetting env. episode 660.000000, reward total was -20.000000. running mean: -20.188284\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -20.196402\n",
            "resetting env. episode 662.000000, reward total was -17.000000. running mean: -20.164438\n",
            "resetting env. episode 663.000000, reward total was -19.000000. running mean: -20.152793\n",
            "resetting env. episode 664.000000, reward total was -18.000000. running mean: -20.131265\n",
            "resetting env. episode 665.000000, reward total was -20.000000. running mean: -20.129953\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -20.138653\n",
            "resetting env. episode 667.000000, reward total was -21.000000. running mean: -20.147267\n",
            "resetting env. episode 668.000000, reward total was -21.000000. running mean: -20.155794\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.164236\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.172594\n",
            "resetting env. episode 671.000000, reward total was -20.000000. running mean: -20.170868\n",
            "resetting env. episode 672.000000, reward total was -20.000000. running mean: -20.169159\n",
            "resetting env. episode 673.000000, reward total was -21.000000. running mean: -20.177467\n",
            "resetting env. episode 674.000000, reward total was -18.000000. running mean: -20.155693\n",
            "resetting env. episode 675.000000, reward total was -20.000000. running mean: -20.154136\n",
            "resetting env. episode 676.000000, reward total was -21.000000. running mean: -20.162594\n",
            "resetting env. episode 677.000000, reward total was -19.000000. running mean: -20.150968\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -20.159459\n",
            "resetting env. episode 679.000000, reward total was -20.000000. running mean: -20.157864\n",
            "resetting env. episode 680.000000, reward total was -20.000000. running mean: -20.156286\n",
            "resetting env. episode 681.000000, reward total was -21.000000. running mean: -20.164723\n",
            "resetting env. episode 682.000000, reward total was -18.000000. running mean: -20.143075\n",
            "resetting env. episode 683.000000, reward total was -19.000000. running mean: -20.131645\n",
            "resetting env. episode 684.000000, reward total was -19.000000. running mean: -20.120328\n",
            "resetting env. episode 685.000000, reward total was -20.000000. running mean: -20.119125\n",
            "resetting env. episode 686.000000, reward total was -20.000000. running mean: -20.117934\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -20.116754\n",
            "resetting env. episode 688.000000, reward total was -20.000000. running mean: -20.115587\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.124431\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -20.123187\n",
            "resetting env. episode 691.000000, reward total was -19.000000. running mean: -20.111955\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -20.120835\n",
            "resetting env. episode 693.000000, reward total was -20.000000. running mean: -20.119627\n",
            "resetting env. episode 694.000000, reward total was -21.000000. running mean: -20.128431\n",
            "resetting env. episode 695.000000, reward total was -21.000000. running mean: -20.137146\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -20.145775\n",
            "resetting env. episode 697.000000, reward total was -21.000000. running mean: -20.154317\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -20.162774\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.171146\n",
            "resetting env. episode 700.000000, reward total was -21.000000. running mean: -20.179435\n",
            "resetting env. episode 701.000000, reward total was -20.000000. running mean: -20.177640\n",
            "resetting env. episode 702.000000, reward total was -20.000000. running mean: -20.175864\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -20.184105\n",
            "resetting env. episode 704.000000, reward total was -21.000000. running mean: -20.192264\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -20.190342\n",
            "resetting env. episode 706.000000, reward total was -20.000000. running mean: -20.188438\n",
            "resetting env. episode 707.000000, reward total was -21.000000. running mean: -20.196554\n",
            "resetting env. episode 708.000000, reward total was -21.000000. running mean: -20.204588\n",
            "resetting env. episode 709.000000, reward total was -19.000000. running mean: -20.192542\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -20.200617\n",
            "resetting env. episode 711.000000, reward total was -19.000000. running mean: -20.188611\n",
            "resetting env. episode 712.000000, reward total was -21.000000. running mean: -20.196725\n",
            "resetting env. episode 713.000000, reward total was -20.000000. running mean: -20.194757\n",
            "resetting env. episode 714.000000, reward total was -19.000000. running mean: -20.182810\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.190982\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.199072\n",
            "resetting env. episode 717.000000, reward total was -21.000000. running mean: -20.207081\n",
            "resetting env. episode 718.000000, reward total was -21.000000. running mean: -20.215010\n",
            "resetting env. episode 719.000000, reward total was -20.000000. running mean: -20.212860\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -20.220732\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.228524\n",
            "resetting env. episode 722.000000, reward total was -21.000000. running mean: -20.236239\n",
            "resetting env. episode 723.000000, reward total was -19.000000. running mean: -20.223877\n",
            "resetting env. episode 724.000000, reward total was -19.000000. running mean: -20.211638\n",
            "resetting env. episode 725.000000, reward total was -18.000000. running mean: -20.189522\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -20.187626\n",
            "resetting env. episode 727.000000, reward total was -21.000000. running mean: -20.195750\n",
            "resetting env. episode 728.000000, reward total was -20.000000. running mean: -20.193793\n",
            "resetting env. episode 729.000000, reward total was -18.000000. running mean: -20.171855\n",
            "resetting env. episode 730.000000, reward total was -21.000000. running mean: -20.180136\n",
            "resetting env. episode 731.000000, reward total was -18.000000. running mean: -20.158335\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.166751\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.175084\n",
            "resetting env. episode 734.000000, reward total was -20.000000. running mean: -20.173333\n",
            "resetting env. episode 735.000000, reward total was -20.000000. running mean: -20.171600\n",
            "resetting env. episode 736.000000, reward total was -21.000000. running mean: -20.179884\n",
            "resetting env. episode 737.000000, reward total was -18.000000. running mean: -20.158085\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.166504\n",
            "resetting env. episode 739.000000, reward total was -21.000000. running mean: -20.174839\n",
            "resetting env. episode 740.000000, reward total was -21.000000. running mean: -20.183091\n",
            "resetting env. episode 741.000000, reward total was -20.000000. running mean: -20.181260\n",
            "resetting env. episode 742.000000, reward total was -21.000000. running mean: -20.189447\n",
            "resetting env. episode 743.000000, reward total was -21.000000. running mean: -20.197553\n",
            "resetting env. episode 744.000000, reward total was -21.000000. running mean: -20.205577\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -20.213521\n",
            "resetting env. episode 746.000000, reward total was -21.000000. running mean: -20.221386\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.229172\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -20.236881\n",
            "resetting env. episode 749.000000, reward total was -20.000000. running mean: -20.234512\n",
            "resetting env. episode 750.000000, reward total was -20.000000. running mean: -20.232167\n",
            "resetting env. episode 751.000000, reward total was -21.000000. running mean: -20.239845\n",
            "resetting env. episode 752.000000, reward total was -20.000000. running mean: -20.237447\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.245072\n",
            "resetting env. episode 754.000000, reward total was -21.000000. running mean: -20.252621\n",
            "resetting env. episode 755.000000, reward total was -19.000000. running mean: -20.240095\n",
            "resetting env. episode 756.000000, reward total was -20.000000. running mean: -20.237694\n",
            "resetting env. episode 757.000000, reward total was -20.000000. running mean: -20.235317\n",
            "resetting env. episode 758.000000, reward total was -18.000000. running mean: -20.212964\n",
            "resetting env. episode 759.000000, reward total was -20.000000. running mean: -20.210834\n",
            "resetting env. episode 760.000000, reward total was -21.000000. running mean: -20.218726\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -20.226539\n",
            "resetting env. episode 762.000000, reward total was -21.000000. running mean: -20.234273\n",
            "resetting env. episode 763.000000, reward total was -20.000000. running mean: -20.231931\n",
            "resetting env. episode 764.000000, reward total was -20.000000. running mean: -20.229611\n",
            "resetting env. episode 765.000000, reward total was -19.000000. running mean: -20.217315\n",
            "resetting env. episode 766.000000, reward total was -21.000000. running mean: -20.225142\n",
            "resetting env. episode 767.000000, reward total was -20.000000. running mean: -20.222891\n",
            "resetting env. episode 768.000000, reward total was -20.000000. running mean: -20.220662\n",
            "resetting env. episode 769.000000, reward total was -19.000000. running mean: -20.208455\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -20.216371\n",
            "resetting env. episode 771.000000, reward total was -19.000000. running mean: -20.204207\n",
            "resetting env. episode 772.000000, reward total was -21.000000. running mean: -20.212165\n",
            "resetting env. episode 773.000000, reward total was -21.000000. running mean: -20.220043\n",
            "resetting env. episode 774.000000, reward total was -20.000000. running mean: -20.217843\n",
            "resetting env. episode 775.000000, reward total was -20.000000. running mean: -20.215664\n",
            "resetting env. episode 776.000000, reward total was -20.000000. running mean: -20.213508\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -20.221373\n",
            "resetting env. episode 778.000000, reward total was -20.000000. running mean: -20.219159\n",
            "resetting env. episode 779.000000, reward total was -20.000000. running mean: -20.216967\n",
            "resetting env. episode 780.000000, reward total was -20.000000. running mean: -20.214798\n",
            "resetting env. episode 781.000000, reward total was -21.000000. running mean: -20.222650\n",
            "resetting env. episode 782.000000, reward total was -20.000000. running mean: -20.220423\n",
            "resetting env. episode 783.000000, reward total was -21.000000. running mean: -20.228219\n",
            "resetting env. episode 784.000000, reward total was -20.000000. running mean: -20.225937\n",
            "resetting env. episode 785.000000, reward total was -20.000000. running mean: -20.223677\n",
            "resetting env. episode 786.000000, reward total was -21.000000. running mean: -20.231441\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -20.239126\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -20.246735\n",
            "resetting env. episode 789.000000, reward total was -21.000000. running mean: -20.254268\n",
            "resetting env. episode 790.000000, reward total was -21.000000. running mean: -20.261725\n",
            "resetting env. episode 791.000000, reward total was -21.000000. running mean: -20.269108\n",
            "resetting env. episode 792.000000, reward total was -18.000000. running mean: -20.246417\n",
            "resetting env. episode 793.000000, reward total was -20.000000. running mean: -20.243952\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.251513\n",
            "resetting env. episode 795.000000, reward total was -19.000000. running mean: -20.238998\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -20.246608\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -20.254142\n",
            "resetting env. episode 798.000000, reward total was -21.000000. running mean: -20.261600\n",
            "resetting env. episode 799.000000, reward total was -20.000000. running mean: -20.258984\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.266394\n",
            "resetting env. episode 801.000000, reward total was -20.000000. running mean: -20.263731\n",
            "resetting env. episode 802.000000, reward total was -20.000000. running mean: -20.261093\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.268482\n",
            "resetting env. episode 804.000000, reward total was -18.000000. running mean: -20.245797\n",
            "resetting env. episode 805.000000, reward total was -21.000000. running mean: -20.253339\n",
            "resetting env. episode 806.000000, reward total was -21.000000. running mean: -20.260806\n",
            "resetting env. episode 807.000000, reward total was -21.000000. running mean: -20.268198\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.275516\n",
            "resetting env. episode 809.000000, reward total was -19.000000. running mean: -20.262761\n",
            "resetting env. episode 810.000000, reward total was -20.000000. running mean: -20.260133\n",
            "resetting env. episode 811.000000, reward total was -21.000000. running mean: -20.267532\n",
            "resetting env. episode 812.000000, reward total was -20.000000. running mean: -20.264857\n",
            "resetting env. episode 813.000000, reward total was -21.000000. running mean: -20.272208\n",
            "resetting env. episode 814.000000, reward total was -19.000000. running mean: -20.259486\n",
            "resetting env. episode 815.000000, reward total was -21.000000. running mean: -20.266891\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.274222\n",
            "resetting env. episode 817.000000, reward total was -19.000000. running mean: -20.261480\n",
            "resetting env. episode 818.000000, reward total was -21.000000. running mean: -20.268865\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -20.276177\n",
            "resetting env. episode 820.000000, reward total was -20.000000. running mean: -20.273415\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -20.280681\n",
            "resetting env. episode 822.000000, reward total was -20.000000. running mean: -20.277874\n",
            "resetting env. episode 823.000000, reward total was -20.000000. running mean: -20.275095\n",
            "resetting env. episode 824.000000, reward total was -21.000000. running mean: -20.282344\n",
            "resetting env. episode 825.000000, reward total was -20.000000. running mean: -20.279521\n",
            "resetting env. episode 826.000000, reward total was -20.000000. running mean: -20.276725\n",
            "resetting env. episode 827.000000, reward total was -20.000000. running mean: -20.273958\n",
            "resetting env. episode 828.000000, reward total was -20.000000. running mean: -20.271219\n",
            "resetting env. episode 829.000000, reward total was -20.000000. running mean: -20.268506\n",
            "resetting env. episode 830.000000, reward total was -19.000000. running mean: -20.255821\n",
            "resetting env. episode 831.000000, reward total was -21.000000. running mean: -20.263263\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -20.270631\n",
            "resetting env. episode 833.000000, reward total was -20.000000. running mean: -20.267924\n",
            "resetting env. episode 834.000000, reward total was -21.000000. running mean: -20.275245\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.282493\n",
            "resetting env. episode 836.000000, reward total was -21.000000. running mean: -20.289668\n",
            "resetting env. episode 837.000000, reward total was -21.000000. running mean: -20.296771\n",
            "resetting env. episode 838.000000, reward total was -21.000000. running mean: -20.303803\n",
            "resetting env. episode 839.000000, reward total was -20.000000. running mean: -20.300765\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -20.307758\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -20.314680\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -20.321533\n",
            "resetting env. episode 843.000000, reward total was -21.000000. running mean: -20.328318\n",
            "resetting env. episode 844.000000, reward total was -21.000000. running mean: -20.335035\n",
            "resetting env. episode 845.000000, reward total was -18.000000. running mean: -20.311684\n",
            "resetting env. episode 846.000000, reward total was -21.000000. running mean: -20.318567\n",
            "resetting env. episode 847.000000, reward total was -19.000000. running mean: -20.305382\n",
            "resetting env. episode 848.000000, reward total was -21.000000. running mean: -20.312328\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -20.319205\n",
            "resetting env. episode 850.000000, reward total was -20.000000. running mean: -20.316013\n",
            "resetting env. episode 851.000000, reward total was -21.000000. running mean: -20.322853\n",
            "resetting env. episode 852.000000, reward total was -20.000000. running mean: -20.319624\n",
            "resetting env. episode 853.000000, reward total was -19.000000. running mean: -20.306428\n",
            "resetting env. episode 854.000000, reward total was -21.000000. running mean: -20.313363\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.320230\n",
            "resetting env. episode 856.000000, reward total was -21.000000. running mean: -20.327028\n",
            "resetting env. episode 857.000000, reward total was -20.000000. running mean: -20.323757\n",
            "resetting env. episode 858.000000, reward total was -21.000000. running mean: -20.330520\n",
            "resetting env. episode 859.000000, reward total was -20.000000. running mean: -20.327215\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -20.333942\n",
            "resetting env. episode 861.000000, reward total was -20.000000. running mean: -20.330603\n",
            "resetting env. episode 862.000000, reward total was -20.000000. running mean: -20.327297\n",
            "resetting env. episode 863.000000, reward total was -19.000000. running mean: -20.314024\n",
            "resetting env. episode 864.000000, reward total was -20.000000. running mean: -20.310884\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -20.317775\n",
            "resetting env. episode 866.000000, reward total was -20.000000. running mean: -20.314597\n",
            "resetting env. episode 867.000000, reward total was -18.000000. running mean: -20.291451\n",
            "resetting env. episode 868.000000, reward total was -21.000000. running mean: -20.298537\n",
            "resetting env. episode 869.000000, reward total was -20.000000. running mean: -20.295551\n",
            "resetting env. episode 870.000000, reward total was -20.000000. running mean: -20.292596\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -20.299670\n",
            "resetting env. episode 872.000000, reward total was -20.000000. running mean: -20.296673\n",
            "resetting env. episode 873.000000, reward total was -20.000000. running mean: -20.293706\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -20.300769\n",
            "resetting env. episode 875.000000, reward total was -21.000000. running mean: -20.307762\n",
            "resetting env. episode 876.000000, reward total was -20.000000. running mean: -20.304684\n",
            "resetting env. episode 877.000000, reward total was -20.000000. running mean: -20.301637\n",
            "resetting env. episode 878.000000, reward total was -21.000000. running mean: -20.308621\n",
            "resetting env. episode 879.000000, reward total was -20.000000. running mean: -20.305535\n",
            "resetting env. episode 880.000000, reward total was -19.000000. running mean: -20.292479\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.299554\n",
            "resetting env. episode 882.000000, reward total was -20.000000. running mean: -20.296559\n",
            "resetting env. episode 883.000000, reward total was -21.000000. running mean: -20.303593\n",
            "resetting env. episode 884.000000, reward total was -19.000000. running mean: -20.290557\n",
            "resetting env. episode 885.000000, reward total was -19.000000. running mean: -20.277652\n",
            "resetting env. episode 886.000000, reward total was -18.000000. running mean: -20.254875\n",
            "resetting env. episode 887.000000, reward total was -21.000000. running mean: -20.262327\n",
            "resetting env. episode 888.000000, reward total was -20.000000. running mean: -20.259703\n",
            "resetting env. episode 889.000000, reward total was -20.000000. running mean: -20.257106\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -20.264535\n",
            "resetting env. episode 891.000000, reward total was -21.000000. running mean: -20.271890\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -20.279171\n",
            "resetting env. episode 893.000000, reward total was -20.000000. running mean: -20.276379\n",
            "resetting env. episode 894.000000, reward total was -19.000000. running mean: -20.263615\n",
            "resetting env. episode 895.000000, reward total was -20.000000. running mean: -20.260979\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -20.268369\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.275686\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -20.282929\n",
            "resetting env. episode 899.000000, reward total was -19.000000. running mean: -20.270100\n",
            "resetting env. episode 900.000000, reward total was -18.000000. running mean: -20.247399\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -20.254925\n",
            "resetting env. episode 902.000000, reward total was -19.000000. running mean: -20.242375\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -20.249952\n",
            "resetting env. episode 904.000000, reward total was -21.000000. running mean: -20.257452\n",
            "resetting env. episode 905.000000, reward total was -21.000000. running mean: -20.264878\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -20.272229\n",
            "resetting env. episode 907.000000, reward total was -21.000000. running mean: -20.279507\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -20.286711\n",
            "resetting env. episode 909.000000, reward total was -20.000000. running mean: -20.283844\n",
            "resetting env. episode 910.000000, reward total was -19.000000. running mean: -20.271006\n",
            "resetting env. episode 911.000000, reward total was -19.000000. running mean: -20.258296\n",
            "resetting env. episode 912.000000, reward total was -18.000000. running mean: -20.235713\n",
            "resetting env. episode 913.000000, reward total was -19.000000. running mean: -20.223356\n",
            "resetting env. episode 914.000000, reward total was -20.000000. running mean: -20.221122\n",
            "resetting env. episode 915.000000, reward total was -21.000000. running mean: -20.228911\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -20.236622\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -20.244256\n",
            "resetting env. episode 918.000000, reward total was -21.000000. running mean: -20.251813\n",
            "resetting env. episode 919.000000, reward total was -19.000000. running mean: -20.239295\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -20.246902\n",
            "resetting env. episode 921.000000, reward total was -19.000000. running mean: -20.234433\n",
            "resetting env. episode 922.000000, reward total was -21.000000. running mean: -20.242089\n",
            "resetting env. episode 923.000000, reward total was -20.000000. running mean: -20.239668\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.247271\n",
            "resetting env. episode 925.000000, reward total was -19.000000. running mean: -20.234798\n",
            "resetting env. episode 926.000000, reward total was -20.000000. running mean: -20.232450\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -20.240126\n",
            "resetting env. episode 928.000000, reward total was -21.000000. running mean: -20.247725\n",
            "resetting env. episode 929.000000, reward total was -20.000000. running mean: -20.245247\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -20.252795\n",
            "resetting env. episode 931.000000, reward total was -21.000000. running mean: -20.260267\n",
            "resetting env. episode 932.000000, reward total was -20.000000. running mean: -20.257664\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -20.265088\n",
            "resetting env. episode 934.000000, reward total was -19.000000. running mean: -20.252437\n",
            "resetting env. episode 935.000000, reward total was -20.000000. running mean: -20.249912\n",
            "resetting env. episode 936.000000, reward total was -19.000000. running mean: -20.237413\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -20.235039\n",
            "resetting env. episode 938.000000, reward total was -19.000000. running mean: -20.222689\n",
            "resetting env. episode 939.000000, reward total was -21.000000. running mean: -20.230462\n",
            "resetting env. episode 940.000000, reward total was -19.000000. running mean: -20.218157\n",
            "resetting env. episode 941.000000, reward total was -19.000000. running mean: -20.205976\n",
            "resetting env. episode 942.000000, reward total was -19.000000. running mean: -20.193916\n",
            "resetting env. episode 943.000000, reward total was -21.000000. running mean: -20.201977\n",
            "resetting env. episode 944.000000, reward total was -20.000000. running mean: -20.199957\n",
            "resetting env. episode 945.000000, reward total was -20.000000. running mean: -20.197957\n",
            "resetting env. episode 946.000000, reward total was -21.000000. running mean: -20.205978\n",
            "resetting env. episode 947.000000, reward total was -19.000000. running mean: -20.193918\n",
            "resetting env. episode 948.000000, reward total was -20.000000. running mean: -20.191979\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -20.200059\n",
            "resetting env. episode 950.000000, reward total was -19.000000. running mean: -20.188059\n",
            "resetting env. episode 951.000000, reward total was -18.000000. running mean: -20.166178\n",
            "resetting env. episode 952.000000, reward total was -20.000000. running mean: -20.164516\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -20.172871\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -20.181142\n",
            "resetting env. episode 955.000000, reward total was -20.000000. running mean: -20.179331\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -20.187538\n",
            "resetting env. episode 957.000000, reward total was -20.000000. running mean: -20.185662\n",
            "resetting env. episode 958.000000, reward total was -21.000000. running mean: -20.193806\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -20.201868\n",
            "resetting env. episode 960.000000, reward total was -21.000000. running mean: -20.209849\n",
            "resetting env. episode 961.000000, reward total was -18.000000. running mean: -20.187750\n",
            "resetting env. episode 962.000000, reward total was -21.000000. running mean: -20.195873\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -20.203914\n",
            "resetting env. episode 964.000000, reward total was -21.000000. running mean: -20.211875\n",
            "resetting env. episode 965.000000, reward total was -20.000000. running mean: -20.209756\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.217659\n",
            "resetting env. episode 967.000000, reward total was -21.000000. running mean: -20.225482\n",
            "resetting env. episode 968.000000, reward total was -19.000000. running mean: -20.213227\n",
            "resetting env. episode 969.000000, reward total was -20.000000. running mean: -20.211095\n",
            "resetting env. episode 970.000000, reward total was -21.000000. running mean: -20.218984\n",
            "resetting env. episode 971.000000, reward total was -20.000000. running mean: -20.216794\n",
            "resetting env. episode 972.000000, reward total was -21.000000. running mean: -20.224626\n",
            "resetting env. episode 973.000000, reward total was -21.000000. running mean: -20.232380\n",
            "resetting env. episode 974.000000, reward total was -21.000000. running mean: -20.240056\n",
            "resetting env. episode 975.000000, reward total was -19.000000. running mean: -20.227656\n",
            "resetting env. episode 976.000000, reward total was -19.000000. running mean: -20.215379\n",
            "resetting env. episode 977.000000, reward total was -21.000000. running mean: -20.223225\n",
            "resetting env. episode 978.000000, reward total was -20.000000. running mean: -20.220993\n",
            "resetting env. episode 979.000000, reward total was -18.000000. running mean: -20.198783\n",
            "resetting env. episode 980.000000, reward total was -21.000000. running mean: -20.206795\n",
            "resetting env. episode 981.000000, reward total was -20.000000. running mean: -20.204727\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.212680\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.220553\n",
            "resetting env. episode 984.000000, reward total was -18.000000. running mean: -20.198348\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.206364\n",
            "resetting env. episode 986.000000, reward total was -20.000000. running mean: -20.204301\n",
            "resetting env. episode 987.000000, reward total was -19.000000. running mean: -20.192258\n",
            "resetting env. episode 988.000000, reward total was -21.000000. running mean: -20.200335\n",
            "resetting env. episode 989.000000, reward total was -21.000000. running mean: -20.208332\n",
            "resetting env. episode 990.000000, reward total was -19.000000. running mean: -20.196248\n",
            "resetting env. episode 991.000000, reward total was -20.000000. running mean: -20.194286\n",
            "resetting env. episode 992.000000, reward total was -21.000000. running mean: -20.202343\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -20.210320\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.218216\n",
            "resetting env. episode 995.000000, reward total was -21.000000. running mean: -20.226034\n",
            "resetting env. episode 996.000000, reward total was -20.000000. running mean: -20.223774\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -20.231536\n",
            "resetting env. episode 998.000000, reward total was -21.000000. running mean: -20.239221\n",
            "resetting env. episode 999.000000, reward total was -20.000000. running mean: -20.236829\n",
            "resetting env. episode 1000.000000, reward total was -18.000000. running mean: -20.214460\n",
            "resetting env. episode 1001.000000, reward total was -20.000000. running mean: -20.212316\n",
            "resetting env. episode 1002.000000, reward total was -19.000000. running mean: -20.200193\n",
            "resetting env. episode 1003.000000, reward total was -20.000000. running mean: -20.198191\n",
            "resetting env. episode 1004.000000, reward total was -19.000000. running mean: -20.186209\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.194347\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -20.202403\n",
            "resetting env. episode 1007.000000, reward total was -19.000000. running mean: -20.190379\n",
            "resetting env. episode 1008.000000, reward total was -20.000000. running mean: -20.188475\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -20.196591\n",
            "resetting env. episode 1010.000000, reward total was -20.000000. running mean: -20.194625\n",
            "resetting env. episode 1011.000000, reward total was -20.000000. running mean: -20.192678\n",
            "resetting env. episode 1012.000000, reward total was -21.000000. running mean: -20.200752\n",
            "resetting env. episode 1013.000000, reward total was -19.000000. running mean: -20.188744\n",
            "resetting env. episode 1014.000000, reward total was -17.000000. running mean: -20.156857\n",
            "resetting env. episode 1015.000000, reward total was -21.000000. running mean: -20.165288\n",
            "resetting env. episode 1016.000000, reward total was -19.000000. running mean: -20.153635\n",
            "resetting env. episode 1017.000000, reward total was -21.000000. running mean: -20.162099\n",
            "resetting env. episode 1018.000000, reward total was -19.000000. running mean: -20.150478\n",
            "resetting env. episode 1019.000000, reward total was -21.000000. running mean: -20.158973\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -20.167383\n",
            "resetting env. episode 1021.000000, reward total was -21.000000. running mean: -20.175710\n",
            "resetting env. episode 1022.000000, reward total was -20.000000. running mean: -20.173952\n",
            "resetting env. episode 1023.000000, reward total was -20.000000. running mean: -20.172213\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.180491\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -20.188686\n",
            "resetting env. episode 1026.000000, reward total was -20.000000. running mean: -20.186799\n",
            "resetting env. episode 1027.000000, reward total was -20.000000. running mean: -20.184931\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -20.193082\n",
            "resetting env. episode 1029.000000, reward total was -21.000000. running mean: -20.201151\n",
            "resetting env. episode 1030.000000, reward total was -20.000000. running mean: -20.199139\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.207148\n",
            "resetting env. episode 1032.000000, reward total was -17.000000. running mean: -20.175077\n",
            "resetting env. episode 1033.000000, reward total was -20.000000. running mean: -20.173326\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -20.181592\n",
            "resetting env. episode 1035.000000, reward total was -21.000000. running mean: -20.189777\n",
            "resetting env. episode 1036.000000, reward total was -20.000000. running mean: -20.187879\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.196000\n",
            "resetting env. episode 1038.000000, reward total was -21.000000. running mean: -20.204040\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.212000\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -20.219880\n",
            "resetting env. episode 1041.000000, reward total was -19.000000. running mean: -20.207681\n",
            "resetting env. episode 1042.000000, reward total was -20.000000. running mean: -20.205604\n",
            "resetting env. episode 1043.000000, reward total was -19.000000. running mean: -20.193548\n",
            "resetting env. episode 1044.000000, reward total was -19.000000. running mean: -20.181612\n",
            "resetting env. episode 1045.000000, reward total was -19.000000. running mean: -20.169796\n",
            "resetting env. episode 1046.000000, reward total was -20.000000. running mean: -20.168098\n",
            "resetting env. episode 1047.000000, reward total was -20.000000. running mean: -20.166417\n",
            "resetting env. episode 1048.000000, reward total was -21.000000. running mean: -20.174753\n",
            "resetting env. episode 1049.000000, reward total was -20.000000. running mean: -20.173006\n",
            "resetting env. episode 1050.000000, reward total was -21.000000. running mean: -20.181276\n",
            "resetting env. episode 1051.000000, reward total was -21.000000. running mean: -20.189463\n",
            "resetting env. episode 1052.000000, reward total was -21.000000. running mean: -20.197568\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -20.205593\n",
            "resetting env. episode 1054.000000, reward total was -19.000000. running mean: -20.193537\n",
            "resetting env. episode 1055.000000, reward total was -21.000000. running mean: -20.201601\n",
            "resetting env. episode 1056.000000, reward total was -20.000000. running mean: -20.199585\n",
            "resetting env. episode 1057.000000, reward total was -21.000000. running mean: -20.207589\n",
            "resetting env. episode 1058.000000, reward total was -19.000000. running mean: -20.195514\n",
            "resetting env. episode 1059.000000, reward total was -19.000000. running mean: -20.183558\n",
            "resetting env. episode 1060.000000, reward total was -19.000000. running mean: -20.171723\n",
            "resetting env. episode 1061.000000, reward total was -19.000000. running mean: -20.160006\n",
            "resetting env. episode 1062.000000, reward total was -21.000000. running mean: -20.168406\n",
            "resetting env. episode 1063.000000, reward total was -19.000000. running mean: -20.156721\n",
            "resetting env. episode 1064.000000, reward total was -19.000000. running mean: -20.145154\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.153703\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -20.152166\n",
            "resetting env. episode 1067.000000, reward total was -20.000000. running mean: -20.150644\n",
            "resetting env. episode 1068.000000, reward total was -20.000000. running mean: -20.149138\n",
            "resetting env. episode 1069.000000, reward total was -20.000000. running mean: -20.147646\n",
            "resetting env. episode 1070.000000, reward total was -21.000000. running mean: -20.156170\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -20.164608\n",
            "resetting env. episode 1072.000000, reward total was -21.000000. running mean: -20.172962\n",
            "resetting env. episode 1073.000000, reward total was -21.000000. running mean: -20.181232\n",
            "resetting env. episode 1074.000000, reward total was -20.000000. running mean: -20.179420\n",
            "resetting env. episode 1075.000000, reward total was -19.000000. running mean: -20.167626\n",
            "resetting env. episode 1076.000000, reward total was -21.000000. running mean: -20.175950\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -20.184190\n",
            "resetting env. episode 1078.000000, reward total was -20.000000. running mean: -20.182348\n",
            "resetting env. episode 1079.000000, reward total was -20.000000. running mean: -20.180525\n",
            "resetting env. episode 1080.000000, reward total was -19.000000. running mean: -20.168719\n",
            "resetting env. episode 1081.000000, reward total was -20.000000. running mean: -20.167032\n",
            "resetting env. episode 1082.000000, reward total was -21.000000. running mean: -20.175362\n",
            "resetting env. episode 1083.000000, reward total was -20.000000. running mean: -20.173608\n",
            "resetting env. episode 1084.000000, reward total was -20.000000. running mean: -20.171872\n",
            "resetting env. episode 1085.000000, reward total was -21.000000. running mean: -20.180154\n",
            "resetting env. episode 1086.000000, reward total was -21.000000. running mean: -20.188352\n",
            "resetting env. episode 1087.000000, reward total was -20.000000. running mean: -20.186468\n",
            "resetting env. episode 1088.000000, reward total was -18.000000. running mean: -20.164604\n",
            "resetting env. episode 1089.000000, reward total was -20.000000. running mean: -20.162958\n",
            "resetting env. episode 1090.000000, reward total was -20.000000. running mean: -20.161328\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.169715\n",
            "resetting env. episode 1092.000000, reward total was -21.000000. running mean: -20.178018\n",
            "resetting env. episode 1093.000000, reward total was -21.000000. running mean: -20.186238\n",
            "resetting env. episode 1094.000000, reward total was -20.000000. running mean: -20.184375\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -20.192531\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -20.200606\n",
            "resetting env. episode 1097.000000, reward total was -20.000000. running mean: -20.198600\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -20.206614\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -20.214548\n",
            "resetting env. episode 1100.000000, reward total was -21.000000. running mean: -20.222402\n",
            "resetting env. episode 1101.000000, reward total was -20.000000. running mean: -20.220178\n",
            "resetting env. episode 1102.000000, reward total was -21.000000. running mean: -20.227977\n",
            "resetting env. episode 1103.000000, reward total was -19.000000. running mean: -20.215697\n",
            "resetting env. episode 1104.000000, reward total was -20.000000. running mean: -20.213540\n",
            "resetting env. episode 1105.000000, reward total was -21.000000. running mean: -20.221404\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -20.229190\n",
            "resetting env. episode 1107.000000, reward total was -20.000000. running mean: -20.226899\n",
            "resetting env. episode 1108.000000, reward total was -21.000000. running mean: -20.234630\n",
            "resetting env. episode 1109.000000, reward total was -20.000000. running mean: -20.232283\n",
            "resetting env. episode 1110.000000, reward total was -21.000000. running mean: -20.239960\n",
            "resetting env. episode 1111.000000, reward total was -18.000000. running mean: -20.217561\n",
            "resetting env. episode 1112.000000, reward total was -20.000000. running mean: -20.215385\n",
            "resetting env. episode 1113.000000, reward total was -20.000000. running mean: -20.213231\n",
            "resetting env. episode 1114.000000, reward total was -20.000000. running mean: -20.211099\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -20.218988\n",
            "resetting env. episode 1116.000000, reward total was -20.000000. running mean: -20.216798\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -20.224630\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -20.232384\n",
            "resetting env. episode 1119.000000, reward total was -18.000000. running mean: -20.210060\n",
            "resetting env. episode 1120.000000, reward total was -20.000000. running mean: -20.207959\n",
            "resetting env. episode 1121.000000, reward total was -19.000000. running mean: -20.195880\n",
            "resetting env. episode 1122.000000, reward total was -19.000000. running mean: -20.183921\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -20.192082\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -20.200161\n",
            "resetting env. episode 1125.000000, reward total was -20.000000. running mean: -20.198159\n",
            "resetting env. episode 1126.000000, reward total was -21.000000. running mean: -20.206178\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -20.214116\n",
            "resetting env. episode 1128.000000, reward total was -21.000000. running mean: -20.221975\n",
            "resetting env. episode 1129.000000, reward total was -20.000000. running mean: -20.219755\n",
            "resetting env. episode 1130.000000, reward total was -19.000000. running mean: -20.207558\n",
            "resetting env. episode 1131.000000, reward total was -21.000000. running mean: -20.215482\n",
            "resetting env. episode 1132.000000, reward total was -19.000000. running mean: -20.203327\n",
            "resetting env. episode 1133.000000, reward total was -20.000000. running mean: -20.201294\n",
            "resetting env. episode 1134.000000, reward total was -20.000000. running mean: -20.199281\n",
            "resetting env. episode 1135.000000, reward total was -21.000000. running mean: -20.207288\n",
            "resetting env. episode 1136.000000, reward total was -20.000000. running mean: -20.205215\n",
            "resetting env. episode 1137.000000, reward total was -20.000000. running mean: -20.203163\n",
            "resetting env. episode 1138.000000, reward total was -18.000000. running mean: -20.181131\n",
            "resetting env. episode 1139.000000, reward total was -21.000000. running mean: -20.189320\n",
            "resetting env. episode 1140.000000, reward total was -20.000000. running mean: -20.187427\n",
            "resetting env. episode 1141.000000, reward total was -20.000000. running mean: -20.185553\n",
            "resetting env. episode 1142.000000, reward total was -21.000000. running mean: -20.193697\n",
            "resetting env. episode 1143.000000, reward total was -18.000000. running mean: -20.171760\n",
            "resetting env. episode 1144.000000, reward total was -21.000000. running mean: -20.180043\n",
            "resetting env. episode 1145.000000, reward total was -20.000000. running mean: -20.178242\n",
            "resetting env. episode 1146.000000, reward total was -19.000000. running mean: -20.166460\n",
            "resetting env. episode 1147.000000, reward total was -18.000000. running mean: -20.144795\n",
            "resetting env. episode 1148.000000, reward total was -20.000000. running mean: -20.143347\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -20.151914\n",
            "resetting env. episode 1150.000000, reward total was -19.000000. running mean: -20.140395\n",
            "resetting env. episode 1151.000000, reward total was -18.000000. running mean: -20.118991\n",
            "resetting env. episode 1152.000000, reward total was -21.000000. running mean: -20.127801\n",
            "resetting env. episode 1153.000000, reward total was -20.000000. running mean: -20.126523\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -20.135258\n",
            "resetting env. episode 1155.000000, reward total was -20.000000. running mean: -20.133905\n",
            "resetting env. episode 1156.000000, reward total was -19.000000. running mean: -20.122566\n",
            "resetting env. episode 1157.000000, reward total was -21.000000. running mean: -20.131340\n",
            "resetting env. episode 1158.000000, reward total was -20.000000. running mean: -20.130027\n",
            "resetting env. episode 1159.000000, reward total was -19.000000. running mean: -20.118727\n",
            "resetting env. episode 1160.000000, reward total was -21.000000. running mean: -20.127539\n",
            "resetting env. episode 1161.000000, reward total was -21.000000. running mean: -20.136264\n",
            "resetting env. episode 1162.000000, reward total was -20.000000. running mean: -20.134901\n",
            "resetting env. episode 1163.000000, reward total was -21.000000. running mean: -20.143552\n",
            "resetting env. episode 1164.000000, reward total was -20.000000. running mean: -20.142117\n",
            "resetting env. episode 1165.000000, reward total was -21.000000. running mean: -20.150696\n",
            "resetting env. episode 1166.000000, reward total was -21.000000. running mean: -20.159189\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -20.167597\n",
            "resetting env. episode 1168.000000, reward total was -20.000000. running mean: -20.165921\n",
            "resetting env. episode 1169.000000, reward total was -21.000000. running mean: -20.174262\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -20.182519\n",
            "resetting env. episode 1171.000000, reward total was -20.000000. running mean: -20.180694\n",
            "resetting env. episode 1172.000000, reward total was -18.000000. running mean: -20.158887\n",
            "resetting env. episode 1173.000000, reward total was -20.000000. running mean: -20.157298\n",
            "resetting env. episode 1174.000000, reward total was -19.000000. running mean: -20.145725\n",
            "resetting env. episode 1175.000000, reward total was -20.000000. running mean: -20.144268\n",
            "resetting env. episode 1176.000000, reward total was -20.000000. running mean: -20.142825\n",
            "resetting env. episode 1177.000000, reward total was -19.000000. running mean: -20.131397\n",
            "resetting env. episode 1178.000000, reward total was -20.000000. running mean: -20.130083\n",
            "resetting env. episode 1179.000000, reward total was -21.000000. running mean: -20.138782\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -20.147394\n",
            "resetting env. episode 1181.000000, reward total was -19.000000. running mean: -20.135920\n",
            "resetting env. episode 1182.000000, reward total was -20.000000. running mean: -20.134561\n",
            "resetting env. episode 1183.000000, reward total was -19.000000. running mean: -20.123215\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -20.131983\n",
            "resetting env. episode 1185.000000, reward total was -21.000000. running mean: -20.140663\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.149257\n",
            "resetting env. episode 1187.000000, reward total was -20.000000. running mean: -20.147764\n",
            "resetting env. episode 1188.000000, reward total was -20.000000. running mean: -20.146287\n",
            "resetting env. episode 1189.000000, reward total was -20.000000. running mean: -20.144824\n",
            "resetting env. episode 1190.000000, reward total was -21.000000. running mean: -20.153375\n",
            "resetting env. episode 1191.000000, reward total was -17.000000. running mean: -20.121842\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -20.130623\n",
            "resetting env. episode 1193.000000, reward total was -19.000000. running mean: -20.119317\n",
            "resetting env. episode 1194.000000, reward total was -19.000000. running mean: -20.108124\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.117043\n",
            "resetting env. episode 1196.000000, reward total was -21.000000. running mean: -20.125872\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -20.134614\n",
            "resetting env. episode 1198.000000, reward total was -20.000000. running mean: -20.133267\n",
            "resetting env. episode 1199.000000, reward total was -20.000000. running mean: -20.131935\n",
            "resetting env. episode 1200.000000, reward total was -20.000000. running mean: -20.130615\n",
            "resetting env. episode 1201.000000, reward total was -19.000000. running mean: -20.119309\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.128116\n",
            "resetting env. episode 1203.000000, reward total was -21.000000. running mean: -20.136835\n",
            "resetting env. episode 1204.000000, reward total was -20.000000. running mean: -20.135467\n",
            "resetting env. episode 1205.000000, reward total was -20.000000. running mean: -20.134112\n",
            "resetting env. episode 1206.000000, reward total was -20.000000. running mean: -20.132771\n",
            "resetting env. episode 1207.000000, reward total was -20.000000. running mean: -20.131443\n",
            "resetting env. episode 1208.000000, reward total was -21.000000. running mean: -20.140129\n",
            "resetting env. episode 1209.000000, reward total was -19.000000. running mean: -20.128727\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.137440\n",
            "resetting env. episode 1211.000000, reward total was -19.000000. running mean: -20.126066\n",
            "resetting env. episode 1212.000000, reward total was -21.000000. running mean: -20.134805\n",
            "resetting env. episode 1213.000000, reward total was -21.000000. running mean: -20.143457\n",
            "resetting env. episode 1214.000000, reward total was -21.000000. running mean: -20.152022\n",
            "resetting env. episode 1215.000000, reward total was -21.000000. running mean: -20.160502\n",
            "resetting env. episode 1216.000000, reward total was -20.000000. running mean: -20.158897\n",
            "resetting env. episode 1217.000000, reward total was -18.000000. running mean: -20.137308\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -20.145935\n",
            "resetting env. episode 1219.000000, reward total was -19.000000. running mean: -20.134476\n",
            "resetting env. episode 1220.000000, reward total was -19.000000. running mean: -20.123131\n",
            "resetting env. episode 1221.000000, reward total was -20.000000. running mean: -20.121900\n",
            "resetting env. episode 1222.000000, reward total was -19.000000. running mean: -20.110681\n",
            "resetting env. episode 1223.000000, reward total was -21.000000. running mean: -20.119574\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -20.128378\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -20.127094\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -20.135823\n",
            "resetting env. episode 1227.000000, reward total was -20.000000. running mean: -20.134465\n",
            "resetting env. episode 1228.000000, reward total was -21.000000. running mean: -20.143121\n",
            "resetting env. episode 1229.000000, reward total was -19.000000. running mean: -20.131689\n",
            "resetting env. episode 1230.000000, reward total was -20.000000. running mean: -20.130372\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -20.139069\n",
            "resetting env. episode 1232.000000, reward total was -19.000000. running mean: -20.127678\n",
            "resetting env. episode 1233.000000, reward total was -20.000000. running mean: -20.126401\n",
            "resetting env. episode 1234.000000, reward total was -20.000000. running mean: -20.125137\n",
            "resetting env. episode 1235.000000, reward total was -21.000000. running mean: -20.133886\n",
            "resetting env. episode 1236.000000, reward total was -19.000000. running mean: -20.122547\n",
            "resetting env. episode 1237.000000, reward total was -19.000000. running mean: -20.111322\n",
            "resetting env. episode 1238.000000, reward total was -20.000000. running mean: -20.110208\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -20.119106\n",
            "resetting env. episode 1240.000000, reward total was -19.000000. running mean: -20.107915\n",
            "resetting env. episode 1241.000000, reward total was -20.000000. running mean: -20.106836\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -20.115768\n",
            "resetting env. episode 1243.000000, reward total was -21.000000. running mean: -20.124610\n",
            "resetting env. episode 1244.000000, reward total was -21.000000. running mean: -20.133364\n",
            "resetting env. episode 1245.000000, reward total was -21.000000. running mean: -20.142030\n",
            "resetting env. episode 1246.000000, reward total was -19.000000. running mean: -20.130610\n",
            "resetting env. episode 1247.000000, reward total was -17.000000. running mean: -20.099304\n",
            "resetting env. episode 1248.000000, reward total was -20.000000. running mean: -20.098311\n",
            "resetting env. episode 1249.000000, reward total was -20.000000. running mean: -20.097328\n",
            "resetting env. episode 1250.000000, reward total was -21.000000. running mean: -20.106354\n",
            "resetting env. episode 1251.000000, reward total was -20.000000. running mean: -20.105291\n",
            "resetting env. episode 1252.000000, reward total was -20.000000. running mean: -20.104238\n",
            "resetting env. episode 1253.000000, reward total was -20.000000. running mean: -20.103196\n",
            "resetting env. episode 1254.000000, reward total was -21.000000. running mean: -20.112164\n",
            "resetting env. episode 1255.000000, reward total was -20.000000. running mean: -20.111042\n",
            "resetting env. episode 1256.000000, reward total was -20.000000. running mean: -20.109932\n",
            "resetting env. episode 1257.000000, reward total was -20.000000. running mean: -20.108832\n",
            "resetting env. episode 1258.000000, reward total was -20.000000. running mean: -20.107744\n",
            "resetting env. episode 1259.000000, reward total was -21.000000. running mean: -20.116667\n",
            "resetting env. episode 1260.000000, reward total was -20.000000. running mean: -20.115500\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -20.124345\n",
            "resetting env. episode 1262.000000, reward total was -21.000000. running mean: -20.133101\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -20.141770\n",
            "resetting env. episode 1264.000000, reward total was -19.000000. running mean: -20.130353\n",
            "resetting env. episode 1265.000000, reward total was -20.000000. running mean: -20.129049\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -20.137759\n",
            "resetting env. episode 1267.000000, reward total was -20.000000. running mean: -20.136381\n",
            "resetting env. episode 1268.000000, reward total was -21.000000. running mean: -20.145017\n",
            "resetting env. episode 1269.000000, reward total was -20.000000. running mean: -20.143567\n",
            "resetting env. episode 1270.000000, reward total was -21.000000. running mean: -20.152131\n",
            "resetting env. episode 1271.000000, reward total was -18.000000. running mean: -20.130610\n",
            "resetting env. episode 1272.000000, reward total was -20.000000. running mean: -20.129304\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -20.138011\n",
            "resetting env. episode 1274.000000, reward total was -19.000000. running mean: -20.126631\n",
            "resetting env. episode 1275.000000, reward total was -21.000000. running mean: -20.135365\n",
            "resetting env. episode 1276.000000, reward total was -21.000000. running mean: -20.144011\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -20.152571\n",
            "resetting env. episode 1278.000000, reward total was -20.000000. running mean: -20.151045\n",
            "resetting env. episode 1279.000000, reward total was -21.000000. running mean: -20.159535\n",
            "resetting env. episode 1280.000000, reward total was -17.000000. running mean: -20.127939\n",
            "resetting env. episode 1281.000000, reward total was -17.000000. running mean: -20.096660\n",
            "resetting env. episode 1282.000000, reward total was -19.000000. running mean: -20.085693\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -20.094836\n",
            "resetting env. episode 1284.000000, reward total was -21.000000. running mean: -20.103888\n",
            "resetting env. episode 1285.000000, reward total was -21.000000. running mean: -20.112849\n",
            "resetting env. episode 1286.000000, reward total was -20.000000. running mean: -20.111721\n",
            "resetting env. episode 1287.000000, reward total was -21.000000. running mean: -20.120603\n",
            "resetting env. episode 1288.000000, reward total was -20.000000. running mean: -20.119397\n",
            "resetting env. episode 1289.000000, reward total was -21.000000. running mean: -20.128203\n",
            "resetting env. episode 1290.000000, reward total was -20.000000. running mean: -20.126921\n",
            "resetting env. episode 1291.000000, reward total was -21.000000. running mean: -20.135652\n",
            "resetting env. episode 1292.000000, reward total was -19.000000. running mean: -20.124296\n",
            "resetting env. episode 1293.000000, reward total was -19.000000. running mean: -20.113053\n",
            "resetting env. episode 1294.000000, reward total was -20.000000. running mean: -20.111922\n",
            "resetting env. episode 1295.000000, reward total was -17.000000. running mean: -20.080803\n",
            "resetting env. episode 1296.000000, reward total was -20.000000. running mean: -20.079995\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -20.089195\n",
            "resetting env. episode 1298.000000, reward total was -20.000000. running mean: -20.088303\n",
            "resetting env. episode 1299.000000, reward total was -21.000000. running mean: -20.097420\n",
            "resetting env. episode 1300.000000, reward total was -18.000000. running mean: -20.076446\n",
            "resetting env. episode 1301.000000, reward total was -19.000000. running mean: -20.065681\n",
            "resetting env. episode 1302.000000, reward total was -19.000000. running mean: -20.055025\n",
            "resetting env. episode 1303.000000, reward total was -20.000000. running mean: -20.054474\n",
            "resetting env. episode 1304.000000, reward total was -21.000000. running mean: -20.063930\n",
            "resetting env. episode 1305.000000, reward total was -17.000000. running mean: -20.033290\n",
            "resetting env. episode 1306.000000, reward total was -19.000000. running mean: -20.022957\n",
            "resetting env. episode 1307.000000, reward total was -21.000000. running mean: -20.032728\n",
            "resetting env. episode 1308.000000, reward total was -20.000000. running mean: -20.032400\n",
            "resetting env. episode 1309.000000, reward total was -21.000000. running mean: -20.042076\n",
            "resetting env. episode 1310.000000, reward total was -21.000000. running mean: -20.051656\n",
            "resetting env. episode 1311.000000, reward total was -20.000000. running mean: -20.051139\n",
            "resetting env. episode 1312.000000, reward total was -20.000000. running mean: -20.050628\n",
            "resetting env. episode 1313.000000, reward total was -20.000000. running mean: -20.050121\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -20.059620\n",
            "resetting env. episode 1315.000000, reward total was -19.000000. running mean: -20.049024\n",
            "resetting env. episode 1316.000000, reward total was -20.000000. running mean: -20.048534\n",
            "resetting env. episode 1317.000000, reward total was -20.000000. running mean: -20.048048\n",
            "resetting env. episode 1318.000000, reward total was -19.000000. running mean: -20.037568\n",
            "resetting env. episode 1319.000000, reward total was -20.000000. running mean: -20.037192\n",
            "resetting env. episode 1320.000000, reward total was -21.000000. running mean: -20.046820\n",
            "resetting env. episode 1321.000000, reward total was -20.000000. running mean: -20.046352\n",
            "resetting env. episode 1322.000000, reward total was -21.000000. running mean: -20.055889\n",
            "resetting env. episode 1323.000000, reward total was -18.000000. running mean: -20.035330\n",
            "resetting env. episode 1324.000000, reward total was -21.000000. running mean: -20.044976\n",
            "resetting env. episode 1325.000000, reward total was -21.000000. running mean: -20.054527\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -20.063981\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -20.073342\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -20.082608\n",
            "resetting env. episode 1329.000000, reward total was -18.000000. running mean: -20.061782\n",
            "resetting env. episode 1330.000000, reward total was -20.000000. running mean: -20.061164\n",
            "resetting env. episode 1331.000000, reward total was -20.000000. running mean: -20.060553\n",
            "resetting env. episode 1332.000000, reward total was -20.000000. running mean: -20.059947\n",
            "resetting env. episode 1333.000000, reward total was -21.000000. running mean: -20.069348\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -20.078654\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -20.087868\n",
            "resetting env. episode 1336.000000, reward total was -20.000000. running mean: -20.086989\n",
            "resetting env. episode 1337.000000, reward total was -20.000000. running mean: -20.086119\n",
            "resetting env. episode 1338.000000, reward total was -21.000000. running mean: -20.095258\n",
            "resetting env. episode 1339.000000, reward total was -17.000000. running mean: -20.064305\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -20.073662\n",
            "resetting env. episode 1341.000000, reward total was -20.000000. running mean: -20.072926\n",
            "resetting env. episode 1342.000000, reward total was -21.000000. running mean: -20.082196\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -20.091374\n",
            "resetting env. episode 1344.000000, reward total was -21.000000. running mean: -20.100461\n",
            "resetting env. episode 1345.000000, reward total was -20.000000. running mean: -20.099456\n",
            "resetting env. episode 1346.000000, reward total was -21.000000. running mean: -20.108462\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -20.117377\n",
            "resetting env. episode 1348.000000, reward total was -19.000000. running mean: -20.106203\n",
            "resetting env. episode 1349.000000, reward total was -19.000000. running mean: -20.095141\n",
            "resetting env. episode 1350.000000, reward total was -20.000000. running mean: -20.094190\n",
            "resetting env. episode 1351.000000, reward total was -21.000000. running mean: -20.103248\n",
            "resetting env. episode 1352.000000, reward total was -21.000000. running mean: -20.112215\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -20.121093\n",
            "resetting env. episode 1354.000000, reward total was -20.000000. running mean: -20.119882\n",
            "resetting env. episode 1355.000000, reward total was -20.000000. running mean: -20.118683\n",
            "resetting env. episode 1356.000000, reward total was -20.000000. running mean: -20.117497\n",
            "resetting env. episode 1357.000000, reward total was -21.000000. running mean: -20.126322\n",
            "resetting env. episode 1358.000000, reward total was -21.000000. running mean: -20.135058\n",
            "resetting env. episode 1359.000000, reward total was -21.000000. running mean: -20.143708\n",
            "resetting env. episode 1360.000000, reward total was -18.000000. running mean: -20.122271\n",
            "resetting env. episode 1361.000000, reward total was -19.000000. running mean: -20.111048\n",
            "resetting env. episode 1362.000000, reward total was -21.000000. running mean: -20.119938\n",
            "resetting env. episode 1363.000000, reward total was -21.000000. running mean: -20.128738\n",
            "resetting env. episode 1364.000000, reward total was -20.000000. running mean: -20.127451\n",
            "resetting env. episode 1365.000000, reward total was -21.000000. running mean: -20.136176\n",
            "resetting env. episode 1366.000000, reward total was -18.000000. running mean: -20.114815\n",
            "resetting env. episode 1367.000000, reward total was -21.000000. running mean: -20.123666\n",
            "resetting env. episode 1368.000000, reward total was -20.000000. running mean: -20.122430\n",
            "resetting env. episode 1369.000000, reward total was -21.000000. running mean: -20.131205\n",
            "resetting env. episode 1370.000000, reward total was -20.000000. running mean: -20.129893\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -20.138594\n",
            "resetting env. episode 1372.000000, reward total was -21.000000. running mean: -20.147208\n",
            "resetting env. episode 1373.000000, reward total was -19.000000. running mean: -20.135736\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -20.144379\n",
            "resetting env. episode 1375.000000, reward total was -20.000000. running mean: -20.142935\n",
            "resetting env. episode 1376.000000, reward total was -21.000000. running mean: -20.151506\n",
            "resetting env. episode 1377.000000, reward total was -21.000000. running mean: -20.159991\n",
            "resetting env. episode 1378.000000, reward total was -21.000000. running mean: -20.168391\n",
            "resetting env. episode 1379.000000, reward total was -21.000000. running mean: -20.176707\n",
            "resetting env. episode 1380.000000, reward total was -19.000000. running mean: -20.164940\n",
            "resetting env. episode 1381.000000, reward total was -20.000000. running mean: -20.163291\n",
            "resetting env. episode 1382.000000, reward total was -20.000000. running mean: -20.161658\n",
            "resetting env. episode 1383.000000, reward total was -19.000000. running mean: -20.150041\n",
            "resetting env. episode 1384.000000, reward total was -20.000000. running mean: -20.148541\n",
            "resetting env. episode 1385.000000, reward total was -20.000000. running mean: -20.147055\n",
            "resetting env. episode 1386.000000, reward total was -20.000000. running mean: -20.145585\n",
            "resetting env. episode 1387.000000, reward total was -19.000000. running mean: -20.134129\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -20.142788\n",
            "resetting env. episode 1389.000000, reward total was -21.000000. running mean: -20.151360\n",
            "resetting env. episode 1390.000000, reward total was -19.000000. running mean: -20.139846\n",
            "resetting env. episode 1391.000000, reward total was -19.000000. running mean: -20.128448\n",
            "resetting env. episode 1392.000000, reward total was -21.000000. running mean: -20.137163\n",
            "resetting env. episode 1393.000000, reward total was -19.000000. running mean: -20.125792\n",
            "resetting env. episode 1394.000000, reward total was -21.000000. running mean: -20.134534\n",
            "resetting env. episode 1395.000000, reward total was -20.000000. running mean: -20.133188\n",
            "resetting env. episode 1396.000000, reward total was -18.000000. running mean: -20.111856\n",
            "resetting env. episode 1397.000000, reward total was -21.000000. running mean: -20.120738\n",
            "resetting env. episode 1398.000000, reward total was -20.000000. running mean: -20.119530\n",
            "resetting env. episode 1399.000000, reward total was -21.000000. running mean: -20.128335\n",
            "resetting env. episode 1400.000000, reward total was -21.000000. running mean: -20.137052\n",
            "resetting env. episode 1401.000000, reward total was -20.000000. running mean: -20.135681\n",
            "resetting env. episode 1402.000000, reward total was -21.000000. running mean: -20.144324\n",
            "resetting env. episode 1403.000000, reward total was -21.000000. running mean: -20.152881\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -20.161352\n",
            "resetting env. episode 1405.000000, reward total was -20.000000. running mean: -20.159739\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -20.168141\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -20.176460\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -20.174695\n",
            "resetting env. episode 1409.000000, reward total was -20.000000. running mean: -20.172949\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -20.181219\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -20.189407\n",
            "resetting env. episode 1412.000000, reward total was -20.000000. running mean: -20.187513\n",
            "resetting env. episode 1413.000000, reward total was -17.000000. running mean: -20.155638\n",
            "resetting env. episode 1414.000000, reward total was -20.000000. running mean: -20.154081\n",
            "resetting env. episode 1415.000000, reward total was -21.000000. running mean: -20.162540\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -20.170915\n",
            "resetting env. episode 1417.000000, reward total was -18.000000. running mean: -20.149206\n",
            "resetting env. episode 1418.000000, reward total was -21.000000. running mean: -20.157714\n",
            "resetting env. episode 1419.000000, reward total was -20.000000. running mean: -20.156137\n",
            "resetting env. episode 1420.000000, reward total was -19.000000. running mean: -20.144575\n",
            "resetting env. episode 1421.000000, reward total was -20.000000. running mean: -20.143130\n",
            "resetting env. episode 1422.000000, reward total was -21.000000. running mean: -20.151698\n",
            "resetting env. episode 1423.000000, reward total was -20.000000. running mean: -20.150181\n",
            "resetting env. episode 1424.000000, reward total was -19.000000. running mean: -20.138679\n",
            "resetting env. episode 1425.000000, reward total was -19.000000. running mean: -20.127293\n",
            "resetting env. episode 1426.000000, reward total was -20.000000. running mean: -20.126020\n",
            "resetting env. episode 1427.000000, reward total was -20.000000. running mean: -20.124760\n",
            "resetting env. episode 1428.000000, reward total was -19.000000. running mean: -20.113512\n",
            "resetting env. episode 1429.000000, reward total was -20.000000. running mean: -20.112377\n",
            "resetting env. episode 1430.000000, reward total was -21.000000. running mean: -20.121253\n",
            "resetting env. episode 1431.000000, reward total was -20.000000. running mean: -20.120041\n",
            "resetting env. episode 1432.000000, reward total was -19.000000. running mean: -20.108840\n",
            "resetting env. episode 1433.000000, reward total was -19.000000. running mean: -20.097752\n",
            "resetting env. episode 1434.000000, reward total was -19.000000. running mean: -20.086774\n",
            "resetting env. episode 1435.000000, reward total was -19.000000. running mean: -20.075906\n",
            "resetting env. episode 1436.000000, reward total was -20.000000. running mean: -20.075147\n",
            "resetting env. episode 1437.000000, reward total was -20.000000. running mean: -20.074396\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.083652\n",
            "resetting env. episode 1439.000000, reward total was -20.000000. running mean: -20.082815\n",
            "resetting env. episode 1440.000000, reward total was -21.000000. running mean: -20.091987\n",
            "resetting env. episode 1441.000000, reward total was -21.000000. running mean: -20.101067\n",
            "resetting env. episode 1442.000000, reward total was -20.000000. running mean: -20.100057\n",
            "resetting env. episode 1443.000000, reward total was -20.000000. running mean: -20.099056\n",
            "resetting env. episode 1444.000000, reward total was -21.000000. running mean: -20.108066\n",
            "resetting env. episode 1445.000000, reward total was -20.000000. running mean: -20.106985\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -20.115915\n",
            "resetting env. episode 1447.000000, reward total was -21.000000. running mean: -20.124756\n",
            "resetting env. episode 1448.000000, reward total was -21.000000. running mean: -20.133508\n",
            "resetting env. episode 1449.000000, reward total was -21.000000. running mean: -20.142173\n",
            "resetting env. episode 1450.000000, reward total was -20.000000. running mean: -20.140752\n",
            "resetting env. episode 1451.000000, reward total was -20.000000. running mean: -20.139344\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -20.147951\n",
            "resetting env. episode 1453.000000, reward total was -20.000000. running mean: -20.146471\n",
            "resetting env. episode 1454.000000, reward total was -20.000000. running mean: -20.145006\n",
            "resetting env. episode 1455.000000, reward total was -20.000000. running mean: -20.143556\n",
            "resetting env. episode 1456.000000, reward total was -20.000000. running mean: -20.142121\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -20.150700\n",
            "resetting env. episode 1458.000000, reward total was -21.000000. running mean: -20.159193\n",
            "resetting env. episode 1459.000000, reward total was -20.000000. running mean: -20.157601\n",
            "resetting env. episode 1460.000000, reward total was -20.000000. running mean: -20.156025\n",
            "resetting env. episode 1461.000000, reward total was -18.000000. running mean: -20.134464\n",
            "resetting env. episode 1462.000000, reward total was -19.000000. running mean: -20.123120\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -20.131889\n",
            "resetting env. episode 1464.000000, reward total was -18.000000. running mean: -20.110570\n",
            "resetting env. episode 1465.000000, reward total was -21.000000. running mean: -20.119464\n",
            "resetting env. episode 1466.000000, reward total was -21.000000. running mean: -20.128269\n",
            "resetting env. episode 1467.000000, reward total was -21.000000. running mean: -20.136987\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -20.145617\n",
            "resetting env. episode 1469.000000, reward total was -20.000000. running mean: -20.144161\n",
            "resetting env. episode 1470.000000, reward total was -20.000000. running mean: -20.142719\n",
            "resetting env. episode 1471.000000, reward total was -18.000000. running mean: -20.121292\n",
            "resetting env. episode 1472.000000, reward total was -19.000000. running mean: -20.110079\n",
            "resetting env. episode 1473.000000, reward total was -18.000000. running mean: -20.088978\n",
            "resetting env. episode 1474.000000, reward total was -21.000000. running mean: -20.098088\n",
            "resetting env. episode 1475.000000, reward total was -21.000000. running mean: -20.107107\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -20.116036\n",
            "resetting env. episode 1477.000000, reward total was -20.000000. running mean: -20.114876\n",
            "resetting env. episode 1478.000000, reward total was -17.000000. running mean: -20.083727\n",
            "resetting env. episode 1479.000000, reward total was -19.000000. running mean: -20.072890\n",
            "resetting env. episode 1480.000000, reward total was -21.000000. running mean: -20.082161\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -20.091339\n",
            "resetting env. episode 1482.000000, reward total was -20.000000. running mean: -20.090426\n",
            "resetting env. episode 1483.000000, reward total was -21.000000. running mean: -20.099522\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -20.108527\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -20.117441\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -20.126267\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -20.135004\n",
            "resetting env. episode 1488.000000, reward total was -19.000000. running mean: -20.123654\n",
            "resetting env. episode 1489.000000, reward total was -21.000000. running mean: -20.132418\n",
            "resetting env. episode 1490.000000, reward total was -21.000000. running mean: -20.141093\n",
            "resetting env. episode 1491.000000, reward total was -20.000000. running mean: -20.139683\n",
            "resetting env. episode 1492.000000, reward total was -19.000000. running mean: -20.128286\n",
            "resetting env. episode 1493.000000, reward total was -21.000000. running mean: -20.137003\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -20.145633\n",
            "resetting env. episode 1495.000000, reward total was -21.000000. running mean: -20.154177\n",
            "resetting env. episode 1496.000000, reward total was -21.000000. running mean: -20.162635\n",
            "resetting env. episode 1497.000000, reward total was -21.000000. running mean: -20.171008\n",
            "resetting env. episode 1498.000000, reward total was -21.000000. running mean: -20.179298\n",
            "resetting env. episode 1499.000000, reward total was -21.000000. running mean: -20.187505\n",
            "resetting env. episode 1500.000000, reward total was -20.000000. running mean: -20.185630\n",
            "CPU times: user 1h 12min 25s, sys: 32min 12s, total: 1h 44min 37s\n",
            "Wall time: 53min 30s\n"
          ]
        }
      ],
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2NblmwDsL3y"
      },
      "outputs": [],
      "source": [
        "play_game(env, model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "H=800_le-3 (Original).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}