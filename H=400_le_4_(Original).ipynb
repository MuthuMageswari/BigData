{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "H=400_le-4 (Original).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "cell_type": "code",
      "source": [
        "!pip install JSAnimation >/dev/null"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "1bdacf56-b298-451c-ac3c-1918bc3daed5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 8.8 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=d005a97ef0c755020193bd4adfe2ca866a47a36ea21b04b119e7be9f4c5f8a00\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "362b24fc-dc04-4eaf-b78b-fb2edf418b68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "dad08ab7-b0d6-496b-d84c-1d582f8f1809",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "6d3cb57f-5fe1-49ba-f68a-6af02dcabe67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "e5695e81-941c-468f-924f-eac730338929",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -17.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 400 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "cd881d4a-bad5-42ca-b5c4-4862329ce79a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.019900\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.029701\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.029404\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.039110\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.048719\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.048232\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.047749\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.057272\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.056699\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.066132\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.065471\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.064816\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.074168\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.073426\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.082692\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.081865\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.091046\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.100136\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.109135\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.108043\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.116963\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.125793\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.124535\n",
            "resetting env. episode 27.000000, reward total was -18.000000. running mean: -20.103290\n",
            "resetting env. episode 28.000000, reward total was -19.000000. running mean: -20.092257\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.101334\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.110321\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.119218\n",
            "resetting env. episode 32.000000, reward total was -19.000000. running mean: -20.108026\n",
            "resetting env. episode 33.000000, reward total was -18.000000. running mean: -20.086945\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.096076\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.105115\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.114064\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.122923\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.131694\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.130377\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.129074\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.137783\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.146405\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.154941\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.163391\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.161758\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.160140\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.168539\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.176853\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.175085\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.183334\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.191500\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.199585\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.197590\n",
            "resetting env. episode 54.000000, reward total was -19.000000. running mean: -20.185614\n",
            "resetting env. episode 55.000000, reward total was -19.000000. running mean: -20.173758\n",
            "resetting env. episode 56.000000, reward total was -18.000000. running mean: -20.152020\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.160500\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.158895\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.167306\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.165633\n",
            "resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.163976\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.172337\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.180613\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.188807\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.196919\n",
            "resetting env. episode 66.000000, reward total was -19.000000. running mean: -20.184950\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.183100\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.191269\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.189357\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.187463\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.195589\n",
            "resetting env. episode 72.000000, reward total was -19.000000. running mean: -20.183633\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.181796\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -20.169978\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.178279\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.186496\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.184631\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.182785\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.190957\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.199047\n",
            "resetting env. episode 81.000000, reward total was -19.000000. running mean: -20.187057\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.195186\n",
            "resetting env. episode 83.000000, reward total was -18.000000. running mean: -20.173234\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.171502\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.179787\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.187989\n",
            "resetting env. episode 87.000000, reward total was -19.000000. running mean: -20.176109\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.174348\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.172605\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.180878\n",
            "resetting env. episode 91.000000, reward total was -20.000000. running mean: -20.179070\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.177279\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.175506\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.183751\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.191914\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.199995\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.207995\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -20.205915\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.213855\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.221717\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.219500\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.227305\n",
            "resetting env. episode 103.000000, reward total was -18.000000. running mean: -20.205032\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.212981\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.210852\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.218743\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.216556\n",
            "resetting env. episode 108.000000, reward total was -19.000000. running mean: -20.204390\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.212346\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.210223\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.218120\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.225939\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.233680\n",
            "resetting env. episode 114.000000, reward total was -20.000000. running mean: -20.231343\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.239030\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.246639\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.254173\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.261631\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.269015\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.276325\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.273562\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.270826\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.278118\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.285336\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.292483\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.299558\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.306563\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.313497\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.320362\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.327158\n",
            "resetting env. episode 131.000000, reward total was -20.000000. running mean: -20.323887\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.330648\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.337342\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.343968\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.350528\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.347023\n",
            "resetting env. episode 137.000000, reward total was -20.000000. running mean: -20.343553\n",
            "resetting env. episode 138.000000, reward total was -18.000000. running mean: -20.320117\n",
            "resetting env. episode 139.000000, reward total was -19.000000. running mean: -20.306916\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.313847\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.310709\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.317602\n",
            "resetting env. episode 143.000000, reward total was -19.000000. running mean: -20.304425\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.301381\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.298367\n",
            "resetting env. episode 146.000000, reward total was -19.000000. running mean: -20.285384\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.292530\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.289605\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.296709\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.303741\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.310704\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.317597\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.324421\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.321177\n",
            "resetting env. episode 155.000000, reward total was -18.000000. running mean: -20.297965\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.304985\n",
            "resetting env. episode 157.000000, reward total was -20.000000. running mean: -20.301936\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.298916\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.295927\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.292968\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.300038\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.307038\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.313967\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.320828\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.327619\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.334343\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.341000\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.347590\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.344114\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.350673\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.357166\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.363594\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.359958\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.356359\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.362795\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.369167\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.365476\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.371821\n",
            "resetting env. episode 179.000000, reward total was -19.000000. running mean: -20.358103\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.364522\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.360876\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.367268\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.373595\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.379859\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.386060\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.392200\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.388278\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.384395\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.390551\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.386646\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.382779\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.388951\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.395062\n",
            "resetting env. episode 194.000000, reward total was -19.000000. running mean: -20.381111\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.387300\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.383427\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.389593\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.385697\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.391840\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.397922\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.403942\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.409903\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.405804\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.411746\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.407628\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.413552\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.419417\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.425222\n",
            "resetting env. episode 209.000000, reward total was -19.000000. running mean: -20.410970\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.416860\n",
            "resetting env. episode 211.000000, reward total was -19.000000. running mean: -20.402692\n",
            "resetting env. episode 212.000000, reward total was -19.000000. running mean: -20.388665\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.384778\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.380931\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.377121\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.383350\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.379516\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.385721\n",
            "resetting env. episode 219.000000, reward total was -19.000000. running mean: -20.371864\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.378145\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.374364\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.380620\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.376814\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.383046\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.389216\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.395323\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.401370\n",
            "resetting env. episode 228.000000, reward total was -19.000000. running mean: -20.387356\n",
            "resetting env. episode 229.000000, reward total was -20.000000. running mean: -20.383483\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.379648\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.385852\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.391993\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.398073\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.404092\n",
            "resetting env. episode 235.000000, reward total was -19.000000. running mean: -20.390052\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.396151\n",
            "resetting env. episode 237.000000, reward total was -18.000000. running mean: -20.372189\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.368468\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.364783\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.361135\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.367524\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.373848\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.380110\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.376309\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.372546\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.368820\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.375132\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.371381\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.377667\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.383890\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.390051\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.396151\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.402189\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.408168\n",
            "resetting env. episode 255.000000, reward total was -19.000000. running mean: -20.394086\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.400145\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.396144\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.402182\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.408160\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.404079\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.400038\n",
            "resetting env. episode 262.000000, reward total was -18.000000. running mean: -20.376038\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.382277\n",
            "resetting env. episode 264.000000, reward total was -19.000000. running mean: -20.368454\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.364770\n",
            "resetting env. episode 266.000000, reward total was -19.000000. running mean: -20.351122\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.357611\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.364035\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.360394\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.366791\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.363123\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.369491\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.375796\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.372039\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.378318\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.384535\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.390690\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.386783\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.382915\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.389086\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.385195\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.391343\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.397429\n",
            "resetting env. episode 284.000000, reward total was -19.000000. running mean: -20.383455\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.379621\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.385824\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.381966\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.388147\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.394265\n",
            "resetting env. episode 290.000000, reward total was -19.000000. running mean: -20.380322\n",
            "resetting env. episode 291.000000, reward total was -19.000000. running mean: -20.366519\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.372854\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.379125\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.385334\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.391481\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.377566\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.383790\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.379952\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.376153\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.382391\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.378568\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.374782\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.381034\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.367224\n",
            "resetting env. episode 305.000000, reward total was -20.000000. running mean: -20.363551\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.369916\n",
            "resetting env. episode 307.000000, reward total was -18.000000. running mean: -20.346217\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.342755\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.349327\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.355834\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.362275\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.368653\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.364966\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.361317\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.367703\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.374026\n",
            "resetting env. episode 317.000000, reward total was -19.000000. running mean: -20.360286\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.366683\n",
            "resetting env. episode 319.000000, reward total was -17.000000. running mean: -20.333016\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.339686\n",
            "resetting env. episode 321.000000, reward total was -19.000000. running mean: -20.326289\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.333026\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.329696\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.336399\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.343035\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -20.329605\n",
            "resetting env. episode 327.000000, reward total was -19.000000. running mean: -20.316309\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.323146\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.329914\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.326615\n",
            "resetting env. episode 331.000000, reward total was -19.000000. running mean: -20.313349\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -20.300215\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.297213\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.304241\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.301199\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.298187\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.305205\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.312153\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.319031\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.325841\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.322583\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.319357\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.326163\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.322902\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.329673\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.326376\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.323112\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.329881\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.336582\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.333216\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.339884\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.346485\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.343020\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.349590\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.356094\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.362533\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.368908\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.375219\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.371467\n",
            "resetting env. episode 360.000000, reward total was -17.000000. running mean: -20.337752\n",
            "resetting env. episode 361.000000, reward total was -19.000000. running mean: -20.324375\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.331131\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.337820\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.334441\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.331097\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.337786\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.334408\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.341064\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.337653\n",
            "resetting env. episode 370.000000, reward total was -20.000000. running mean: -20.334277\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.330934\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.337625\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.334249\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.340906\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.347497\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.354022\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.350482\n",
            "resetting env. episode 378.000000, reward total was -17.000000. running mean: -20.316977\n",
            "resetting env. episode 379.000000, reward total was -19.000000. running mean: -20.303807\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.310769\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.307661\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.304585\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.311539\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.308424\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.315339\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.322186\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.328964\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.325674\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.332418\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.329094\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.335803\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.342445\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.349020\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.355530\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.361975\n",
            "resetting env. episode 396.000000, reward total was -19.000000. running mean: -20.348355\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.354871\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.361323\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.357709\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.354132\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.360591\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.356985\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.353415\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.349881\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.356382\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.362818\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.369190\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.375498\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.381743\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.387926\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.384047\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.390206\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.386304\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.392441\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.398517\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.394532\n",
            "resetting env. episode 417.000000, reward total was -19.000000. running mean: -20.380586\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.386780\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.392913\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.398983\n",
            "resetting env. episode 421.000000, reward total was -19.000000. running mean: -20.384994\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.391144\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.387232\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.393360\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.389426\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.385532\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.391677\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.397760\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.403782\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.409745\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.405647\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.401591\n",
            "resetting env. episode 433.000000, reward total was -19.000000. running mean: -20.387575\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.393699\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.399762\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.405764\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.401707\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.407690\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.413613\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.419477\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.415282\n",
            "resetting env. episode 442.000000, reward total was -20.000000. running mean: -20.411129\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.417018\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.412848\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.418719\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.414532\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.400387\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -20.386383\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.392519\n",
            "resetting env. episode 450.000000, reward total was -19.000000. running mean: -20.378594\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.374808\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.381060\n",
            "resetting env. episode 453.000000, reward total was -19.000000. running mean: -20.367249\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -20.353577\n",
            "resetting env. episode 455.000000, reward total was -20.000000. running mean: -20.350041\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.346540\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.353075\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.359544\n",
            "resetting env. episode 459.000000, reward total was -19.000000. running mean: -20.345949\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.352489\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.358964\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.365375\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.361721\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.358104\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.364523\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.360878\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.367269\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.363596\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.359960\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.366361\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.372697\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.378970\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.375180\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.371428\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.367714\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.364037\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.370397\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.366693\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.373026\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.369296\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.375603\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.381847\n",
            "resetting env. episode 483.000000, reward total was -19.000000. running mean: -20.368028\n",
            "resetting env. episode 484.000000, reward total was -19.000000. running mean: -20.354348\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.360804\n",
            "resetting env. episode 486.000000, reward total was -19.000000. running mean: -20.347196\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.343724\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.350287\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.346784\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.353316\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.359783\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.366185\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.372523\n",
            "resetting env. episode 494.000000, reward total was -20.000000. running mean: -20.368798\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.365110\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.361459\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.367845\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.374166\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.380424\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.376620\n",
            "CPU times: user 34min 1s, sys: 10min 26s, total: 44min 28s\n",
            "Wall time: 22min 47s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "0a4fe6c8-db24-4019-e861-110339a575fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -19.000000. running mean: -20.980000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.980200\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.980398\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.970594\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.970888\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.971179\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.971467\n",
            "resetting env. episode 9.000000, reward total was -19.000000. running mean: -20.951753\n",
            "resetting env. episode 10.000000, reward total was -18.000000. running mean: -20.922235\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.913013\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -20.903883\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.894844\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.895895\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.896937\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.897967\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.898987\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.889998\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.891098\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.892187\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.883265\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.874432\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.875688\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.876931\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.878162\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.879380\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.880586\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.881780\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.882963\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.884133\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.885292\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.886439\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.887574\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.888699\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -20.879812\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -20.861013\n",
            "resetting env. episode 37.000000, reward total was -20.000000. running mean: -20.852403\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.853879\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.855340\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.856787\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.848219\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.849737\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.841240\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.842827\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.834399\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.826055\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.827794\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.819516\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.811321\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.803208\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.805176\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.807124\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -20.789053\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.781162\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.783351\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.775517\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.777762\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.779985\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.782185\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.784363\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.786519\n",
            "resetting env. episode 62.000000, reward total was -18.000000. running mean: -20.758654\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.751068\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.753557\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.746021\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.748561\n",
            "resetting env. episode 67.000000, reward total was -19.000000. running mean: -20.731075\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.733765\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.726427\n",
            "resetting env. episode 70.000000, reward total was -19.000000. running mean: -20.709163\n",
            "resetting env. episode 71.000000, reward total was -19.000000. running mean: -20.692071\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.695150\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.688199\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.681317\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.674504\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.677759\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.680981\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.674171\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.677430\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.680655\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.683849\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.687010\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.680140\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.683339\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.676505\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.679740\n",
            "resetting env. episode 87.000000, reward total was -18.000000. running mean: -20.652943\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.656414\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.649849\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.653351\n",
            "resetting env. episode 91.000000, reward total was -19.000000. running mean: -20.636817\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.640449\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.644045\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.647604\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.651128\n",
            "resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.644617\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.638171\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.641789\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.645371\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.648917\n",
            "resetting env. episode 101.000000, reward total was -19.000000. running mean: -20.632428\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.636104\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.639743\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.643346\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.646912\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.640443\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.644039\n",
            "resetting env. episode 108.000000, reward total was -19.000000. running mean: -20.627598\n",
            "resetting env. episode 109.000000, reward total was -17.000000. running mean: -20.591322\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.585409\n",
            "resetting env. episode 111.000000, reward total was -19.000000. running mean: -20.569555\n",
            "resetting env. episode 112.000000, reward total was -19.000000. running mean: -20.553859\n",
            "resetting env. episode 113.000000, reward total was -19.000000. running mean: -20.538321\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.542937\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.547508\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.552033\n",
            "resetting env. episode 117.000000, reward total was -19.000000. running mean: -20.536513\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.541148\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.545736\n",
            "resetting env. episode 120.000000, reward total was -20.000000. running mean: -20.540279\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.544876\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.539427\n",
            "resetting env. episode 123.000000, reward total was -19.000000. running mean: -20.524033\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.518793\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.523605\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.528369\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.523085\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.517854\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.522676\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.527449\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.532174\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.536853\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.541484\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.546069\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.550609\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.555102\n",
            "resetting env. episode 137.000000, reward total was -18.000000. running mean: -20.529551\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.524256\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.529013\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.523723\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.518486\n",
            "resetting env. episode 142.000000, reward total was -19.000000. running mean: -20.503301\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.508268\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.503185\n",
            "resetting env. episode 145.000000, reward total was -19.000000. running mean: -20.488154\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.493272\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.498339\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.493356\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.498422\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.493438\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.498504\n",
            "resetting env. episode 152.000000, reward total was -18.000000. running mean: -20.473519\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.478784\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.483996\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.479156\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.484364\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.489521\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.494625\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.499679\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.504682\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.509635\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.514539\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.519394\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.524200\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.528958\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.533668\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.528332\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.523048\n",
            "resetting env. episode 169.000000, reward total was -19.000000. running mean: -20.507818\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.512740\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.517612\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.522436\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.517212\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.522040\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.516819\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.521651\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.516434\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.521270\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.526057\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.530797\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.535489\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.530134\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.534833\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.539484\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.534089\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.538749\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.533361\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.538027\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.542647\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.537221\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.541849\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.546430\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -20.530966\n",
            "resetting env. episode 194.000000, reward total was -19.000000. running mean: -20.515656\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.520500\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.525295\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.530042\n",
            "resetting env. episode 198.000000, reward total was -19.000000. running mean: -20.514741\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.519594\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.524398\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.529154\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.523862\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.528624\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.533337\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.538004\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.542624\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.547198\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.551726\n",
            "resetting env. episode 209.000000, reward total was -19.000000. running mean: -20.536209\n",
            "resetting env. episode 210.000000, reward total was -17.000000. running mean: -20.500846\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.495838\n",
            "resetting env. episode 212.000000, reward total was -20.000000. running mean: -20.490880\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.485971\n",
            "resetting env. episode 214.000000, reward total was -18.000000. running mean: -20.461111\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.456500\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.451935\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.447416\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.452941\n",
            "resetting env. episode 219.000000, reward total was -18.000000. running mean: -20.428412\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.434128\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.439787\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.445389\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.440935\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.446526\n",
            "resetting env. episode 225.000000, reward total was -19.000000. running mean: -20.432060\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.437740\n",
            "resetting env. episode 227.000000, reward total was -20.000000. running mean: -20.433362\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.429029\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.434738\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.430391\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.436087\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.441726\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.447309\n",
            "resetting env. episode 234.000000, reward total was -18.000000. running mean: -20.422836\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.428608\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.434321\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.429978\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.435678\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.441322\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.436908\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.442539\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.448114\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.443633\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.449197\n",
            "resetting env. episode 245.000000, reward total was -17.000000. running mean: -20.414705\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.420557\n",
            "resetting env. episode 247.000000, reward total was -18.000000. running mean: -20.396352\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.402388\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.408365\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.414281\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.420138\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.415937\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.421777\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.417560\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.423384\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.429150\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.434859\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.440510\n",
            "resetting env. episode 259.000000, reward total was -19.000000. running mean: -20.426105\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.421844\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.427625\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.433349\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.429016\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.424726\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.430478\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.426173\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.431912\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.437593\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.443217\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.438785\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.444397\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.439953\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.445553\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.441098\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.436687\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.432320\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.437997\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.443617\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.449180\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.454689\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.460142\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.455540\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.460985\n",
            "resetting env. episode 284.000000, reward total was -19.000000. running mean: -20.446375\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.441911\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.437492\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.433117\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.428786\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.434498\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.440153\n",
            "resetting env. episode 291.000000, reward total was -18.000000. running mean: -20.415752\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.421594\n",
            "resetting env. episode 293.000000, reward total was -19.000000. running mean: -20.407378\n",
            "resetting env. episode 294.000000, reward total was -19.000000. running mean: -20.393305\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.399372\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.405378\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.411324\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.417211\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.423039\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.418808\n",
            "resetting env. episode 301.000000, reward total was -19.000000. running mean: -20.404620\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.410574\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.406468\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.402404\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -20.388380\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.394496\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.400551\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.396545\n",
            "resetting env. episode 309.000000, reward total was -18.000000. running mean: -20.372580\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.368854\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.375165\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.381414\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.387600\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.393724\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.389786\n",
            "resetting env. episode 316.000000, reward total was -19.000000. running mean: -20.375889\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.382130\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.378308\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.384525\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.380680\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.386873\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.383005\n",
            "resetting env. episode 323.000000, reward total was -19.000000. running mean: -20.369174\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.375483\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.381728\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.387911\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.394032\n",
            "resetting env. episode 328.000000, reward total was -18.000000. running mean: -20.370091\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.366390\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.372726\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.378999\n",
            "resetting env. episode 332.000000, reward total was -19.000000. running mean: -20.365209\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.371557\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.377841\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.384063\n",
            "resetting env. episode 336.000000, reward total was -18.000000. running mean: -20.360222\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.366620\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.372954\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.369224\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.365532\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.371877\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.368158\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.374477\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.380732\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.386924\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.393055\n",
            "resetting env. episode 347.000000, reward total was -19.000000. running mean: -20.379125\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.385333\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.391480\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.387565\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.373690\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.369953\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.376253\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.382491\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.378666\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.374879\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.381130\n",
            "resetting env. episode 358.000000, reward total was -18.000000. running mean: -20.357319\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.363746\n",
            "resetting env. episode 360.000000, reward total was -20.000000. running mean: -20.360108\n",
            "resetting env. episode 361.000000, reward total was -19.000000. running mean: -20.346507\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.343042\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.349612\n",
            "resetting env. episode 364.000000, reward total was -18.000000. running mean: -20.326116\n",
            "resetting env. episode 365.000000, reward total was -17.000000. running mean: -20.292855\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.289926\n",
            "resetting env. episode 367.000000, reward total was -19.000000. running mean: -20.277027\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.274256\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.261514\n",
            "resetting env. episode 370.000000, reward total was -18.000000. running mean: -20.238899\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.246510\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.254045\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.261504\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.258889\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.266300\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.273637\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.280901\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.278092\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.275311\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.282558\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.289732\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.296835\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.303867\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.300828\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.307820\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.314741\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.321594\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.328378\n",
            "resetting env. episode 389.000000, reward total was -19.000000. running mean: -20.315094\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.321943\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.318724\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.315537\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.322381\n",
            "resetting env. episode 394.000000, reward total was -20.000000. running mean: -20.319158\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.325966\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.332706\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.329379\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.336085\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.342725\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.349297\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.355804\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.362246\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.368624\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.364938\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.371288\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.377575\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.383800\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.379962\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.376162\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.382400\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.388576\n",
            "resetting env. episode 412.000000, reward total was -20.000000. running mean: -20.384691\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.390844\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.396935\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.392966\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.399036\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.405046\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.410995\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.416885\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.422717\n",
            "resetting env. episode 421.000000, reward total was -20.000000. running mean: -20.418489\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.414305\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.420162\n",
            "resetting env. episode 424.000000, reward total was -19.000000. running mean: -20.405960\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.411900\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.407781\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.413703\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.419566\n",
            "resetting env. episode 429.000000, reward total was -19.000000. running mean: -20.405371\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.411317\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.417204\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.423032\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.418802\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.424614\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.420367\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.426164\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.421902\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.427683\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.433406\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.439072\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.434681\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.440335\n",
            "resetting env. episode 443.000000, reward total was -19.000000. running mean: -20.425931\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.421672\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.417455\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.413281\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.419148\n",
            "resetting env. episode 448.000000, reward total was -19.000000. running mean: -20.404956\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.410907\n",
            "resetting env. episode 450.000000, reward total was -18.000000. running mean: -20.386798\n",
            "resetting env. episode 451.000000, reward total was -19.000000. running mean: -20.372930\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.379201\n",
            "resetting env. episode 453.000000, reward total was -20.000000. running mean: -20.375409\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.381654\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.387838\n",
            "resetting env. episode 456.000000, reward total was -17.000000. running mean: -20.353960\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.360420\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.356816\n",
            "resetting env. episode 459.000000, reward total was -19.000000. running mean: -20.343248\n",
            "resetting env. episode 460.000000, reward total was -19.000000. running mean: -20.329815\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.336517\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.343152\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.349720\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.356223\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.352661\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.359134\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.365543\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.371887\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.368169\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.374487\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.380742\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.376935\n",
            "resetting env. episode 473.000000, reward total was -20.000000. running mean: -20.373165\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.369434\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.375739\n",
            "resetting env. episode 476.000000, reward total was -18.000000. running mean: -20.351982\n",
            "resetting env. episode 477.000000, reward total was -20.000000. running mean: -20.348462\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.354977\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.361428\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.357813\n",
            "resetting env. episode 481.000000, reward total was -19.000000. running mean: -20.344235\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.340793\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.347385\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.353911\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.350372\n",
            "resetting env. episode 486.000000, reward total was -19.000000. running mean: -20.336868\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.343500\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.350065\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.346564\n",
            "resetting env. episode 490.000000, reward total was -20.000000. running mean: -20.343098\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.339667\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.346271\n",
            "resetting env. episode 493.000000, reward total was -19.000000. running mean: -20.332808\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.339480\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.346085\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.342624\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.349198\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.355706\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.362149\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.368527\n",
            "CPU times: user 34min 53s, sys: 10min 38s, total: 45min 32s\n",
            "Wall time: 23min 13s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "08f8d660-7d85-417e-9271-fc0e5ac44c38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "b0bfd16c-da2b-4969-a157-a65166f269be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -19.000000. running mean: -20.980000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.970200\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.970498\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.970793\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.961085\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.951474\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.951959\n",
            "resetting env. episode 9.000000, reward total was -19.000000. running mean: -20.932440\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.933116\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.923784\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.924547\n",
            "resetting env. episode 13.000000, reward total was -19.000000. running mean: -20.905301\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.906248\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.897186\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.898214\n",
            "resetting env. episode 17.000000, reward total was -19.000000. running mean: -20.879232\n",
            "resetting env. episode 18.000000, reward total was -19.000000. running mean: -20.860439\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.861835\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.863216\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.854584\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.856038\n",
            "resetting env. episode 23.000000, reward total was -17.000000. running mean: -20.817478\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.819303\n",
            "resetting env. episode 25.000000, reward total was -18.000000. running mean: -20.791110\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.793199\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.785267\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.777415\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -20.769640\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.771944\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.764225\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.756582\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.759016\n",
            "resetting env. episode 34.000000, reward total was -19.000000. running mean: -20.741426\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.744012\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.746572\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.749106\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -20.731615\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.724299\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.727056\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.729785\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.732488\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.735163\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.737811\n",
            "resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.730433\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.733129\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.725797\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.728539\n",
            "resetting env. episode 49.000000, reward total was -19.000000. running mean: -20.711254\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.714141\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.717000\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.719830\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.722632\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.725405\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.728151\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.720870\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.723661\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.716425\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -20.699260\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.692268\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.695345\n",
            "resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.688392\n",
            "resetting env. episode 63.000000, reward total was -19.000000. running mean: -20.671508\n",
            "resetting env. episode 64.000000, reward total was -19.000000. running mean: -20.654793\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -20.648245\n",
            "resetting env. episode 66.000000, reward total was -20.000000. running mean: -20.641762\n",
            "resetting env. episode 67.000000, reward total was -18.000000. running mean: -20.615345\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.619191\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.622999\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.626769\n",
            "resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.630502\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.634197\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.627855\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.631576\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.635260\n",
            "resetting env. episode 76.000000, reward total was -20.000000. running mean: -20.628908\n",
            "resetting env. episode 77.000000, reward total was -20.000000. running mean: -20.622619\n",
            "resetting env. episode 78.000000, reward total was -18.000000. running mean: -20.596392\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.600428\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -20.594424\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.598480\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.602495\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.596470\n",
            "resetting env. episode 84.000000, reward total was -18.000000. running mean: -20.570505\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.564800\n",
            "resetting env. episode 86.000000, reward total was -19.000000. running mean: -20.549152\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.553661\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.558124\n",
            "resetting env. episode 89.000000, reward total was -20.000000. running mean: -20.552543\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.557018\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.561447\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.565833\n",
            "resetting env. episode 93.000000, reward total was -18.000000. running mean: -20.540175\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.544773\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.549325\n",
            "resetting env. episode 96.000000, reward total was -19.000000. running mean: -20.533832\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.528494\n",
            "resetting env. episode 98.000000, reward total was -17.000000. running mean: -20.493209\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -20.478277\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.483494\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.488659\n",
            "resetting env. episode 102.000000, reward total was -18.000000. running mean: -20.463772\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -20.449135\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.444643\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.440197\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.435795\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.431437\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.437122\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.432751\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.438424\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.444040\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.439599\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.435203\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.440851\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.436443\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.442078\n",
            "resetting env. episode 117.000000, reward total was -19.000000. running mean: -20.427657\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.423381\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.429147\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.434856\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.440507\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.446102\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.451641\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.457124\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.462553\n",
            "resetting env. episode 126.000000, reward total was -20.000000. running mean: -20.457928\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.463348\n",
            "resetting env. episode 128.000000, reward total was -19.000000. running mean: -20.448715\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.444228\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.449786\n",
            "resetting env. episode 131.000000, reward total was -17.000000. running mean: -20.415288\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.411135\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.417023\n",
            "resetting env. episode 134.000000, reward total was -19.000000. running mean: -20.402853\n",
            "resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.408825\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.414736\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.420589\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.426383\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.432119\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.437798\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.443420\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.448986\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.444496\n",
            "resetting env. episode 144.000000, reward total was -20.000000. running mean: -20.440051\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.435651\n",
            "resetting env. episode 146.000000, reward total was -19.000000. running mean: -20.421294\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.427081\n",
            "resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.422810\n",
            "resetting env. episode 149.000000, reward total was -19.000000. running mean: -20.408582\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.404496\n",
            "resetting env. episode 151.000000, reward total was -21.000000. running mean: -20.410451\n",
            "resetting env. episode 152.000000, reward total was -20.000000. running mean: -20.406347\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.412283\n",
            "resetting env. episode 154.000000, reward total was -20.000000. running mean: -20.408161\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.414079\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.419938\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.425739\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.431481\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.437167\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.442795\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.448367\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.453883\n",
            "resetting env. episode 163.000000, reward total was -18.000000. running mean: -20.429345\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.435051\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.440701\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.446294\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.441831\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.437412\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.443038\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.438608\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.444222\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.449780\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.455282\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.450729\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.456222\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.451659\n",
            "resetting env. episode 177.000000, reward total was -20.000000. running mean: -20.447143\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.442671\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.448245\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.453762\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.459225\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.454632\n",
            "resetting env. episode 183.000000, reward total was -19.000000. running mean: -20.440086\n",
            "resetting env. episode 184.000000, reward total was -18.000000. running mean: -20.415685\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.421528\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.427313\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.433040\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.438710\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.444322\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.449879\n",
            "resetting env. episode 191.000000, reward total was -19.000000. running mean: -20.435380\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.431027\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.436716\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.432349\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.438026\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.443645\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.449209\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.454717\n",
            "resetting env. episode 199.000000, reward total was -19.000000. running mean: -20.440170\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.445768\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.451310\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.456797\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.452229\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.447707\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.453230\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.458698\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.464111\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.469470\n",
            "resetting env. episode 209.000000, reward total was -19.000000. running mean: -20.454775\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.450227\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.455725\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.461168\n",
            "resetting env. episode 213.000000, reward total was -20.000000. running mean: -20.456556\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.461990\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.467370\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.462697\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.468070\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.463389\n",
            "resetting env. episode 219.000000, reward total was -18.000000. running mean: -20.438755\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.444368\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.439924\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.445525\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.451069\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.446559\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.452093\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.457572\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.462997\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.458367\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.463783\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.469145\n",
            "resetting env. episode 231.000000, reward total was -20.000000. running mean: -20.464454\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.459809\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.455211\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.460659\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.466052\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.471392\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.476678\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.481911\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.477092\n",
            "resetting env. episode 240.000000, reward total was -19.000000. running mean: -20.462321\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.457698\n",
            "resetting env. episode 242.000000, reward total was -19.000000. running mean: -20.443121\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.438690\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.444303\n",
            "resetting env. episode 245.000000, reward total was -19.000000. running mean: -20.429860\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.435561\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.431205\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.436893\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.442525\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.448099\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.443618\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.439182\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.444790\n",
            "resetting env. episode 254.000000, reward total was -19.000000. running mean: -20.430342\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.436039\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.441679\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.437262\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.442889\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.438460\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.444076\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.439635\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.435239\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.440886\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.446477\n",
            "resetting env. episode 265.000000, reward total was -20.000000. running mean: -20.442013\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.437592\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.443216\n",
            "resetting env. episode 268.000000, reward total was -19.000000. running mean: -20.428784\n",
            "resetting env. episode 269.000000, reward total was -18.000000. running mean: -20.404496\n",
            "resetting env. episode 270.000000, reward total was -19.000000. running mean: -20.390452\n",
            "resetting env. episode 271.000000, reward total was -20.000000. running mean: -20.386547\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.382682\n",
            "resetting env. episode 273.000000, reward total was -19.000000. running mean: -20.368855\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.375166\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.381414\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.387600\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.383724\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.379887\n",
            "resetting env. episode 279.000000, reward total was -19.000000. running mean: -20.366088\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.372427\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.378703\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.374916\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.381167\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.377355\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.383582\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.379746\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.385948\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.392089\n",
            "resetting env. episode 289.000000, reward total was -20.000000. running mean: -20.388168\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.384286\n",
            "resetting env. episode 291.000000, reward total was -19.000000. running mean: -20.370443\n",
            "resetting env. episode 292.000000, reward total was -18.000000. running mean: -20.346739\n",
            "resetting env. episode 293.000000, reward total was -18.000000. running mean: -20.323272\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.320039\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.316839\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.323670\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.320433\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.327229\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.323957\n",
            "resetting env. episode 300.000000, reward total was -20.000000. running mean: -20.320717\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.327510\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.334235\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.340893\n",
            "resetting env. episode 304.000000, reward total was -18.000000. running mean: -20.317484\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.324309\n",
            "resetting env. episode 306.000000, reward total was -18.000000. running mean: -20.301066\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.308055\n",
            "resetting env. episode 308.000000, reward total was -16.000000. running mean: -20.264975\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.272325\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.269602\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.276906\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.284137\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.281295\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.288482\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.295597\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.302641\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.299615\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.306619\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.303553\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.310517\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.317412\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.314238\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.311095\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.317984\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.314805\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.321657\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.328440\n",
            "resetting env. episode 328.000000, reward total was -19.000000. running mean: -20.315156\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.312004\n",
            "resetting env. episode 330.000000, reward total was -19.000000. running mean: -20.298884\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.295895\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.292936\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.300007\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.297007\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.294037\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.301096\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.308085\n",
            "resetting env. episode 338.000000, reward total was -20.000000. running mean: -20.305005\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.311955\n",
            "resetting env. episode 340.000000, reward total was -20.000000. running mean: -20.308835\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.315747\n",
            "resetting env. episode 342.000000, reward total was -19.000000. running mean: -20.302589\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.309563\n",
            "resetting env. episode 344.000000, reward total was -19.000000. running mean: -20.296468\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.293503\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.290568\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.287662\n",
            "resetting env. episode 348.000000, reward total was -20.000000. running mean: -20.284786\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.291938\n",
            "resetting env. episode 350.000000, reward total was -19.000000. running mean: -20.279018\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.286228\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.293366\n",
            "resetting env. episode 353.000000, reward total was -19.000000. running mean: -20.280432\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.287628\n",
            "resetting env. episode 355.000000, reward total was -18.000000. running mean: -20.264752\n",
            "resetting env. episode 356.000000, reward total was -20.000000. running mean: -20.262104\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.269483\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.276788\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.284020\n",
            "resetting env. episode 360.000000, reward total was -19.000000. running mean: -20.271180\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.278468\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.285684\n",
            "resetting env. episode 363.000000, reward total was -19.000000. running mean: -20.272827\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.270099\n",
            "resetting env. episode 365.000000, reward total was -20.000000. running mean: -20.267398\n",
            "resetting env. episode 366.000000, reward total was -19.000000. running mean: -20.254724\n",
            "resetting env. episode 367.000000, reward total was -18.000000. running mean: -20.232176\n",
            "resetting env. episode 368.000000, reward total was -19.000000. running mean: -20.219855\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.227656\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.235380\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.243026\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.250595\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.248090\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.255609\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.263053\n",
            "resetting env. episode 376.000000, reward total was -19.000000. running mean: -20.250422\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.257918\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.255339\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.252785\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.260257\n",
            "resetting env. episode 381.000000, reward total was -18.000000. running mean: -20.237655\n",
            "resetting env. episode 382.000000, reward total was -19.000000. running mean: -20.225278\n",
            "resetting env. episode 383.000000, reward total was -19.000000. running mean: -20.213025\n",
            "resetting env. episode 384.000000, reward total was -19.000000. running mean: -20.200895\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.208886\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.216797\n",
            "resetting env. episode 387.000000, reward total was -18.000000. running mean: -20.194629\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.192683\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.200756\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.208749\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.216661\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.214495\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.212350\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -20.200226\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.208224\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.206142\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.204080\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.212039\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.209919\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.217820\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.225642\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.233385\n",
            "resetting env. episode 403.000000, reward total was -19.000000. running mean: -20.221051\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.228841\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.236552\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.244187\n",
            "resetting env. episode 407.000000, reward total was -18.000000. running mean: -20.221745\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.219528\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.227332\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.235059\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.242708\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.250281\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.247779\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.255301\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.262748\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.270120\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.277419\n",
            "resetting env. episode 418.000000, reward total was -19.000000. running mean: -20.264645\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.261998\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.269378\n",
            "resetting env. episode 421.000000, reward total was -18.000000. running mean: -20.246685\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.254218\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.261676\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.269059\n",
            "resetting env. episode 425.000000, reward total was -19.000000. running mean: -20.256368\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.253805\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.251267\n",
            "resetting env. episode 428.000000, reward total was -19.000000. running mean: -20.238754\n",
            "resetting env. episode 429.000000, reward total was -19.000000. running mean: -20.226366\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.234103\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -20.221762\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.229544\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.227249\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.234976\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.242626\n",
            "resetting env. episode 436.000000, reward total was -19.000000. running mean: -20.230200\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.237898\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.245519\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.243064\n",
            "resetting env. episode 440.000000, reward total was -20.000000. running mean: -20.240633\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.248227\n",
            "resetting env. episode 442.000000, reward total was -19.000000. running mean: -20.235745\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.243387\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.240953\n",
            "resetting env. episode 445.000000, reward total was -19.000000. running mean: -20.228544\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.226258\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.233996\n",
            "resetting env. episode 448.000000, reward total was -18.000000. running mean: -20.211656\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.219539\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.227344\n",
            "resetting env. episode 451.000000, reward total was -20.000000. running mean: -20.225070\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.232820\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.240492\n",
            "resetting env. episode 454.000000, reward total was -19.000000. running mean: -20.228087\n",
            "resetting env. episode 455.000000, reward total was -19.000000. running mean: -20.215806\n",
            "resetting env. episode 456.000000, reward total was -18.000000. running mean: -20.193648\n",
            "resetting env. episode 457.000000, reward total was -19.000000. running mean: -20.181711\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.189894\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.187995\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.186115\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.194254\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.202312\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.200288\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.208286\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.206203\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.204141\n",
            "resetting env. episode 467.000000, reward total was -18.000000. running mean: -20.182099\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.190278\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.198375\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.196392\n",
            "resetting env. episode 471.000000, reward total was -18.000000. running mean: -20.174428\n",
            "resetting env. episode 472.000000, reward total was -19.000000. running mean: -20.162684\n",
            "resetting env. episode 473.000000, reward total was -19.000000. running mean: -20.151057\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.159546\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.167951\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.176271\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.184508\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.182663\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.180837\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.189028\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.187138\n",
            "resetting env. episode 482.000000, reward total was -18.000000. running mean: -20.165267\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.173614\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.181878\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.180059\n",
            "resetting env. episode 486.000000, reward total was -18.000000. running mean: -20.158259\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.166676\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.165009\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.173359\n",
            "resetting env. episode 490.000000, reward total was -19.000000. running mean: -20.161626\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.160009\n",
            "resetting env. episode 492.000000, reward total was -19.000000. running mean: -20.148409\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.156925\n",
            "resetting env. episode 494.000000, reward total was -18.000000. running mean: -20.135356\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.134002\n",
            "resetting env. episode 496.000000, reward total was -20.000000. running mean: -20.132662\n",
            "resetting env. episode 497.000000, reward total was -20.000000. running mean: -20.131336\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.140022\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.148622\n",
            "resetting env. episode 500.000000, reward total was -20.000000. running mean: -20.147136\n",
            "resetting env. episode 501.000000, reward total was -19.000000. running mean: -20.135664\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.144308\n",
            "resetting env. episode 503.000000, reward total was -21.000000. running mean: -20.152865\n",
            "resetting env. episode 504.000000, reward total was -20.000000. running mean: -20.151336\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -20.159823\n",
            "resetting env. episode 506.000000, reward total was -21.000000. running mean: -20.168224\n",
            "resetting env. episode 507.000000, reward total was -20.000000. running mean: -20.166542\n",
            "resetting env. episode 508.000000, reward total was -20.000000. running mean: -20.164877\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.173228\n",
            "resetting env. episode 510.000000, reward total was -21.000000. running mean: -20.181496\n",
            "resetting env. episode 511.000000, reward total was -20.000000. running mean: -20.179681\n",
            "resetting env. episode 512.000000, reward total was -17.000000. running mean: -20.147884\n",
            "resetting env. episode 513.000000, reward total was -20.000000. running mean: -20.146405\n",
            "resetting env. episode 514.000000, reward total was -21.000000. running mean: -20.154941\n",
            "resetting env. episode 515.000000, reward total was -21.000000. running mean: -20.163392\n",
            "resetting env. episode 516.000000, reward total was -19.000000. running mean: -20.151758\n",
            "resetting env. episode 517.000000, reward total was -20.000000. running mean: -20.150240\n",
            "resetting env. episode 518.000000, reward total was -20.000000. running mean: -20.148738\n",
            "resetting env. episode 519.000000, reward total was -21.000000. running mean: -20.157250\n",
            "resetting env. episode 520.000000, reward total was -21.000000. running mean: -20.165678\n",
            "resetting env. episode 521.000000, reward total was -20.000000. running mean: -20.164021\n",
            "resetting env. episode 522.000000, reward total was -19.000000. running mean: -20.152381\n",
            "resetting env. episode 523.000000, reward total was -19.000000. running mean: -20.140857\n",
            "resetting env. episode 524.000000, reward total was -20.000000. running mean: -20.139449\n",
            "resetting env. episode 525.000000, reward total was -19.000000. running mean: -20.128054\n",
            "resetting env. episode 526.000000, reward total was -20.000000. running mean: -20.126774\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.135506\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.144151\n",
            "resetting env. episode 529.000000, reward total was -17.000000. running mean: -20.112709\n",
            "resetting env. episode 530.000000, reward total was -21.000000. running mean: -20.121582\n",
            "resetting env. episode 531.000000, reward total was -19.000000. running mean: -20.110366\n",
            "resetting env. episode 532.000000, reward total was -20.000000. running mean: -20.109263\n",
            "resetting env. episode 533.000000, reward total was -21.000000. running mean: -20.118170\n",
            "resetting env. episode 534.000000, reward total was -21.000000. running mean: -20.126988\n",
            "resetting env. episode 535.000000, reward total was -20.000000. running mean: -20.125718\n",
            "resetting env. episode 536.000000, reward total was -20.000000. running mean: -20.124461\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.133217\n",
            "resetting env. episode 538.000000, reward total was -20.000000. running mean: -20.131884\n",
            "resetting env. episode 539.000000, reward total was -20.000000. running mean: -20.130566\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.139260\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.147867\n",
            "resetting env. episode 542.000000, reward total was -20.000000. running mean: -20.146389\n",
            "resetting env. episode 543.000000, reward total was -20.000000. running mean: -20.144925\n",
            "resetting env. episode 544.000000, reward total was -20.000000. running mean: -20.143476\n",
            "resetting env. episode 545.000000, reward total was -20.000000. running mean: -20.142041\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -20.150620\n",
            "resetting env. episode 547.000000, reward total was -20.000000. running mean: -20.149114\n",
            "resetting env. episode 548.000000, reward total was -21.000000. running mean: -20.157623\n",
            "resetting env. episode 549.000000, reward total was -20.000000. running mean: -20.156047\n",
            "resetting env. episode 550.000000, reward total was -20.000000. running mean: -20.154486\n",
            "resetting env. episode 551.000000, reward total was -20.000000. running mean: -20.152941\n",
            "resetting env. episode 552.000000, reward total was -21.000000. running mean: -20.161412\n",
            "resetting env. episode 553.000000, reward total was -18.000000. running mean: -20.139798\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -20.148400\n",
            "resetting env. episode 555.000000, reward total was -20.000000. running mean: -20.146916\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.145447\n",
            "resetting env. episode 557.000000, reward total was -20.000000. running mean: -20.143992\n",
            "resetting env. episode 558.000000, reward total was -16.000000. running mean: -20.102552\n",
            "resetting env. episode 559.000000, reward total was -21.000000. running mean: -20.111527\n",
            "resetting env. episode 560.000000, reward total was -21.000000. running mean: -20.120412\n",
            "resetting env. episode 561.000000, reward total was -20.000000. running mean: -20.119208\n",
            "resetting env. episode 562.000000, reward total was -19.000000. running mean: -20.108015\n",
            "resetting env. episode 563.000000, reward total was -21.000000. running mean: -20.116935\n",
            "resetting env. episode 564.000000, reward total was -19.000000. running mean: -20.105766\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -20.114708\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.123561\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.132326\n",
            "resetting env. episode 568.000000, reward total was -20.000000. running mean: -20.131002\n",
            "resetting env. episode 569.000000, reward total was -20.000000. running mean: -20.129692\n",
            "resetting env. episode 570.000000, reward total was -18.000000. running mean: -20.108395\n",
            "resetting env. episode 571.000000, reward total was -21.000000. running mean: -20.117311\n",
            "resetting env. episode 572.000000, reward total was -21.000000. running mean: -20.126138\n",
            "resetting env. episode 573.000000, reward total was -20.000000. running mean: -20.124877\n",
            "resetting env. episode 574.000000, reward total was -20.000000. running mean: -20.123628\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.132392\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -20.141068\n",
            "resetting env. episode 577.000000, reward total was -21.000000. running mean: -20.149657\n",
            "resetting env. episode 578.000000, reward total was -21.000000. running mean: -20.158161\n",
            "resetting env. episode 579.000000, reward total was -21.000000. running mean: -20.166579\n",
            "resetting env. episode 580.000000, reward total was -21.000000. running mean: -20.174913\n",
            "resetting env. episode 581.000000, reward total was -19.000000. running mean: -20.163164\n",
            "resetting env. episode 582.000000, reward total was -19.000000. running mean: -20.151533\n",
            "resetting env. episode 583.000000, reward total was -20.000000. running mean: -20.150017\n",
            "resetting env. episode 584.000000, reward total was -21.000000. running mean: -20.158517\n",
            "resetting env. episode 585.000000, reward total was -20.000000. running mean: -20.156932\n",
            "resetting env. episode 586.000000, reward total was -20.000000. running mean: -20.155363\n",
            "resetting env. episode 587.000000, reward total was -19.000000. running mean: -20.143809\n",
            "resetting env. episode 588.000000, reward total was -19.000000. running mean: -20.132371\n",
            "resetting env. episode 589.000000, reward total was -20.000000. running mean: -20.131047\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.139737\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.148339\n",
            "resetting env. episode 592.000000, reward total was -21.000000. running mean: -20.156856\n",
            "resetting env. episode 593.000000, reward total was -21.000000. running mean: -20.165287\n",
            "resetting env. episode 594.000000, reward total was -20.000000. running mean: -20.163634\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.171998\n",
            "resetting env. episode 596.000000, reward total was -19.000000. running mean: -20.160278\n",
            "resetting env. episode 597.000000, reward total was -21.000000. running mean: -20.168675\n",
            "resetting env. episode 598.000000, reward total was -21.000000. running mean: -20.176989\n",
            "resetting env. episode 599.000000, reward total was -19.000000. running mean: -20.165219\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.173567\n",
            "resetting env. episode 601.000000, reward total was -20.000000. running mean: -20.171831\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -20.180113\n",
            "resetting env. episode 603.000000, reward total was -19.000000. running mean: -20.168311\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.176628\n",
            "resetting env. episode 605.000000, reward total was -21.000000. running mean: -20.184862\n",
            "resetting env. episode 606.000000, reward total was -21.000000. running mean: -20.193013\n",
            "resetting env. episode 607.000000, reward total was -18.000000. running mean: -20.171083\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -20.179372\n",
            "resetting env. episode 609.000000, reward total was -21.000000. running mean: -20.187579\n",
            "resetting env. episode 610.000000, reward total was -19.000000. running mean: -20.175703\n",
            "resetting env. episode 611.000000, reward total was -21.000000. running mean: -20.183946\n",
            "resetting env. episode 612.000000, reward total was -21.000000. running mean: -20.192106\n",
            "resetting env. episode 613.000000, reward total was -20.000000. running mean: -20.190185\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.198284\n",
            "resetting env. episode 615.000000, reward total was -21.000000. running mean: -20.206301\n",
            "resetting env. episode 616.000000, reward total was -21.000000. running mean: -20.214238\n",
            "resetting env. episode 617.000000, reward total was -19.000000. running mean: -20.202095\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.210074\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.207974\n",
            "resetting env. episode 620.000000, reward total was -20.000000. running mean: -20.205894\n",
            "resetting env. episode 621.000000, reward total was -21.000000. running mean: -20.213835\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -20.221697\n",
            "resetting env. episode 623.000000, reward total was -20.000000. running mean: -20.219480\n",
            "resetting env. episode 624.000000, reward total was -19.000000. running mean: -20.207285\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.215212\n",
            "resetting env. episode 626.000000, reward total was -19.000000. running mean: -20.203060\n",
            "resetting env. episode 627.000000, reward total was -20.000000. running mean: -20.201029\n",
            "resetting env. episode 628.000000, reward total was -20.000000. running mean: -20.199019\n",
            "resetting env. episode 629.000000, reward total was -19.000000. running mean: -20.187029\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.195158\n",
            "resetting env. episode 631.000000, reward total was -21.000000. running mean: -20.203207\n",
            "resetting env. episode 632.000000, reward total was -20.000000. running mean: -20.201175\n",
            "resetting env. episode 633.000000, reward total was -21.000000. running mean: -20.209163\n",
            "resetting env. episode 634.000000, reward total was -21.000000. running mean: -20.217071\n",
            "resetting env. episode 635.000000, reward total was -21.000000. running mean: -20.224901\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.232652\n",
            "resetting env. episode 637.000000, reward total was -18.000000. running mean: -20.210325\n",
            "resetting env. episode 638.000000, reward total was -18.000000. running mean: -20.188222\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.196340\n",
            "resetting env. episode 640.000000, reward total was -21.000000. running mean: -20.204376\n",
            "resetting env. episode 641.000000, reward total was -19.000000. running mean: -20.192333\n",
            "resetting env. episode 642.000000, reward total was -20.000000. running mean: -20.190409\n",
            "resetting env. episode 643.000000, reward total was -20.000000. running mean: -20.188505\n",
            "resetting env. episode 644.000000, reward total was -21.000000. running mean: -20.196620\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.204654\n",
            "resetting env. episode 646.000000, reward total was -20.000000. running mean: -20.202607\n",
            "resetting env. episode 647.000000, reward total was -19.000000. running mean: -20.190581\n",
            "resetting env. episode 648.000000, reward total was -18.000000. running mean: -20.168675\n",
            "resetting env. episode 649.000000, reward total was -21.000000. running mean: -20.176989\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.185219\n",
            "resetting env. episode 651.000000, reward total was -19.000000. running mean: -20.173367\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.181633\n",
            "resetting env. episode 653.000000, reward total was -19.000000. running mean: -20.169817\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -20.178118\n",
            "resetting env. episode 655.000000, reward total was -20.000000. running mean: -20.176337\n",
            "resetting env. episode 656.000000, reward total was -21.000000. running mean: -20.184574\n",
            "resetting env. episode 657.000000, reward total was -18.000000. running mean: -20.162728\n",
            "resetting env. episode 658.000000, reward total was -21.000000. running mean: -20.171101\n",
            "resetting env. episode 659.000000, reward total was -20.000000. running mean: -20.169390\n",
            "resetting env. episode 660.000000, reward total was -20.000000. running mean: -20.167696\n",
            "resetting env. episode 661.000000, reward total was -20.000000. running mean: -20.166019\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -20.164359\n",
            "resetting env. episode 663.000000, reward total was -20.000000. running mean: -20.162715\n",
            "resetting env. episode 664.000000, reward total was -20.000000. running mean: -20.161088\n",
            "resetting env. episode 665.000000, reward total was -19.000000. running mean: -20.149477\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -20.157982\n",
            "resetting env. episode 667.000000, reward total was -21.000000. running mean: -20.166403\n",
            "resetting env. episode 668.000000, reward total was -18.000000. running mean: -20.144739\n",
            "resetting env. episode 669.000000, reward total was -21.000000. running mean: -20.153291\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.161758\n",
            "resetting env. episode 671.000000, reward total was -20.000000. running mean: -20.160141\n",
            "resetting env. episode 672.000000, reward total was -20.000000. running mean: -20.158539\n",
            "resetting env. episode 673.000000, reward total was -20.000000. running mean: -20.156954\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -20.165384\n",
            "resetting env. episode 675.000000, reward total was -21.000000. running mean: -20.173731\n",
            "resetting env. episode 676.000000, reward total was -20.000000. running mean: -20.171993\n",
            "resetting env. episode 677.000000, reward total was -20.000000. running mean: -20.170273\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -20.178571\n",
            "resetting env. episode 679.000000, reward total was -19.000000. running mean: -20.166785\n",
            "resetting env. episode 680.000000, reward total was -19.000000. running mean: -20.155117\n",
            "resetting env. episode 681.000000, reward total was -18.000000. running mean: -20.133566\n",
            "resetting env. episode 682.000000, reward total was -21.000000. running mean: -20.142230\n",
            "resetting env. episode 683.000000, reward total was -21.000000. running mean: -20.150808\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -20.149300\n",
            "resetting env. episode 685.000000, reward total was -20.000000. running mean: -20.147807\n",
            "resetting env. episode 686.000000, reward total was -21.000000. running mean: -20.156329\n",
            "resetting env. episode 687.000000, reward total was -21.000000. running mean: -20.164765\n",
            "resetting env. episode 688.000000, reward total was -19.000000. running mean: -20.153118\n",
            "resetting env. episode 689.000000, reward total was -19.000000. running mean: -20.141587\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -20.140171\n",
            "resetting env. episode 691.000000, reward total was -21.000000. running mean: -20.148769\n",
            "resetting env. episode 692.000000, reward total was -20.000000. running mean: -20.147281\n",
            "resetting env. episode 693.000000, reward total was -19.000000. running mean: -20.135809\n",
            "resetting env. episode 694.000000, reward total was -18.000000. running mean: -20.114450\n",
            "resetting env. episode 695.000000, reward total was -20.000000. running mean: -20.113306\n",
            "resetting env. episode 696.000000, reward total was -20.000000. running mean: -20.112173\n",
            "resetting env. episode 697.000000, reward total was -21.000000. running mean: -20.121051\n",
            "resetting env. episode 698.000000, reward total was -19.000000. running mean: -20.109841\n",
            "resetting env. episode 699.000000, reward total was -19.000000. running mean: -20.098742\n",
            "resetting env. episode 700.000000, reward total was -21.000000. running mean: -20.107755\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.116677\n",
            "resetting env. episode 702.000000, reward total was -20.000000. running mean: -20.115511\n",
            "resetting env. episode 703.000000, reward total was -20.000000. running mean: -20.114355\n",
            "resetting env. episode 704.000000, reward total was -20.000000. running mean: -20.113212\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -20.112080\n",
            "resetting env. episode 706.000000, reward total was -20.000000. running mean: -20.110959\n",
            "resetting env. episode 707.000000, reward total was -19.000000. running mean: -20.099849\n",
            "resetting env. episode 708.000000, reward total was -21.000000. running mean: -20.108851\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -20.117762\n",
            "resetting env. episode 710.000000, reward total was -20.000000. running mean: -20.116585\n",
            "resetting env. episode 711.000000, reward total was -20.000000. running mean: -20.115419\n",
            "resetting env. episode 712.000000, reward total was -21.000000. running mean: -20.124265\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.133022\n",
            "resetting env. episode 714.000000, reward total was -19.000000. running mean: -20.121692\n",
            "resetting env. episode 715.000000, reward total was -20.000000. running mean: -20.120475\n",
            "resetting env. episode 716.000000, reward total was -20.000000. running mean: -20.119270\n",
            "resetting env. episode 717.000000, reward total was -20.000000. running mean: -20.118077\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -20.116897\n",
            "resetting env. episode 719.000000, reward total was -20.000000. running mean: -20.115728\n",
            "resetting env. episode 720.000000, reward total was -21.000000. running mean: -20.124570\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.133325\n",
            "resetting env. episode 722.000000, reward total was -20.000000. running mean: -20.131991\n",
            "resetting env. episode 723.000000, reward total was -19.000000. running mean: -20.120672\n",
            "resetting env. episode 724.000000, reward total was -21.000000. running mean: -20.129465\n",
            "resetting env. episode 725.000000, reward total was -21.000000. running mean: -20.138170\n",
            "resetting env. episode 726.000000, reward total was -20.000000. running mean: -20.136788\n",
            "resetting env. episode 727.000000, reward total was -20.000000. running mean: -20.135421\n",
            "resetting env. episode 728.000000, reward total was -19.000000. running mean: -20.124066\n",
            "resetting env. episode 729.000000, reward total was -20.000000. running mean: -20.122826\n",
            "resetting env. episode 730.000000, reward total was -19.000000. running mean: -20.111597\n",
            "resetting env. episode 731.000000, reward total was -20.000000. running mean: -20.110482\n",
            "resetting env. episode 732.000000, reward total was -20.000000. running mean: -20.109377\n",
            "resetting env. episode 733.000000, reward total was -21.000000. running mean: -20.118283\n",
            "resetting env. episode 734.000000, reward total was -20.000000. running mean: -20.117100\n",
            "resetting env. episode 735.000000, reward total was -20.000000. running mean: -20.115929\n",
            "resetting env. episode 736.000000, reward total was -18.000000. running mean: -20.094770\n",
            "resetting env. episode 737.000000, reward total was -20.000000. running mean: -20.093822\n",
            "resetting env. episode 738.000000, reward total was -19.000000. running mean: -20.082884\n",
            "resetting env. episode 739.000000, reward total was -21.000000. running mean: -20.092055\n",
            "resetting env. episode 740.000000, reward total was -20.000000. running mean: -20.091134\n",
            "resetting env. episode 741.000000, reward total was -20.000000. running mean: -20.090223\n",
            "resetting env. episode 742.000000, reward total was -20.000000. running mean: -20.089321\n",
            "resetting env. episode 743.000000, reward total was -20.000000. running mean: -20.088428\n",
            "resetting env. episode 744.000000, reward total was -20.000000. running mean: -20.087543\n",
            "resetting env. episode 745.000000, reward total was -21.000000. running mean: -20.096668\n",
            "resetting env. episode 746.000000, reward total was -21.000000. running mean: -20.105701\n",
            "resetting env. episode 747.000000, reward total was -21.000000. running mean: -20.114644\n",
            "resetting env. episode 748.000000, reward total was -21.000000. running mean: -20.123498\n",
            "resetting env. episode 749.000000, reward total was -19.000000. running mean: -20.112263\n",
            "resetting env. episode 750.000000, reward total was -21.000000. running mean: -20.121140\n",
            "resetting env. episode 751.000000, reward total was -20.000000. running mean: -20.119929\n",
            "resetting env. episode 752.000000, reward total was -17.000000. running mean: -20.088730\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.097842\n",
            "resetting env. episode 754.000000, reward total was -18.000000. running mean: -20.076864\n",
            "resetting env. episode 755.000000, reward total was -19.000000. running mean: -20.066095\n",
            "resetting env. episode 756.000000, reward total was -21.000000. running mean: -20.075434\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.084680\n",
            "resetting env. episode 758.000000, reward total was -20.000000. running mean: -20.083833\n",
            "resetting env. episode 759.000000, reward total was -20.000000. running mean: -20.082995\n",
            "resetting env. episode 760.000000, reward total was -21.000000. running mean: -20.092165\n",
            "resetting env. episode 761.000000, reward total was -21.000000. running mean: -20.101243\n",
            "resetting env. episode 762.000000, reward total was -21.000000. running mean: -20.110231\n",
            "resetting env. episode 763.000000, reward total was -19.000000. running mean: -20.099128\n",
            "resetting env. episode 764.000000, reward total was -21.000000. running mean: -20.108137\n",
            "resetting env. episode 765.000000, reward total was -20.000000. running mean: -20.107056\n",
            "resetting env. episode 766.000000, reward total was -19.000000. running mean: -20.095985\n",
            "resetting env. episode 767.000000, reward total was -19.000000. running mean: -20.085025\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -20.094175\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.103233\n",
            "resetting env. episode 770.000000, reward total was -20.000000. running mean: -20.102201\n",
            "resetting env. episode 771.000000, reward total was -20.000000. running mean: -20.101179\n",
            "resetting env. episode 772.000000, reward total was -20.000000. running mean: -20.100167\n",
            "resetting env. episode 773.000000, reward total was -21.000000. running mean: -20.109166\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.118074\n",
            "resetting env. episode 775.000000, reward total was -21.000000. running mean: -20.126893\n",
            "resetting env. episode 776.000000, reward total was -20.000000. running mean: -20.125624\n",
            "resetting env. episode 777.000000, reward total was -21.000000. running mean: -20.134368\n",
            "resetting env. episode 778.000000, reward total was -19.000000. running mean: -20.123024\n",
            "resetting env. episode 779.000000, reward total was -21.000000. running mean: -20.131794\n",
            "resetting env. episode 780.000000, reward total was -18.000000. running mean: -20.110476\n",
            "resetting env. episode 781.000000, reward total was -21.000000. running mean: -20.119371\n",
            "resetting env. episode 782.000000, reward total was -20.000000. running mean: -20.118178\n",
            "resetting env. episode 783.000000, reward total was -17.000000. running mean: -20.086996\n",
            "resetting env. episode 784.000000, reward total was -21.000000. running mean: -20.096126\n",
            "resetting env. episode 785.000000, reward total was -19.000000. running mean: -20.085165\n",
            "resetting env. episode 786.000000, reward total was -21.000000. running mean: -20.094313\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -20.103370\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -20.112336\n",
            "resetting env. episode 789.000000, reward total was -17.000000. running mean: -20.081213\n",
            "resetting env. episode 790.000000, reward total was -21.000000. running mean: -20.090401\n",
            "resetting env. episode 791.000000, reward total was -20.000000. running mean: -20.089497\n",
            "resetting env. episode 792.000000, reward total was -21.000000. running mean: -20.098602\n",
            "resetting env. episode 793.000000, reward total was -19.000000. running mean: -20.087616\n",
            "resetting env. episode 794.000000, reward total was -21.000000. running mean: -20.096740\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.105772\n",
            "resetting env. episode 796.000000, reward total was -20.000000. running mean: -20.104714\n",
            "resetting env. episode 797.000000, reward total was -21.000000. running mean: -20.113667\n",
            "resetting env. episode 798.000000, reward total was -21.000000. running mean: -20.122531\n",
            "resetting env. episode 799.000000, reward total was -20.000000. running mean: -20.121305\n",
            "resetting env. episode 800.000000, reward total was -18.000000. running mean: -20.100092\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -20.109091\n",
            "resetting env. episode 802.000000, reward total was -19.000000. running mean: -20.098000\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.107020\n",
            "resetting env. episode 804.000000, reward total was -20.000000. running mean: -20.105950\n",
            "resetting env. episode 805.000000, reward total was -19.000000. running mean: -20.094891\n",
            "resetting env. episode 806.000000, reward total was -18.000000. running mean: -20.073942\n",
            "resetting env. episode 807.000000, reward total was -20.000000. running mean: -20.073202\n",
            "resetting env. episode 808.000000, reward total was -21.000000. running mean: -20.082470\n",
            "resetting env. episode 809.000000, reward total was -20.000000. running mean: -20.081646\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -20.090829\n",
            "resetting env. episode 811.000000, reward total was -19.000000. running mean: -20.079921\n",
            "resetting env. episode 812.000000, reward total was -19.000000. running mean: -20.069122\n",
            "resetting env. episode 813.000000, reward total was -21.000000. running mean: -20.078430\n",
            "resetting env. episode 814.000000, reward total was -20.000000. running mean: -20.077646\n",
            "resetting env. episode 815.000000, reward total was -18.000000. running mean: -20.056870\n",
            "resetting env. episode 816.000000, reward total was -18.000000. running mean: -20.036301\n",
            "resetting env. episode 817.000000, reward total was -17.000000. running mean: -20.005938\n",
            "resetting env. episode 818.000000, reward total was -21.000000. running mean: -20.015879\n",
            "resetting env. episode 819.000000, reward total was -20.000000. running mean: -20.015720\n",
            "resetting env. episode 820.000000, reward total was -20.000000. running mean: -20.015563\n",
            "resetting env. episode 821.000000, reward total was -19.000000. running mean: -20.005407\n",
            "resetting env. episode 822.000000, reward total was -20.000000. running mean: -20.005353\n",
            "resetting env. episode 823.000000, reward total was -19.000000. running mean: -19.995299\n",
            "resetting env. episode 824.000000, reward total was -19.000000. running mean: -19.985346\n",
            "resetting env. episode 825.000000, reward total was -19.000000. running mean: -19.975493\n",
            "resetting env. episode 826.000000, reward total was -20.000000. running mean: -19.975738\n",
            "resetting env. episode 827.000000, reward total was -20.000000. running mean: -19.975981\n",
            "resetting env. episode 828.000000, reward total was -19.000000. running mean: -19.966221\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -19.976559\n",
            "resetting env. episode 830.000000, reward total was -21.000000. running mean: -19.986793\n",
            "resetting env. episode 831.000000, reward total was -21.000000. running mean: -19.996925\n",
            "resetting env. episode 832.000000, reward total was -20.000000. running mean: -19.996956\n",
            "resetting env. episode 833.000000, reward total was -21.000000. running mean: -20.006986\n",
            "resetting env. episode 834.000000, reward total was -19.000000. running mean: -19.996916\n",
            "resetting env. episode 835.000000, reward total was -20.000000. running mean: -19.996947\n",
            "resetting env. episode 836.000000, reward total was -21.000000. running mean: -20.006978\n",
            "resetting env. episode 837.000000, reward total was -20.000000. running mean: -20.006908\n",
            "resetting env. episode 838.000000, reward total was -20.000000. running mean: -20.006839\n",
            "resetting env. episode 839.000000, reward total was -21.000000. running mean: -20.016771\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -20.026603\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -20.036337\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -20.045973\n",
            "resetting env. episode 843.000000, reward total was -20.000000. running mean: -20.045514\n",
            "resetting env. episode 844.000000, reward total was -18.000000. running mean: -20.025059\n",
            "resetting env. episode 845.000000, reward total was -18.000000. running mean: -20.004808\n",
            "resetting env. episode 846.000000, reward total was -21.000000. running mean: -20.014760\n",
            "resetting env. episode 847.000000, reward total was -19.000000. running mean: -20.004612\n",
            "resetting env. episode 848.000000, reward total was -20.000000. running mean: -20.004566\n",
            "resetting env. episode 849.000000, reward total was -20.000000. running mean: -20.004521\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -20.014475\n",
            "resetting env. episode 851.000000, reward total was -19.000000. running mean: -20.004331\n",
            "resetting env. episode 852.000000, reward total was -20.000000. running mean: -20.004287\n",
            "resetting env. episode 853.000000, reward total was -18.000000. running mean: -19.984244\n",
            "resetting env. episode 854.000000, reward total was -21.000000. running mean: -19.994402\n",
            "resetting env. episode 855.000000, reward total was -21.000000. running mean: -20.004458\n",
            "resetting env. episode 856.000000, reward total was -20.000000. running mean: -20.004413\n",
            "resetting env. episode 857.000000, reward total was -21.000000. running mean: -20.014369\n",
            "resetting env. episode 858.000000, reward total was -21.000000. running mean: -20.024226\n",
            "resetting env. episode 859.000000, reward total was -20.000000. running mean: -20.023983\n",
            "resetting env. episode 860.000000, reward total was -20.000000. running mean: -20.023743\n",
            "resetting env. episode 861.000000, reward total was -20.000000. running mean: -20.023506\n",
            "resetting env. episode 862.000000, reward total was -20.000000. running mean: -20.023271\n",
            "resetting env. episode 863.000000, reward total was -19.000000. running mean: -20.013038\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.022908\n",
            "resetting env. episode 865.000000, reward total was -20.000000. running mean: -20.022679\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -20.032452\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -20.042127\n",
            "resetting env. episode 868.000000, reward total was -21.000000. running mean: -20.051706\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -20.061189\n",
            "resetting env. episode 870.000000, reward total was -21.000000. running mean: -20.070577\n",
            "resetting env. episode 871.000000, reward total was -20.000000. running mean: -20.069871\n",
            "resetting env. episode 872.000000, reward total was -19.000000. running mean: -20.059173\n",
            "resetting env. episode 873.000000, reward total was -19.000000. running mean: -20.048581\n",
            "resetting env. episode 874.000000, reward total was -21.000000. running mean: -20.058095\n",
            "resetting env. episode 875.000000, reward total was -20.000000. running mean: -20.057514\n",
            "resetting env. episode 876.000000, reward total was -21.000000. running mean: -20.066939\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.076270\n",
            "resetting env. episode 878.000000, reward total was -21.000000. running mean: -20.085507\n",
            "resetting env. episode 879.000000, reward total was -20.000000. running mean: -20.084652\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -20.093805\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.102867\n",
            "resetting env. episode 882.000000, reward total was -18.000000. running mean: -20.081839\n",
            "resetting env. episode 883.000000, reward total was -21.000000. running mean: -20.091020\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -20.100110\n",
            "resetting env. episode 885.000000, reward total was -21.000000. running mean: -20.109109\n",
            "resetting env. episode 886.000000, reward total was -19.000000. running mean: -20.098018\n",
            "resetting env. episode 887.000000, reward total was -19.000000. running mean: -20.087038\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -20.096167\n",
            "resetting env. episode 889.000000, reward total was -21.000000. running mean: -20.105206\n",
            "resetting env. episode 890.000000, reward total was -20.000000. running mean: -20.104154\n",
            "resetting env. episode 891.000000, reward total was -20.000000. running mean: -20.103112\n",
            "resetting env. episode 892.000000, reward total was -21.000000. running mean: -20.112081\n",
            "resetting env. episode 893.000000, reward total was -20.000000. running mean: -20.110960\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -20.119851\n",
            "resetting env. episode 895.000000, reward total was -21.000000. running mean: -20.128652\n",
            "resetting env. episode 896.000000, reward total was -21.000000. running mean: -20.137366\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.145992\n",
            "resetting env. episode 898.000000, reward total was -20.000000. running mean: -20.144532\n",
            "resetting env. episode 899.000000, reward total was -18.000000. running mean: -20.123087\n",
            "resetting env. episode 900.000000, reward total was -21.000000. running mean: -20.131856\n",
            "resetting env. episode 901.000000, reward total was -16.000000. running mean: -20.090537\n",
            "resetting env. episode 902.000000, reward total was -19.000000. running mean: -20.079632\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -20.088836\n",
            "resetting env. episode 904.000000, reward total was -20.000000. running mean: -20.087947\n",
            "resetting env. episode 905.000000, reward total was -20.000000. running mean: -20.087068\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -20.096197\n",
            "resetting env. episode 907.000000, reward total was -21.000000. running mean: -20.105235\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -20.114183\n",
            "resetting env. episode 909.000000, reward total was -20.000000. running mean: -20.113041\n",
            "resetting env. episode 910.000000, reward total was -20.000000. running mean: -20.111910\n",
            "resetting env. episode 911.000000, reward total was -20.000000. running mean: -20.110791\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -20.119683\n",
            "resetting env. episode 913.000000, reward total was -21.000000. running mean: -20.128487\n",
            "resetting env. episode 914.000000, reward total was -20.000000. running mean: -20.127202\n",
            "resetting env. episode 915.000000, reward total was -21.000000. running mean: -20.135930\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -20.144570\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -20.153125\n",
            "resetting env. episode 918.000000, reward total was -21.000000. running mean: -20.161593\n",
            "resetting env. episode 919.000000, reward total was -19.000000. running mean: -20.149978\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -20.158478\n",
            "resetting env. episode 921.000000, reward total was -21.000000. running mean: -20.166893\n",
            "resetting env. episode 922.000000, reward total was -21.000000. running mean: -20.175224\n",
            "resetting env. episode 923.000000, reward total was -21.000000. running mean: -20.183472\n",
            "resetting env. episode 924.000000, reward total was -20.000000. running mean: -20.181637\n",
            "resetting env. episode 925.000000, reward total was -21.000000. running mean: -20.189821\n",
            "resetting env. episode 926.000000, reward total was -21.000000. running mean: -20.197923\n",
            "resetting env. episode 927.000000, reward total was -20.000000. running mean: -20.195943\n",
            "resetting env. episode 928.000000, reward total was -20.000000. running mean: -20.193984\n",
            "resetting env. episode 929.000000, reward total was -21.000000. running mean: -20.202044\n",
            "resetting env. episode 930.000000, reward total was -20.000000. running mean: -20.200024\n",
            "resetting env. episode 931.000000, reward total was -19.000000. running mean: -20.188023\n",
            "resetting env. episode 932.000000, reward total was -20.000000. running mean: -20.186143\n",
            "resetting env. episode 933.000000, reward total was -21.000000. running mean: -20.194282\n",
            "resetting env. episode 934.000000, reward total was -21.000000. running mean: -20.202339\n",
            "resetting env. episode 935.000000, reward total was -20.000000. running mean: -20.200315\n",
            "resetting env. episode 936.000000, reward total was -21.000000. running mean: -20.208312\n",
            "resetting env. episode 937.000000, reward total was -21.000000. running mean: -20.216229\n",
            "resetting env. episode 938.000000, reward total was -20.000000. running mean: -20.214067\n",
            "resetting env. episode 939.000000, reward total was -20.000000. running mean: -20.211926\n",
            "resetting env. episode 940.000000, reward total was -21.000000. running mean: -20.219807\n",
            "resetting env. episode 941.000000, reward total was -21.000000. running mean: -20.227609\n",
            "resetting env. episode 942.000000, reward total was -19.000000. running mean: -20.215333\n",
            "resetting env. episode 943.000000, reward total was -21.000000. running mean: -20.223180\n",
            "resetting env. episode 944.000000, reward total was -18.000000. running mean: -20.200948\n",
            "resetting env. episode 945.000000, reward total was -21.000000. running mean: -20.208938\n",
            "resetting env. episode 946.000000, reward total was -20.000000. running mean: -20.206849\n",
            "resetting env. episode 947.000000, reward total was -21.000000. running mean: -20.214780\n",
            "resetting env. episode 948.000000, reward total was -18.000000. running mean: -20.192633\n",
            "resetting env. episode 949.000000, reward total was -20.000000. running mean: -20.190706\n",
            "resetting env. episode 950.000000, reward total was -18.000000. running mean: -20.168799\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.177111\n",
            "resetting env. episode 952.000000, reward total was -19.000000. running mean: -20.165340\n",
            "resetting env. episode 953.000000, reward total was -19.000000. running mean: -20.153687\n",
            "resetting env. episode 954.000000, reward total was -20.000000. running mean: -20.152150\n",
            "resetting env. episode 955.000000, reward total was -21.000000. running mean: -20.160628\n",
            "resetting env. episode 956.000000, reward total was -20.000000. running mean: -20.159022\n",
            "resetting env. episode 957.000000, reward total was -21.000000. running mean: -20.167432\n",
            "resetting env. episode 958.000000, reward total was -19.000000. running mean: -20.155757\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -20.164200\n",
            "resetting env. episode 960.000000, reward total was -20.000000. running mean: -20.162558\n",
            "resetting env. episode 961.000000, reward total was -21.000000. running mean: -20.170932\n",
            "resetting env. episode 962.000000, reward total was -20.000000. running mean: -20.169223\n",
            "resetting env. episode 963.000000, reward total was -20.000000. running mean: -20.167531\n",
            "resetting env. episode 964.000000, reward total was -19.000000. running mean: -20.155855\n",
            "resetting env. episode 965.000000, reward total was -19.000000. running mean: -20.144297\n",
            "resetting env. episode 966.000000, reward total was -21.000000. running mean: -20.152854\n",
            "resetting env. episode 967.000000, reward total was -20.000000. running mean: -20.151325\n",
            "resetting env. episode 968.000000, reward total was -20.000000. running mean: -20.149812\n",
            "resetting env. episode 969.000000, reward total was -18.000000. running mean: -20.128314\n",
            "resetting env. episode 970.000000, reward total was -20.000000. running mean: -20.127031\n",
            "resetting env. episode 971.000000, reward total was -18.000000. running mean: -20.105761\n",
            "resetting env. episode 972.000000, reward total was -19.000000. running mean: -20.094703\n",
            "resetting env. episode 973.000000, reward total was -20.000000. running mean: -20.093756\n",
            "resetting env. episode 974.000000, reward total was -20.000000. running mean: -20.092818\n",
            "resetting env. episode 975.000000, reward total was -21.000000. running mean: -20.101890\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -20.110871\n",
            "resetting env. episode 977.000000, reward total was -21.000000. running mean: -20.119763\n",
            "resetting env. episode 978.000000, reward total was -21.000000. running mean: -20.128565\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.137279\n",
            "resetting env. episode 980.000000, reward total was -20.000000. running mean: -20.135907\n",
            "resetting env. episode 981.000000, reward total was -20.000000. running mean: -20.134547\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.143202\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.151770\n",
            "resetting env. episode 984.000000, reward total was -20.000000. running mean: -20.150252\n",
            "resetting env. episode 985.000000, reward total was -21.000000. running mean: -20.158750\n",
            "resetting env. episode 986.000000, reward total was -20.000000. running mean: -20.157162\n",
            "resetting env. episode 987.000000, reward total was -21.000000. running mean: -20.165591\n",
            "resetting env. episode 988.000000, reward total was -20.000000. running mean: -20.163935\n",
            "resetting env. episode 989.000000, reward total was -21.000000. running mean: -20.172295\n",
            "resetting env. episode 990.000000, reward total was -20.000000. running mean: -20.170572\n",
            "resetting env. episode 991.000000, reward total was -21.000000. running mean: -20.178867\n",
            "resetting env. episode 992.000000, reward total was -21.000000. running mean: -20.187078\n",
            "resetting env. episode 993.000000, reward total was -20.000000. running mean: -20.185207\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.193355\n",
            "resetting env. episode 995.000000, reward total was -19.000000. running mean: -20.181422\n",
            "resetting env. episode 996.000000, reward total was -21.000000. running mean: -20.189607\n",
            "resetting env. episode 997.000000, reward total was -19.000000. running mean: -20.177711\n",
            "resetting env. episode 998.000000, reward total was -20.000000. running mean: -20.175934\n",
            "resetting env. episode 999.000000, reward total was -21.000000. running mean: -20.184175\n",
            "resetting env. episode 1000.000000, reward total was -21.000000. running mean: -20.192333\n",
            "resetting env. episode 1001.000000, reward total was -19.000000. running mean: -20.180410\n",
            "resetting env. episode 1002.000000, reward total was -20.000000. running mean: -20.178606\n",
            "resetting env. episode 1003.000000, reward total was -21.000000. running mean: -20.186820\n",
            "resetting env. episode 1004.000000, reward total was -18.000000. running mean: -20.164951\n",
            "resetting env. episode 1005.000000, reward total was -18.000000. running mean: -20.143302\n",
            "resetting env. episode 1006.000000, reward total was -20.000000. running mean: -20.141869\n",
            "resetting env. episode 1007.000000, reward total was -20.000000. running mean: -20.140450\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -20.149046\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -20.157555\n",
            "resetting env. episode 1010.000000, reward total was -20.000000. running mean: -20.155980\n",
            "resetting env. episode 1011.000000, reward total was -21.000000. running mean: -20.164420\n",
            "resetting env. episode 1012.000000, reward total was -17.000000. running mean: -20.132776\n",
            "resetting env. episode 1013.000000, reward total was -19.000000. running mean: -20.121448\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -20.130233\n",
            "resetting env. episode 1015.000000, reward total was -21.000000. running mean: -20.138931\n",
            "resetting env. episode 1016.000000, reward total was -20.000000. running mean: -20.137542\n",
            "resetting env. episode 1017.000000, reward total was -20.000000. running mean: -20.136166\n",
            "resetting env. episode 1018.000000, reward total was -21.000000. running mean: -20.144805\n",
            "resetting env. episode 1019.000000, reward total was -20.000000. running mean: -20.143357\n",
            "resetting env. episode 1020.000000, reward total was -20.000000. running mean: -20.141923\n",
            "resetting env. episode 1021.000000, reward total was -20.000000. running mean: -20.140504\n",
            "resetting env. episode 1022.000000, reward total was -19.000000. running mean: -20.129099\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -20.137808\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.146430\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -20.154965\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -20.163416\n",
            "resetting env. episode 1027.000000, reward total was -20.000000. running mean: -20.161782\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -20.170164\n",
            "resetting env. episode 1029.000000, reward total was -20.000000. running mean: -20.168462\n",
            "resetting env. episode 1030.000000, reward total was -21.000000. running mean: -20.176778\n",
            "resetting env. episode 1031.000000, reward total was -19.000000. running mean: -20.165010\n",
            "resetting env. episode 1032.000000, reward total was -18.000000. running mean: -20.143360\n",
            "resetting env. episode 1033.000000, reward total was -21.000000. running mean: -20.151926\n",
            "resetting env. episode 1034.000000, reward total was -18.000000. running mean: -20.130407\n",
            "resetting env. episode 1035.000000, reward total was -18.000000. running mean: -20.109103\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -20.118012\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.126832\n",
            "resetting env. episode 1038.000000, reward total was -21.000000. running mean: -20.135563\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.144208\n",
            "resetting env. episode 1040.000000, reward total was -21.000000. running mean: -20.152766\n",
            "resetting env. episode 1041.000000, reward total was -19.000000. running mean: -20.141238\n",
            "resetting env. episode 1042.000000, reward total was -21.000000. running mean: -20.149826\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -20.158327\n",
            "resetting env. episode 1044.000000, reward total was -18.000000. running mean: -20.136744\n",
            "resetting env. episode 1045.000000, reward total was -19.000000. running mean: -20.125377\n",
            "resetting env. episode 1046.000000, reward total was -20.000000. running mean: -20.124123\n",
            "resetting env. episode 1047.000000, reward total was -20.000000. running mean: -20.122882\n",
            "resetting env. episode 1048.000000, reward total was -20.000000. running mean: -20.121653\n",
            "resetting env. episode 1049.000000, reward total was -20.000000. running mean: -20.120436\n",
            "resetting env. episode 1050.000000, reward total was -20.000000. running mean: -20.119232\n",
            "resetting env. episode 1051.000000, reward total was -20.000000. running mean: -20.118040\n",
            "resetting env. episode 1052.000000, reward total was -20.000000. running mean: -20.116859\n",
            "resetting env. episode 1053.000000, reward total was -20.000000. running mean: -20.115691\n",
            "resetting env. episode 1054.000000, reward total was -17.000000. running mean: -20.084534\n",
            "resetting env. episode 1055.000000, reward total was -21.000000. running mean: -20.093688\n",
            "resetting env. episode 1056.000000, reward total was -21.000000. running mean: -20.102751\n",
            "resetting env. episode 1057.000000, reward total was -19.000000. running mean: -20.091724\n",
            "resetting env. episode 1058.000000, reward total was -19.000000. running mean: -20.080807\n",
            "resetting env. episode 1059.000000, reward total was -20.000000. running mean: -20.079999\n",
            "resetting env. episode 1060.000000, reward total was -20.000000. running mean: -20.079199\n",
            "resetting env. episode 1061.000000, reward total was -21.000000. running mean: -20.088407\n",
            "resetting env. episode 1062.000000, reward total was -20.000000. running mean: -20.087523\n",
            "resetting env. episode 1063.000000, reward total was -20.000000. running mean: -20.086647\n",
            "resetting env. episode 1064.000000, reward total was -20.000000. running mean: -20.085781\n",
            "resetting env. episode 1065.000000, reward total was -21.000000. running mean: -20.094923\n",
            "resetting env. episode 1066.000000, reward total was -20.000000. running mean: -20.093974\n",
            "resetting env. episode 1067.000000, reward total was -19.000000. running mean: -20.083034\n",
            "resetting env. episode 1068.000000, reward total was -20.000000. running mean: -20.082204\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -20.091382\n",
            "resetting env. episode 1070.000000, reward total was -20.000000. running mean: -20.090468\n",
            "resetting env. episode 1071.000000, reward total was -16.000000. running mean: -20.049563\n",
            "resetting env. episode 1072.000000, reward total was -20.000000. running mean: -20.049068\n",
            "resetting env. episode 1073.000000, reward total was -20.000000. running mean: -20.048577\n",
            "resetting env. episode 1074.000000, reward total was -20.000000. running mean: -20.048091\n",
            "resetting env. episode 1075.000000, reward total was -20.000000. running mean: -20.047610\n",
            "resetting env. episode 1076.000000, reward total was -19.000000. running mean: -20.037134\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -20.046763\n",
            "resetting env. episode 1078.000000, reward total was -21.000000. running mean: -20.056295\n",
            "resetting env. episode 1079.000000, reward total was -19.000000. running mean: -20.045732\n",
            "resetting env. episode 1080.000000, reward total was -20.000000. running mean: -20.045275\n",
            "resetting env. episode 1081.000000, reward total was -20.000000. running mean: -20.044822\n",
            "resetting env. episode 1082.000000, reward total was -19.000000. running mean: -20.034374\n",
            "resetting env. episode 1083.000000, reward total was -21.000000. running mean: -20.044030\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -20.053590\n",
            "resetting env. episode 1085.000000, reward total was -21.000000. running mean: -20.063054\n",
            "resetting env. episode 1086.000000, reward total was -20.000000. running mean: -20.062423\n",
            "resetting env. episode 1087.000000, reward total was -19.000000. running mean: -20.051799\n",
            "resetting env. episode 1088.000000, reward total was -20.000000. running mean: -20.051281\n",
            "resetting env. episode 1089.000000, reward total was -19.000000. running mean: -20.040768\n",
            "resetting env. episode 1090.000000, reward total was -19.000000. running mean: -20.030361\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.040057\n",
            "resetting env. episode 1092.000000, reward total was -19.000000. running mean: -20.029657\n",
            "resetting env. episode 1093.000000, reward total was -20.000000. running mean: -20.029360\n",
            "resetting env. episode 1094.000000, reward total was -20.000000. running mean: -20.029066\n",
            "resetting env. episode 1095.000000, reward total was -20.000000. running mean: -20.028776\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -20.038488\n",
            "resetting env. episode 1097.000000, reward total was -20.000000. running mean: -20.038103\n",
            "resetting env. episode 1098.000000, reward total was -20.000000. running mean: -20.037722\n",
            "resetting env. episode 1099.000000, reward total was -20.000000. running mean: -20.037345\n",
            "resetting env. episode 1100.000000, reward total was -21.000000. running mean: -20.046971\n",
            "resetting env. episode 1101.000000, reward total was -19.000000. running mean: -20.036502\n",
            "resetting env. episode 1102.000000, reward total was -21.000000. running mean: -20.046137\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -20.055675\n",
            "resetting env. episode 1104.000000, reward total was -20.000000. running mean: -20.055119\n",
            "resetting env. episode 1105.000000, reward total was -20.000000. running mean: -20.054567\n",
            "resetting env. episode 1106.000000, reward total was -20.000000. running mean: -20.054022\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -20.063481\n",
            "resetting env. episode 1108.000000, reward total was -21.000000. running mean: -20.072847\n",
            "resetting env. episode 1109.000000, reward total was -20.000000. running mean: -20.072118\n",
            "resetting env. episode 1110.000000, reward total was -18.000000. running mean: -20.051397\n",
            "resetting env. episode 1111.000000, reward total was -21.000000. running mean: -20.060883\n",
            "resetting env. episode 1112.000000, reward total was -21.000000. running mean: -20.070274\n",
            "resetting env. episode 1113.000000, reward total was -21.000000. running mean: -20.079571\n",
            "resetting env. episode 1114.000000, reward total was -21.000000. running mean: -20.088776\n",
            "resetting env. episode 1115.000000, reward total was -19.000000. running mean: -20.077888\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.087109\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -20.096238\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -20.105276\n",
            "resetting env. episode 1119.000000, reward total was -20.000000. running mean: -20.104223\n",
            "resetting env. episode 1120.000000, reward total was -21.000000. running mean: -20.113181\n",
            "resetting env. episode 1121.000000, reward total was -21.000000. running mean: -20.122049\n",
            "resetting env. episode 1122.000000, reward total was -19.000000. running mean: -20.110828\n",
            "resetting env. episode 1123.000000, reward total was -20.000000. running mean: -20.109720\n",
            "resetting env. episode 1124.000000, reward total was -19.000000. running mean: -20.098623\n",
            "resetting env. episode 1125.000000, reward total was -20.000000. running mean: -20.097637\n",
            "resetting env. episode 1126.000000, reward total was -20.000000. running mean: -20.096660\n",
            "resetting env. episode 1127.000000, reward total was -19.000000. running mean: -20.085694\n",
            "resetting env. episode 1128.000000, reward total was -18.000000. running mean: -20.064837\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -20.074188\n",
            "resetting env. episode 1130.000000, reward total was -19.000000. running mean: -20.063446\n",
            "resetting env. episode 1131.000000, reward total was -20.000000. running mean: -20.062812\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -20.072184\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -20.081462\n",
            "resetting env. episode 1134.000000, reward total was -20.000000. running mean: -20.080647\n",
            "resetting env. episode 1135.000000, reward total was -18.000000. running mean: -20.059841\n",
            "resetting env. episode 1136.000000, reward total was -19.000000. running mean: -20.049243\n",
            "resetting env. episode 1137.000000, reward total was -21.000000. running mean: -20.058750\n",
            "resetting env. episode 1138.000000, reward total was -21.000000. running mean: -20.068163\n",
            "resetting env. episode 1139.000000, reward total was -20.000000. running mean: -20.067481\n",
            "resetting env. episode 1140.000000, reward total was -21.000000. running mean: -20.076806\n",
            "resetting env. episode 1141.000000, reward total was -20.000000. running mean: -20.076038\n",
            "resetting env. episode 1142.000000, reward total was -19.000000. running mean: -20.065278\n",
            "resetting env. episode 1143.000000, reward total was -18.000000. running mean: -20.044625\n",
            "resetting env. episode 1144.000000, reward total was -20.000000. running mean: -20.044179\n",
            "resetting env. episode 1145.000000, reward total was -21.000000. running mean: -20.053737\n",
            "resetting env. episode 1146.000000, reward total was -19.000000. running mean: -20.043200\n",
            "resetting env. episode 1147.000000, reward total was -17.000000. running mean: -20.012768\n",
            "resetting env. episode 1148.000000, reward total was -21.000000. running mean: -20.022640\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -20.032414\n",
            "resetting env. episode 1150.000000, reward total was -20.000000. running mean: -20.032089\n",
            "resetting env. episode 1151.000000, reward total was -19.000000. running mean: -20.021768\n",
            "resetting env. episode 1152.000000, reward total was -20.000000. running mean: -20.021551\n",
            "resetting env. episode 1153.000000, reward total was -20.000000. running mean: -20.021335\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -20.031122\n",
            "resetting env. episode 1155.000000, reward total was -18.000000. running mean: -20.010811\n",
            "resetting env. episode 1156.000000, reward total was -20.000000. running mean: -20.010703\n",
            "resetting env. episode 1157.000000, reward total was -20.000000. running mean: -20.010596\n",
            "resetting env. episode 1158.000000, reward total was -20.000000. running mean: -20.010490\n",
            "resetting env. episode 1159.000000, reward total was -18.000000. running mean: -19.990385\n",
            "resetting env. episode 1160.000000, reward total was -20.000000. running mean: -19.990481\n",
            "resetting env. episode 1161.000000, reward total was -19.000000. running mean: -19.980576\n",
            "resetting env. episode 1162.000000, reward total was -20.000000. running mean: -19.980770\n",
            "resetting env. episode 1163.000000, reward total was -19.000000. running mean: -19.970963\n",
            "resetting env. episode 1164.000000, reward total was -21.000000. running mean: -19.981253\n",
            "resetting env. episode 1165.000000, reward total was -20.000000. running mean: -19.981440\n",
            "resetting env. episode 1166.000000, reward total was -18.000000. running mean: -19.961626\n",
            "resetting env. episode 1167.000000, reward total was -18.000000. running mean: -19.942010\n",
            "resetting env. episode 1168.000000, reward total was -19.000000. running mean: -19.932590\n",
            "resetting env. episode 1169.000000, reward total was -21.000000. running mean: -19.943264\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -19.953831\n",
            "resetting env. episode 1171.000000, reward total was -18.000000. running mean: -19.934293\n",
            "resetting env. episode 1172.000000, reward total was -21.000000. running mean: -19.944950\n",
            "resetting env. episode 1173.000000, reward total was -19.000000. running mean: -19.935500\n",
            "resetting env. episode 1174.000000, reward total was -20.000000. running mean: -19.936145\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -19.946784\n",
            "resetting env. episode 1176.000000, reward total was -18.000000. running mean: -19.927316\n",
            "resetting env. episode 1177.000000, reward total was -21.000000. running mean: -19.938043\n",
            "resetting env. episode 1178.000000, reward total was -19.000000. running mean: -19.928663\n",
            "resetting env. episode 1179.000000, reward total was -20.000000. running mean: -19.929376\n",
            "resetting env. episode 1180.000000, reward total was -18.000000. running mean: -19.910082\n",
            "resetting env. episode 1181.000000, reward total was -19.000000. running mean: -19.900981\n",
            "resetting env. episode 1182.000000, reward total was -20.000000. running mean: -19.901972\n",
            "resetting env. episode 1183.000000, reward total was -20.000000. running mean: -19.902952\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -19.913922\n",
            "resetting env. episode 1185.000000, reward total was -20.000000. running mean: -19.914783\n",
            "resetting env. episode 1186.000000, reward total was -20.000000. running mean: -19.915635\n",
            "resetting env. episode 1187.000000, reward total was -20.000000. running mean: -19.916479\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -19.927314\n",
            "resetting env. episode 1189.000000, reward total was -19.000000. running mean: -19.918041\n",
            "resetting env. episode 1190.000000, reward total was -21.000000. running mean: -19.928861\n",
            "resetting env. episode 1191.000000, reward total was -21.000000. running mean: -19.939572\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -19.950176\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -19.960674\n",
            "resetting env. episode 1194.000000, reward total was -21.000000. running mean: -19.971068\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -19.981357\n",
            "resetting env. episode 1196.000000, reward total was -20.000000. running mean: -19.981543\n",
            "resetting env. episode 1197.000000, reward total was -20.000000. running mean: -19.981728\n",
            "resetting env. episode 1198.000000, reward total was -21.000000. running mean: -19.991911\n",
            "resetting env. episode 1199.000000, reward total was -20.000000. running mean: -19.991992\n",
            "resetting env. episode 1200.000000, reward total was -20.000000. running mean: -19.992072\n",
            "resetting env. episode 1201.000000, reward total was -20.000000. running mean: -19.992151\n",
            "resetting env. episode 1202.000000, reward total was -19.000000. running mean: -19.982229\n",
            "resetting env. episode 1203.000000, reward total was -20.000000. running mean: -19.982407\n",
            "resetting env. episode 1204.000000, reward total was -19.000000. running mean: -19.972583\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -19.982857\n",
            "resetting env. episode 1206.000000, reward total was -19.000000. running mean: -19.973029\n",
            "resetting env. episode 1207.000000, reward total was -19.000000. running mean: -19.963298\n",
            "resetting env. episode 1208.000000, reward total was -20.000000. running mean: -19.963665\n",
            "resetting env. episode 1209.000000, reward total was -19.000000. running mean: -19.954029\n",
            "resetting env. episode 1210.000000, reward total was -19.000000. running mean: -19.944489\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -19.955044\n",
            "resetting env. episode 1212.000000, reward total was -20.000000. running mean: -19.955493\n",
            "resetting env. episode 1213.000000, reward total was -19.000000. running mean: -19.945938\n",
            "resetting env. episode 1214.000000, reward total was -19.000000. running mean: -19.936479\n",
            "resetting env. episode 1215.000000, reward total was -21.000000. running mean: -19.947114\n",
            "resetting env. episode 1216.000000, reward total was -20.000000. running mean: -19.947643\n",
            "resetting env. episode 1217.000000, reward total was -20.000000. running mean: -19.948167\n",
            "resetting env. episode 1218.000000, reward total was -20.000000. running mean: -19.948685\n",
            "resetting env. episode 1219.000000, reward total was -19.000000. running mean: -19.939198\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -19.949806\n",
            "resetting env. episode 1221.000000, reward total was -21.000000. running mean: -19.960308\n",
            "resetting env. episode 1222.000000, reward total was -21.000000. running mean: -19.970705\n",
            "resetting env. episode 1223.000000, reward total was -21.000000. running mean: -19.980998\n",
            "resetting env. episode 1224.000000, reward total was -21.000000. running mean: -19.991188\n",
            "resetting env. episode 1225.000000, reward total was -20.000000. running mean: -19.991276\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -20.001363\n",
            "resetting env. episode 1227.000000, reward total was -20.000000. running mean: -20.001350\n",
            "resetting env. episode 1228.000000, reward total was -20.000000. running mean: -20.001336\n",
            "resetting env. episode 1229.000000, reward total was -18.000000. running mean: -19.981323\n",
            "resetting env. episode 1230.000000, reward total was -21.000000. running mean: -19.991509\n",
            "resetting env. episode 1231.000000, reward total was -19.000000. running mean: -19.981594\n",
            "resetting env. episode 1232.000000, reward total was -21.000000. running mean: -19.991778\n",
            "resetting env. episode 1233.000000, reward total was -17.000000. running mean: -19.961861\n",
            "resetting env. episode 1234.000000, reward total was -21.000000. running mean: -19.972242\n",
            "resetting env. episode 1235.000000, reward total was -20.000000. running mean: -19.972520\n",
            "resetting env. episode 1236.000000, reward total was -20.000000. running mean: -19.972794\n",
            "resetting env. episode 1237.000000, reward total was -19.000000. running mean: -19.963067\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -19.973436\n",
            "resetting env. episode 1239.000000, reward total was -21.000000. running mean: -19.983701\n",
            "resetting env. episode 1240.000000, reward total was -19.000000. running mean: -19.973864\n",
            "resetting env. episode 1241.000000, reward total was -21.000000. running mean: -19.984126\n",
            "resetting env. episode 1242.000000, reward total was -21.000000. running mean: -19.994285\n",
            "resetting env. episode 1243.000000, reward total was -19.000000. running mean: -19.984342\n",
            "resetting env. episode 1244.000000, reward total was -21.000000. running mean: -19.994498\n",
            "resetting env. episode 1245.000000, reward total was -20.000000. running mean: -19.994553\n",
            "resetting env. episode 1246.000000, reward total was -21.000000. running mean: -20.004608\n",
            "resetting env. episode 1247.000000, reward total was -19.000000. running mean: -19.994562\n",
            "resetting env. episode 1248.000000, reward total was -21.000000. running mean: -20.004616\n",
            "resetting env. episode 1249.000000, reward total was -19.000000. running mean: -19.994570\n",
            "resetting env. episode 1250.000000, reward total was -21.000000. running mean: -20.004624\n",
            "resetting env. episode 1251.000000, reward total was -21.000000. running mean: -20.014578\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -20.024432\n",
            "resetting env. episode 1253.000000, reward total was -21.000000. running mean: -20.034188\n",
            "resetting env. episode 1254.000000, reward total was -20.000000. running mean: -20.033846\n",
            "resetting env. episode 1255.000000, reward total was -19.000000. running mean: -20.023508\n",
            "resetting env. episode 1256.000000, reward total was -21.000000. running mean: -20.033272\n",
            "resetting env. episode 1257.000000, reward total was -19.000000. running mean: -20.022940\n",
            "resetting env. episode 1258.000000, reward total was -20.000000. running mean: -20.022710\n",
            "resetting env. episode 1259.000000, reward total was -20.000000. running mean: -20.022483\n",
            "resetting env. episode 1260.000000, reward total was -21.000000. running mean: -20.032258\n",
            "resetting env. episode 1261.000000, reward total was -20.000000. running mean: -20.031936\n",
            "resetting env. episode 1262.000000, reward total was -21.000000. running mean: -20.041616\n",
            "resetting env. episode 1263.000000, reward total was -19.000000. running mean: -20.031200\n",
            "resetting env. episode 1264.000000, reward total was -21.000000. running mean: -20.040888\n",
            "resetting env. episode 1265.000000, reward total was -20.000000. running mean: -20.040479\n",
            "resetting env. episode 1266.000000, reward total was -19.000000. running mean: -20.030075\n",
            "resetting env. episode 1267.000000, reward total was -21.000000. running mean: -20.039774\n",
            "resetting env. episode 1268.000000, reward total was -21.000000. running mean: -20.049376\n",
            "resetting env. episode 1269.000000, reward total was -20.000000. running mean: -20.048882\n",
            "resetting env. episode 1270.000000, reward total was -20.000000. running mean: -20.048394\n",
            "resetting env. episode 1271.000000, reward total was -20.000000. running mean: -20.047910\n",
            "resetting env. episode 1272.000000, reward total was -17.000000. running mean: -20.017431\n",
            "resetting env. episode 1273.000000, reward total was -19.000000. running mean: -20.007256\n",
            "resetting env. episode 1274.000000, reward total was -21.000000. running mean: -20.017184\n",
            "resetting env. episode 1275.000000, reward total was -21.000000. running mean: -20.027012\n",
            "resetting env. episode 1276.000000, reward total was -20.000000. running mean: -20.026742\n",
            "resetting env. episode 1277.000000, reward total was -21.000000. running mean: -20.036474\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -20.046110\n",
            "resetting env. episode 1279.000000, reward total was -19.000000. running mean: -20.035648\n",
            "resetting env. episode 1280.000000, reward total was -19.000000. running mean: -20.025292\n",
            "resetting env. episode 1281.000000, reward total was -21.000000. running mean: -20.035039\n",
            "resetting env. episode 1282.000000, reward total was -21.000000. running mean: -20.044689\n",
            "resetting env. episode 1283.000000, reward total was -20.000000. running mean: -20.044242\n",
            "resetting env. episode 1284.000000, reward total was -20.000000. running mean: -20.043799\n",
            "resetting env. episode 1285.000000, reward total was -21.000000. running mean: -20.053361\n",
            "resetting env. episode 1286.000000, reward total was -21.000000. running mean: -20.062828\n",
            "resetting env. episode 1287.000000, reward total was -19.000000. running mean: -20.052199\n",
            "resetting env. episode 1288.000000, reward total was -21.000000. running mean: -20.061677\n",
            "resetting env. episode 1289.000000, reward total was -21.000000. running mean: -20.071061\n",
            "resetting env. episode 1290.000000, reward total was -20.000000. running mean: -20.070350\n",
            "resetting env. episode 1291.000000, reward total was -20.000000. running mean: -20.069647\n",
            "resetting env. episode 1292.000000, reward total was -19.000000. running mean: -20.058950\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -20.068361\n",
            "resetting env. episode 1294.000000, reward total was -20.000000. running mean: -20.067677\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -20.077000\n",
            "resetting env. episode 1296.000000, reward total was -20.000000. running mean: -20.076230\n",
            "resetting env. episode 1297.000000, reward total was -16.000000. running mean: -20.035468\n",
            "resetting env. episode 1298.000000, reward total was -20.000000. running mean: -20.035113\n",
            "resetting env. episode 1299.000000, reward total was -21.000000. running mean: -20.044762\n",
            "resetting env. episode 1300.000000, reward total was -20.000000. running mean: -20.044315\n",
            "resetting env. episode 1301.000000, reward total was -20.000000. running mean: -20.043871\n",
            "resetting env. episode 1302.000000, reward total was -20.000000. running mean: -20.043433\n",
            "resetting env. episode 1303.000000, reward total was -19.000000. running mean: -20.032998\n",
            "resetting env. episode 1304.000000, reward total was -21.000000. running mean: -20.042668\n",
            "resetting env. episode 1305.000000, reward total was -20.000000. running mean: -20.042242\n",
            "resetting env. episode 1306.000000, reward total was -19.000000. running mean: -20.031819\n",
            "resetting env. episode 1307.000000, reward total was -21.000000. running mean: -20.041501\n",
            "resetting env. episode 1308.000000, reward total was -20.000000. running mean: -20.041086\n",
            "resetting env. episode 1309.000000, reward total was -20.000000. running mean: -20.040675\n",
            "resetting env. episode 1310.000000, reward total was -20.000000. running mean: -20.040268\n",
            "resetting env. episode 1311.000000, reward total was -20.000000. running mean: -20.039866\n",
            "resetting env. episode 1312.000000, reward total was -19.000000. running mean: -20.029467\n",
            "resetting env. episode 1313.000000, reward total was -20.000000. running mean: -20.029172\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -20.038881\n",
            "resetting env. episode 1315.000000, reward total was -21.000000. running mean: -20.048492\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -20.058007\n",
            "resetting env. episode 1317.000000, reward total was -20.000000. running mean: -20.057427\n",
            "resetting env. episode 1318.000000, reward total was -21.000000. running mean: -20.066853\n",
            "resetting env. episode 1319.000000, reward total was -18.000000. running mean: -20.046184\n",
            "resetting env. episode 1320.000000, reward total was -20.000000. running mean: -20.045722\n",
            "resetting env. episode 1321.000000, reward total was -19.000000. running mean: -20.035265\n",
            "resetting env. episode 1322.000000, reward total was -21.000000. running mean: -20.044912\n",
            "resetting env. episode 1323.000000, reward total was -21.000000. running mean: -20.054463\n",
            "resetting env. episode 1324.000000, reward total was -20.000000. running mean: -20.053919\n",
            "resetting env. episode 1325.000000, reward total was -21.000000. running mean: -20.063379\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -20.072746\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -20.082018\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -20.091198\n",
            "resetting env. episode 1329.000000, reward total was -20.000000. running mean: -20.090286\n",
            "resetting env. episode 1330.000000, reward total was -18.000000. running mean: -20.069383\n",
            "resetting env. episode 1331.000000, reward total was -19.000000. running mean: -20.058689\n",
            "resetting env. episode 1332.000000, reward total was -21.000000. running mean: -20.068102\n",
            "resetting env. episode 1333.000000, reward total was -21.000000. running mean: -20.077421\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -20.086647\n",
            "resetting env. episode 1335.000000, reward total was -19.000000. running mean: -20.075781\n",
            "resetting env. episode 1336.000000, reward total was -18.000000. running mean: -20.055023\n",
            "resetting env. episode 1337.000000, reward total was -21.000000. running mean: -20.064473\n",
            "resetting env. episode 1338.000000, reward total was -21.000000. running mean: -20.073828\n",
            "resetting env. episode 1339.000000, reward total was -21.000000. running mean: -20.083090\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -20.092259\n",
            "resetting env. episode 1341.000000, reward total was -21.000000. running mean: -20.101336\n",
            "resetting env. episode 1342.000000, reward total was -20.000000. running mean: -20.100323\n",
            "resetting env. episode 1343.000000, reward total was -21.000000. running mean: -20.109320\n",
            "resetting env. episode 1344.000000, reward total was -21.000000. running mean: -20.118226\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.127044\n",
            "resetting env. episode 1346.000000, reward total was -20.000000. running mean: -20.125774\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -20.134516\n",
            "resetting env. episode 1348.000000, reward total was -19.000000. running mean: -20.123171\n",
            "resetting env. episode 1349.000000, reward total was -21.000000. running mean: -20.131939\n",
            "resetting env. episode 1350.000000, reward total was -21.000000. running mean: -20.140620\n",
            "resetting env. episode 1351.000000, reward total was -19.000000. running mean: -20.129214\n",
            "resetting env. episode 1352.000000, reward total was -20.000000. running mean: -20.127921\n",
            "resetting env. episode 1353.000000, reward total was -21.000000. running mean: -20.136642\n",
            "resetting env. episode 1354.000000, reward total was -21.000000. running mean: -20.145276\n",
            "resetting env. episode 1355.000000, reward total was -21.000000. running mean: -20.153823\n",
            "resetting env. episode 1356.000000, reward total was -20.000000. running mean: -20.152285\n",
            "resetting env. episode 1357.000000, reward total was -21.000000. running mean: -20.160762\n",
            "resetting env. episode 1358.000000, reward total was -20.000000. running mean: -20.159154\n",
            "resetting env. episode 1359.000000, reward total was -20.000000. running mean: -20.157563\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -20.165987\n",
            "resetting env. episode 1361.000000, reward total was -21.000000. running mean: -20.174327\n",
            "resetting env. episode 1362.000000, reward total was -20.000000. running mean: -20.172584\n",
            "resetting env. episode 1363.000000, reward total was -18.000000. running mean: -20.150858\n",
            "resetting env. episode 1364.000000, reward total was -19.000000. running mean: -20.139350\n",
            "resetting env. episode 1365.000000, reward total was -21.000000. running mean: -20.147956\n",
            "resetting env. episode 1366.000000, reward total was -21.000000. running mean: -20.156476\n",
            "resetting env. episode 1367.000000, reward total was -19.000000. running mean: -20.144912\n",
            "resetting env. episode 1368.000000, reward total was -20.000000. running mean: -20.143463\n",
            "resetting env. episode 1369.000000, reward total was -21.000000. running mean: -20.152028\n",
            "resetting env. episode 1370.000000, reward total was -21.000000. running mean: -20.160508\n",
            "resetting env. episode 1371.000000, reward total was -21.000000. running mean: -20.168903\n",
            "resetting env. episode 1372.000000, reward total was -20.000000. running mean: -20.167214\n",
            "resetting env. episode 1373.000000, reward total was -21.000000. running mean: -20.175541\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -20.183786\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -20.191948\n",
            "resetting env. episode 1376.000000, reward total was -20.000000. running mean: -20.190029\n",
            "resetting env. episode 1377.000000, reward total was -21.000000. running mean: -20.198128\n",
            "resetting env. episode 1378.000000, reward total was -21.000000. running mean: -20.206147\n",
            "resetting env. episode 1379.000000, reward total was -21.000000. running mean: -20.214086\n",
            "resetting env. episode 1380.000000, reward total was -20.000000. running mean: -20.211945\n",
            "resetting env. episode 1381.000000, reward total was -20.000000. running mean: -20.209825\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -20.217727\n",
            "resetting env. episode 1383.000000, reward total was -20.000000. running mean: -20.215550\n",
            "resetting env. episode 1384.000000, reward total was -20.000000. running mean: -20.213394\n",
            "resetting env. episode 1385.000000, reward total was -20.000000. running mean: -20.211260\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -20.219148\n",
            "resetting env. episode 1387.000000, reward total was -20.000000. running mean: -20.216956\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -20.224787\n",
            "resetting env. episode 1389.000000, reward total was -21.000000. running mean: -20.232539\n",
            "resetting env. episode 1390.000000, reward total was -20.000000. running mean: -20.230213\n",
            "resetting env. episode 1391.000000, reward total was -20.000000. running mean: -20.227911\n",
            "resetting env. episode 1392.000000, reward total was -19.000000. running mean: -20.215632\n",
            "resetting env. episode 1393.000000, reward total was -20.000000. running mean: -20.213476\n",
            "resetting env. episode 1394.000000, reward total was -19.000000. running mean: -20.201341\n",
            "resetting env. episode 1395.000000, reward total was -17.000000. running mean: -20.169328\n",
            "resetting env. episode 1396.000000, reward total was -18.000000. running mean: -20.147634\n",
            "resetting env. episode 1397.000000, reward total was -20.000000. running mean: -20.146158\n",
            "resetting env. episode 1398.000000, reward total was -19.000000. running mean: -20.134697\n",
            "resetting env. episode 1399.000000, reward total was -21.000000. running mean: -20.143350\n",
            "resetting env. episode 1400.000000, reward total was -21.000000. running mean: -20.151916\n",
            "resetting env. episode 1401.000000, reward total was -20.000000. running mean: -20.150397\n",
            "resetting env. episode 1402.000000, reward total was -18.000000. running mean: -20.128893\n",
            "resetting env. episode 1403.000000, reward total was -20.000000. running mean: -20.127604\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -20.136328\n",
            "resetting env. episode 1405.000000, reward total was -21.000000. running mean: -20.144965\n",
            "resetting env. episode 1406.000000, reward total was -21.000000. running mean: -20.153515\n",
            "resetting env. episode 1407.000000, reward total was -21.000000. running mean: -20.161980\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -20.160360\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -20.168757\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -20.177069\n",
            "resetting env. episode 1411.000000, reward total was -21.000000. running mean: -20.185298\n",
            "resetting env. episode 1412.000000, reward total was -20.000000. running mean: -20.183445\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -20.191611\n",
            "resetting env. episode 1414.000000, reward total was -20.000000. running mean: -20.189695\n",
            "resetting env. episode 1415.000000, reward total was -20.000000. running mean: -20.187798\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -20.195920\n",
            "resetting env. episode 1417.000000, reward total was -21.000000. running mean: -20.203961\n",
            "resetting env. episode 1418.000000, reward total was -19.000000. running mean: -20.191921\n",
            "resetting env. episode 1419.000000, reward total was -21.000000. running mean: -20.200002\n",
            "resetting env. episode 1420.000000, reward total was -21.000000. running mean: -20.208002\n",
            "resetting env. episode 1421.000000, reward total was -20.000000. running mean: -20.205922\n",
            "resetting env. episode 1422.000000, reward total was -19.000000. running mean: -20.193863\n",
            "resetting env. episode 1423.000000, reward total was -20.000000. running mean: -20.191924\n",
            "resetting env. episode 1424.000000, reward total was -19.000000. running mean: -20.180005\n",
            "resetting env. episode 1425.000000, reward total was -21.000000. running mean: -20.188205\n",
            "resetting env. episode 1426.000000, reward total was -19.000000. running mean: -20.176323\n",
            "resetting env. episode 1427.000000, reward total was -20.000000. running mean: -20.174559\n",
            "resetting env. episode 1428.000000, reward total was -19.000000. running mean: -20.162814\n",
            "resetting env. episode 1429.000000, reward total was -21.000000. running mean: -20.171186\n",
            "resetting env. episode 1430.000000, reward total was -20.000000. running mean: -20.169474\n",
            "resetting env. episode 1431.000000, reward total was -20.000000. running mean: -20.167779\n",
            "resetting env. episode 1432.000000, reward total was -19.000000. running mean: -20.156101\n",
            "resetting env. episode 1433.000000, reward total was -21.000000. running mean: -20.164540\n",
            "resetting env. episode 1434.000000, reward total was -20.000000. running mean: -20.162895\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -20.171266\n",
            "resetting env. episode 1436.000000, reward total was -20.000000. running mean: -20.169553\n",
            "resetting env. episode 1437.000000, reward total was -20.000000. running mean: -20.167858\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.176179\n",
            "resetting env. episode 1439.000000, reward total was -18.000000. running mean: -20.154417\n",
            "resetting env. episode 1440.000000, reward total was -18.000000. running mean: -20.132873\n",
            "resetting env. episode 1441.000000, reward total was -20.000000. running mean: -20.131544\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -20.140229\n",
            "resetting env. episode 1443.000000, reward total was -18.000000. running mean: -20.118827\n",
            "resetting env. episode 1444.000000, reward total was -18.000000. running mean: -20.097638\n",
            "resetting env. episode 1445.000000, reward total was -20.000000. running mean: -20.096662\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -20.105695\n",
            "resetting env. episode 1447.000000, reward total was -20.000000. running mean: -20.104638\n",
            "resetting env. episode 1448.000000, reward total was -19.000000. running mean: -20.093592\n",
            "resetting env. episode 1449.000000, reward total was -21.000000. running mean: -20.102656\n",
            "resetting env. episode 1450.000000, reward total was -20.000000. running mean: -20.101630\n",
            "resetting env. episode 1451.000000, reward total was -19.000000. running mean: -20.090613\n",
            "resetting env. episode 1452.000000, reward total was -20.000000. running mean: -20.089707\n",
            "resetting env. episode 1453.000000, reward total was -19.000000. running mean: -20.078810\n",
            "resetting env. episode 1454.000000, reward total was -19.000000. running mean: -20.068022\n",
            "resetting env. episode 1455.000000, reward total was -16.000000. running mean: -20.027342\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -20.037068\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -20.046698\n",
            "resetting env. episode 1458.000000, reward total was -21.000000. running mean: -20.056231\n",
            "resetting env. episode 1459.000000, reward total was -20.000000. running mean: -20.055668\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -20.065112\n",
            "resetting env. episode 1461.000000, reward total was -20.000000. running mean: -20.064461\n",
            "resetting env. episode 1462.000000, reward total was -21.000000. running mean: -20.073816\n",
            "resetting env. episode 1463.000000, reward total was -20.000000. running mean: -20.073078\n",
            "resetting env. episode 1464.000000, reward total was -20.000000. running mean: -20.072347\n",
            "resetting env. episode 1465.000000, reward total was -21.000000. running mean: -20.081624\n",
            "resetting env. episode 1466.000000, reward total was -20.000000. running mean: -20.080807\n",
            "resetting env. episode 1467.000000, reward total was -20.000000. running mean: -20.079999\n",
            "resetting env. episode 1468.000000, reward total was -21.000000. running mean: -20.089199\n",
            "resetting env. episode 1469.000000, reward total was -19.000000. running mean: -20.078307\n",
            "resetting env. episode 1470.000000, reward total was -19.000000. running mean: -20.067524\n",
            "resetting env. episode 1471.000000, reward total was -21.000000. running mean: -20.076849\n",
            "resetting env. episode 1472.000000, reward total was -19.000000. running mean: -20.066080\n",
            "resetting env. episode 1473.000000, reward total was -20.000000. running mean: -20.065420\n",
            "resetting env. episode 1474.000000, reward total was -20.000000. running mean: -20.064765\n",
            "resetting env. episode 1475.000000, reward total was -21.000000. running mean: -20.074118\n",
            "resetting env. episode 1476.000000, reward total was -20.000000. running mean: -20.073377\n",
            "resetting env. episode 1477.000000, reward total was -20.000000. running mean: -20.072643\n",
            "resetting env. episode 1478.000000, reward total was -18.000000. running mean: -20.051916\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -20.061397\n",
            "resetting env. episode 1480.000000, reward total was -21.000000. running mean: -20.070783\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -20.080075\n",
            "resetting env. episode 1482.000000, reward total was -21.000000. running mean: -20.089275\n",
            "resetting env. episode 1483.000000, reward total was -20.000000. running mean: -20.088382\n",
            "resetting env. episode 1484.000000, reward total was -17.000000. running mean: -20.057498\n",
            "resetting env. episode 1485.000000, reward total was -21.000000. running mean: -20.066923\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -20.076254\n",
            "resetting env. episode 1487.000000, reward total was -20.000000. running mean: -20.075491\n",
            "resetting env. episode 1488.000000, reward total was -21.000000. running mean: -20.084736\n",
            "resetting env. episode 1489.000000, reward total was -21.000000. running mean: -20.093889\n",
            "resetting env. episode 1490.000000, reward total was -21.000000. running mean: -20.102950\n",
            "resetting env. episode 1491.000000, reward total was -19.000000. running mean: -20.091921\n",
            "resetting env. episode 1492.000000, reward total was -20.000000. running mean: -20.091002\n",
            "resetting env. episode 1493.000000, reward total was -19.000000. running mean: -20.080091\n",
            "resetting env. episode 1494.000000, reward total was -20.000000. running mean: -20.079291\n",
            "resetting env. episode 1495.000000, reward total was -20.000000. running mean: -20.078498\n",
            "resetting env. episode 1496.000000, reward total was -21.000000. running mean: -20.087713\n",
            "resetting env. episode 1497.000000, reward total was -20.000000. running mean: -20.086836\n",
            "resetting env. episode 1498.000000, reward total was -20.000000. running mean: -20.085967\n",
            "resetting env. episode 1499.000000, reward total was -20.000000. running mean: -20.085108\n",
            "resetting env. episode 1500.000000, reward total was -19.000000. running mean: -20.074256\n",
            "CPU times: user 1h 52min 31s, sys: 34min 25s, total: 2h 26min 56s\n",
            "Wall time: 1h 15min 18s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "c2105f07-3164-4abc-95df-19878d1bbc8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}