{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "H=600_le-4 (Original).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "cell_type": "code",
      "source": [
        "!pip install JSAnimation >/dev/null"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "547a5c2c-3886-48d0-e440-da630589eaf8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=6072c3db858f3a287057a5bc7b109aea62ed74f4542d55f5f0c15e8416f33df7\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "440a7bbe-31cd-46b8-cf04-237bd89c3433",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "e48152ea-40f0-4a72-a5ff-b0f58fed44f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "9c401886-7fcd-4a06-a636-b2c442a2c9d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "f1719852-1fb5-44e3-916e-da9c5df573ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -14.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 600 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "# learning_rate = 1e-4\n",
        "learning_rate = 1e-4\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "78476004-99f2-4c4b-c5cf-ef0ecdb5e287",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.990000\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.990100\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.990199\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -20.980297\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.980494\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.980689\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.980882\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.981073\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -20.971263\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.971550\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.971835\n",
            "resetting env. episode 16.000000, reward total was -17.000000. running mean: -20.932116\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.922795\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.923567\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.924331\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.925088\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.915837\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.916679\n",
            "resetting env. episode 23.000000, reward total was -18.000000. running mean: -20.887512\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.888637\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.889751\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.880853\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.882045\n",
            "resetting env. episode 28.000000, reward total was -20.000000. running mean: -20.873224\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.874492\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.865747\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.867089\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.868419\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.869734\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.871037\n",
            "resetting env. episode 35.000000, reward total was -19.000000. running mean: -20.852327\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.843803\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.845365\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -20.826912\n",
            "resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.828643\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.830356\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.832053\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.833732\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.835395\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.837041\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.838670\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -20.830284\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.831981\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.823661\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.825424\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.827170\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.828898\n",
            "resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.820609\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -20.812403\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.814279\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.816137\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -20.807975\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.809895\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.811797\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.813679\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.815542\n",
            "resetting env. episode 61.000000, reward total was -19.000000. running mean: -20.797386\n",
            "resetting env. episode 62.000000, reward total was -18.000000. running mean: -20.769412\n",
            "resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.761718\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.764101\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.766460\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.768796\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.771108\n",
            "resetting env. episode 68.000000, reward total was -20.000000. running mean: -20.763397\n",
            "resetting env. episode 69.000000, reward total was -20.000000. running mean: -20.755763\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.758205\n",
            "resetting env. episode 71.000000, reward total was -17.000000. running mean: -20.720623\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.723417\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.726182\n",
            "resetting env. episode 74.000000, reward total was -19.000000. running mean: -20.708921\n",
            "resetting env. episode 75.000000, reward total was -19.000000. running mean: -20.691831\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.694913\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.697964\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.700984\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.693975\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -20.677035\n",
            "resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.670264\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.673562\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.666826\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.670158\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.663456\n",
            "resetting env. episode 86.000000, reward total was -19.000000. running mean: -20.646822\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.640354\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.643950\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.647511\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.651035\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.654525\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.647980\n",
            "resetting env. episode 93.000000, reward total was -19.000000. running mean: -20.631500\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -20.625185\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.628933\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.632644\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.636317\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.639954\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.643555\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.647119\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.650648\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.654141\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -20.637600\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.631224\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.624912\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.628663\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.622376\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.626152\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.619891\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.623692\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.627455\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.631180\n",
            "resetting env. episode 113.000000, reward total was -19.000000. running mean: -20.614869\n",
            "resetting env. episode 114.000000, reward total was -17.000000. running mean: -20.578720\n",
            "resetting env. episode 115.000000, reward total was -20.000000. running mean: -20.572933\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -20.567203\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.571531\n",
            "resetting env. episode 118.000000, reward total was -19.000000. running mean: -20.555816\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -20.550258\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.554755\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.549208\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.543716\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.548279\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.542796\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.547368\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.551894\n",
            "resetting env. episode 127.000000, reward total was -19.000000. running mean: -20.536375\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.541011\n",
            "resetting env. episode 129.000000, reward total was -20.000000. running mean: -20.535601\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.540245\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.544843\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.539394\n",
            "resetting env. episode 133.000000, reward total was -19.000000. running mean: -20.524000\n",
            "resetting env. episode 134.000000, reward total was -18.000000. running mean: -20.498760\n",
            "resetting env. episode 135.000000, reward total was -19.000000. running mean: -20.483773\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.478935\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.484146\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.479304\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.484511\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.489666\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.494769\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.489822\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.484924\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.490074\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.495174\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.490222\n",
            "resetting env. episode 147.000000, reward total was -21.000000. running mean: -20.495320\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.500366\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.495363\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.500409\n",
            "resetting env. episode 151.000000, reward total was -19.000000. running mean: -20.485405\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.490551\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.495645\n",
            "resetting env. episode 154.000000, reward total was -18.000000. running mean: -20.470689\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.465982\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.471322\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.476609\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.471843\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.467125\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.472453\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.477729\n",
            "resetting env. episode 162.000000, reward total was -19.000000. running mean: -20.462952\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.458322\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.453739\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.459201\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.454609\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.460063\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.465463\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.460808\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.466200\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.471538\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.466823\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.452154\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.447633\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.453156\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.448625\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.454139\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.459597\n",
            "resetting env. episode 179.000000, reward total was -19.000000. running mean: -20.445001\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.450551\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.456046\n",
            "resetting env. episode 182.000000, reward total was -19.000000. running mean: -20.441485\n",
            "resetting env. episode 183.000000, reward total was -20.000000. running mean: -20.437070\n",
            "resetting env. episode 184.000000, reward total was -18.000000. running mean: -20.412700\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.418573\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.424387\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.430143\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.425842\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.431583\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.437267\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.442895\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.448466\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -20.433981\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.439641\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.435245\n",
            "resetting env. episode 196.000000, reward total was -19.000000. running mean: -20.420893\n",
            "resetting env. episode 197.000000, reward total was -21.000000. running mean: -20.426684\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.422417\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.428193\n",
            "resetting env. episode 200.000000, reward total was -19.000000. running mean: -20.413911\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.419772\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.425574\n",
            "resetting env. episode 203.000000, reward total was -18.000000. running mean: -20.401318\n",
            "resetting env. episode 204.000000, reward total was -20.000000. running mean: -20.397305\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.403332\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.399299\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.405306\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.411253\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.417140\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.412969\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.418839\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.424651\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.430404\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.436100\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.431739\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.437422\n",
            "resetting env. episode 217.000000, reward total was -21.000000. running mean: -20.443047\n",
            "resetting env. episode 218.000000, reward total was -19.000000. running mean: -20.428617\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.424331\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.420087\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.425887\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.431628\n",
            "resetting env. episode 223.000000, reward total was -19.000000. running mean: -20.417311\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.423138\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.428907\n",
            "resetting env. episode 226.000000, reward total was -19.000000. running mean: -20.414618\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.420472\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.416267\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.422104\n",
            "resetting env. episode 230.000000, reward total was -21.000000. running mean: -20.427883\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.433604\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.429268\n",
            "resetting env. episode 233.000000, reward total was -20.000000. running mean: -20.424976\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.430726\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.436419\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.442054\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.437634\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.433258\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.438925\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.434536\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.440190\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.445788\n",
            "resetting env. episode 243.000000, reward total was -19.000000. running mean: -20.431331\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.437017\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.442647\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.448221\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.443738\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.449301\n",
            "resetting env. episode 249.000000, reward total was -20.000000. running mean: -20.444808\n",
            "resetting env. episode 250.000000, reward total was -19.000000. running mean: -20.430360\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.436056\n",
            "resetting env. episode 252.000000, reward total was -19.000000. running mean: -20.421696\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.417479\n",
            "resetting env. episode 254.000000, reward total was -19.000000. running mean: -20.403304\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.409271\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.405178\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.411127\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.407015\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.412945\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.408816\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.414728\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.420580\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.426374\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.422111\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.427890\n",
            "resetting env. episode 266.000000, reward total was -19.000000. running mean: -20.413611\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.419475\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.425280\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.421027\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.416817\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.422649\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.428422\n",
            "resetting env. episode 273.000000, reward total was -20.000000. running mean: -20.424138\n",
            "resetting env. episode 274.000000, reward total was -20.000000. running mean: -20.419897\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.425698\n",
            "resetting env. episode 276.000000, reward total was -21.000000. running mean: -20.431441\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.427126\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.422855\n",
            "resetting env. episode 279.000000, reward total was -19.000000. running mean: -20.408626\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.404540\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.410495\n",
            "resetting env. episode 282.000000, reward total was -21.000000. running mean: -20.416390\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.422226\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.428004\n",
            "resetting env. episode 285.000000, reward total was -19.000000. running mean: -20.413724\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.409586\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -20.395490\n",
            "resetting env. episode 288.000000, reward total was -19.000000. running mean: -20.381536\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.387720\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.383843\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.390005\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.386105\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.392243\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.398321\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.404338\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.410294\n",
            "resetting env. episode 297.000000, reward total was -20.000000. running mean: -20.406191\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.412130\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.408008\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.413928\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.419789\n",
            "resetting env. episode 302.000000, reward total was -18.000000. running mean: -20.395591\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.401635\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.407619\n",
            "resetting env. episode 305.000000, reward total was -19.000000. running mean: -20.393543\n",
            "resetting env. episode 306.000000, reward total was -19.000000. running mean: -20.379607\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.385811\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.391953\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.378033\n",
            "resetting env. episode 310.000000, reward total was -20.000000. running mean: -20.374253\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.380511\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.376705\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.382938\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.389109\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.395218\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.401266\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.397253\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.403281\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.409248\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.405155\n",
            "resetting env. episode 321.000000, reward total was -19.000000. running mean: -20.391104\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.397193\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.403221\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.409189\n",
            "resetting env. episode 325.000000, reward total was -20.000000. running mean: -20.405097\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.401046\n",
            "resetting env. episode 327.000000, reward total was -19.000000. running mean: -20.387035\n",
            "resetting env. episode 328.000000, reward total was -20.000000. running mean: -20.383165\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.389333\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.385440\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.391586\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.387670\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.393793\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.399855\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.395856\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.401898\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.407879\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.413800\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.419662\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.425466\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.431211\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.436899\n",
            "resetting env. episode 343.000000, reward total was -21.000000. running mean: -20.442530\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.438104\n",
            "resetting env. episode 345.000000, reward total was -21.000000. running mean: -20.443723\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.439286\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.434893\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.440544\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.446139\n",
            "resetting env. episode 350.000000, reward total was -18.000000. running mean: -20.421678\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.427461\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.423186\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.418954\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.424765\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.430517\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -20.416212\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.422050\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.427829\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.433551\n",
            "resetting env. episode 360.000000, reward total was -19.000000. running mean: -20.419216\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.425023\n",
            "resetting env. episode 362.000000, reward total was -19.000000. running mean: -20.410773\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.406665\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.402599\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.408573\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.414487\n",
            "resetting env. episode 367.000000, reward total was -17.000000. running mean: -20.380342\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.376539\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.372773\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.379046\n",
            "resetting env. episode 371.000000, reward total was -20.000000. running mean: -20.375255\n",
            "resetting env. episode 372.000000, reward total was -19.000000. running mean: -20.361503\n",
            "resetting env. episode 373.000000, reward total was -20.000000. running mean: -20.357888\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.364309\n",
            "resetting env. episode 375.000000, reward total was -20.000000. running mean: -20.360666\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.367059\n",
            "resetting env. episode 377.000000, reward total was -19.000000. running mean: -20.353388\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.349855\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.356356\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.352792\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.349264\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.355772\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.362214\n",
            "resetting env. episode 384.000000, reward total was -20.000000. running mean: -20.358592\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.365006\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.361356\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.357742\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.364165\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.370523\n",
            "resetting env. episode 390.000000, reward total was -19.000000. running mean: -20.356818\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.363250\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.369617\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.375921\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.382162\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.378340\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.384557\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.390711\n",
            "resetting env. episode 398.000000, reward total was -20.000000. running mean: -20.386804\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.382936\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.389107\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.385216\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.391364\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.387450\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.393576\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.399640\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.405643\n",
            "resetting env. episode 407.000000, reward total was -19.000000. running mean: -20.391587\n",
            "resetting env. episode 408.000000, reward total was -20.000000. running mean: -20.387671\n",
            "resetting env. episode 409.000000, reward total was -19.000000. running mean: -20.373794\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.370056\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.376356\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.382592\n",
            "resetting env. episode 413.000000, reward total was -20.000000. running mean: -20.378766\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.384979\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.381129\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.387318\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.393445\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.399510\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.395515\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.391560\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.397644\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.403668\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.399631\n",
            "resetting env. episode 424.000000, reward total was -19.000000. running mean: -20.385635\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.391778\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.387861\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.393982\n",
            "resetting env. episode 428.000000, reward total was -19.000000. running mean: -20.380042\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.386242\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.392379\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.388456\n",
            "resetting env. episode 432.000000, reward total was -20.000000. running mean: -20.384571\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.380725\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.386918\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.393049\n",
            "resetting env. episode 436.000000, reward total was -18.000000. running mean: -20.369118\n",
            "resetting env. episode 437.000000, reward total was -20.000000. running mean: -20.365427\n",
            "resetting env. episode 438.000000, reward total was -18.000000. running mean: -20.341773\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.348355\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.354872\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.361323\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.367710\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.374033\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.380292\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.386489\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.382624\n",
            "resetting env. episode 447.000000, reward total was -20.000000. running mean: -20.378798\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.375010\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.381260\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.387448\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.393573\n",
            "resetting env. episode 452.000000, reward total was -21.000000. running mean: -20.399637\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.405641\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.411585\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.417469\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.413294\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.409161\n",
            "resetting env. episode 458.000000, reward total was -20.000000. running mean: -20.405069\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.411019\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.406909\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.402840\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.408811\n",
            "resetting env. episode 463.000000, reward total was -19.000000. running mean: -20.394723\n",
            "resetting env. episode 464.000000, reward total was -18.000000. running mean: -20.370776\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.377068\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.383297\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.389464\n",
            "resetting env. episode 468.000000, reward total was -19.000000. running mean: -20.375570\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.381814\n",
            "resetting env. episode 470.000000, reward total was -20.000000. running mean: -20.377996\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.384216\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.390374\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.396470\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.392505\n",
            "resetting env. episode 475.000000, reward total was -19.000000. running mean: -20.378580\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.384794\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.390947\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.387037\n",
            "resetting env. episode 479.000000, reward total was -18.000000. running mean: -20.363167\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.359535\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.355940\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.352380\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.358856\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.355268\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.351715\n",
            "resetting env. episode 486.000000, reward total was -20.000000. running mean: -20.348198\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.354716\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.361169\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.367557\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.373882\n",
            "resetting env. episode 491.000000, reward total was -20.000000. running mean: -20.370143\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.366441\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.362777\n",
            "resetting env. episode 494.000000, reward total was -18.000000. running mean: -20.339149\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.335758\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.342400\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.348976\n",
            "resetting env. episode 498.000000, reward total was -17.000000. running mean: -20.315486\n",
            "resetting env. episode 499.000000, reward total was -19.000000. running mean: -20.302332\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.309308\n",
            "CPU times: user 47min 48s, sys: 11min 54s, total: 59min 42s\n",
            "Wall time: 30min 35s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "8cd3e437-4773-4d2e-b5cd-dd036598f105",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -19.000000. running mean: -19.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -19.010000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -19.029900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -19.049601\n",
            "resetting env. episode 5.000000, reward total was -19.000000. running mean: -19.049105\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -19.068614\n",
            "resetting env. episode 7.000000, reward total was -18.000000. running mean: -19.057928\n",
            "resetting env. episode 8.000000, reward total was -20.000000. running mean: -19.067349\n",
            "resetting env. episode 9.000000, reward total was -19.000000. running mean: -19.066675\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -19.086008\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -19.095148\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -19.114197\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -19.133055\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -19.141724\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -19.160307\n",
            "resetting env. episode 16.000000, reward total was -19.000000. running mean: -19.158704\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -19.167117\n",
            "resetting env. episode 18.000000, reward total was -19.000000. running mean: -19.165446\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -19.183791\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -19.201953\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -19.209934\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -19.227834\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -19.245556\n",
            "resetting env. episode 24.000000, reward total was -20.000000. running mean: -19.253101\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -19.270570\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -19.287864\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -19.304985\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -19.321935\n",
            "resetting env. episode 29.000000, reward total was -20.000000. running mean: -19.328716\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -19.345429\n",
            "resetting env. episode 31.000000, reward total was -20.000000. running mean: -19.351975\n",
            "resetting env. episode 32.000000, reward total was -19.000000. running mean: -19.348455\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -19.364970\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -19.381321\n",
            "resetting env. episode 35.000000, reward total was -20.000000. running mean: -19.387507\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -19.383632\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -19.399796\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -19.415798\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -19.421640\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -19.437424\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -19.453049\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -19.458519\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -19.473934\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -19.489194\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -19.504302\n",
            "resetting env. episode 46.000000, reward total was -20.000000. running mean: -19.509259\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -19.514167\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -19.529025\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -19.533735\n",
            "resetting env. episode 50.000000, reward total was -19.000000. running mean: -19.528398\n",
            "resetting env. episode 51.000000, reward total was -18.000000. running mean: -19.513114\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -19.527982\n",
            "resetting env. episode 53.000000, reward total was -19.000000. running mean: -19.522703\n",
            "resetting env. episode 54.000000, reward total was -19.000000. running mean: -19.517476\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -19.532301\n",
            "resetting env. episode 56.000000, reward total was -20.000000. running mean: -19.536978\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -19.551608\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -19.556092\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -19.560531\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.574926\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -19.589176\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.603285\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -19.617252\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -19.631079\n",
            "resetting env. episode 65.000000, reward total was -20.000000. running mean: -19.634769\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -19.648421\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -19.651937\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.665417\n",
            "resetting env. episode 69.000000, reward total was -19.000000. running mean: -19.658763\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.672175\n",
            "resetting env. episode 71.000000, reward total was -19.000000. running mean: -19.665454\n",
            "resetting env. episode 72.000000, reward total was -20.000000. running mean: -19.668799\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -19.682111\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -19.695290\n",
            "resetting env. episode 75.000000, reward total was -19.000000. running mean: -19.688337\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -19.701454\n",
            "resetting env. episode 77.000000, reward total was -19.000000. running mean: -19.694439\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -19.707495\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -19.710420\n",
            "resetting env. episode 80.000000, reward total was -20.000000. running mean: -19.713316\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -19.726183\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -19.738921\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -19.751532\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -19.764016\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -19.766376\n",
            "resetting env. episode 86.000000, reward total was -19.000000. running mean: -19.758712\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -19.771125\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -19.783414\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -19.795580\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -19.807624\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -19.819548\n",
            "resetting env. episode 92.000000, reward total was -19.000000. running mean: -19.811352\n",
            "resetting env. episode 93.000000, reward total was -20.000000. running mean: -19.813239\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -19.825106\n",
            "resetting env. episode 95.000000, reward total was -20.000000. running mean: -19.826855\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -19.838587\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -19.850201\n",
            "resetting env. episode 98.000000, reward total was -20.000000. running mean: -19.851699\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -19.863182\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -19.874550\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -19.875805\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -19.877047\n",
            "resetting env. episode 103.000000, reward total was -19.000000. running mean: -19.868276\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -19.869593\n",
            "resetting env. episode 105.000000, reward total was -20.000000. running mean: -19.870897\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -19.882188\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -19.883366\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -19.884533\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -19.895688\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -19.906731\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -19.917663\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -19.928487\n",
            "resetting env. episode 113.000000, reward total was -19.000000. running mean: -19.919202\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -19.930010\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -19.940710\n",
            "resetting env. episode 116.000000, reward total was -20.000000. running mean: -19.941303\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -19.951890\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -19.962371\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -19.972747\n",
            "resetting env. episode 120.000000, reward total was -19.000000. running mean: -19.963020\n",
            "resetting env. episode 121.000000, reward total was -19.000000. running mean: -19.953389\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -19.963855\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -19.974217\n",
            "resetting env. episode 124.000000, reward total was -20.000000. running mean: -19.974475\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -19.974730\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -19.984983\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -19.995133\n",
            "resetting env. episode 128.000000, reward total was -18.000000. running mean: -19.975181\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -19.985430\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -19.995575\n",
            "resetting env. episode 131.000000, reward total was -19.000000. running mean: -19.985620\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -19.995763\n",
            "resetting env. episode 133.000000, reward total was -20.000000. running mean: -19.995806\n",
            "resetting env. episode 134.000000, reward total was -20.000000. running mean: -19.995848\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -19.995889\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.005930\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.015871\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.025712\n",
            "resetting env. episode 139.000000, reward total was -19.000000. running mean: -20.015455\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.015301\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.025148\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.024896\n",
            "resetting env. episode 143.000000, reward total was -19.000000. running mean: -20.014647\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.024501\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.034256\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.033913\n",
            "resetting env. episode 147.000000, reward total was -19.000000. running mean: -20.023574\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.033338\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.043005\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.052575\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.052049\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.061529\n",
            "resetting env. episode 153.000000, reward total was -18.000000. running mean: -20.040913\n",
            "resetting env. episode 154.000000, reward total was -19.000000. running mean: -20.030504\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.040199\n",
            "resetting env. episode 156.000000, reward total was -18.000000. running mean: -20.019797\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.029599\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.039303\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.048910\n",
            "resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.048421\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.057937\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.067358\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.076684\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.075917\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.075158\n",
            "resetting env. episode 166.000000, reward total was -20.000000. running mean: -20.074406\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.083662\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.092826\n",
            "resetting env. episode 169.000000, reward total was -19.000000. running mean: -20.081897\n",
            "resetting env. episode 170.000000, reward total was -18.000000. running mean: -20.061078\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.070468\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.079763\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.088965\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.098076\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.107095\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.116024\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.124864\n",
            "resetting env. episode 178.000000, reward total was -19.000000. running mean: -20.113615\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.122479\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.131254\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.129942\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.128642\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.137356\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.135982\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.134622\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.143276\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.141843\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.140425\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.129021\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.137731\n",
            "resetting env. episode 191.000000, reward total was -18.000000. running mean: -20.116353\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.125190\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.133938\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.142598\n",
            "resetting env. episode 195.000000, reward total was -18.000000. running mean: -20.121172\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.129961\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.128661\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.137374\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.136001\n",
            "resetting env. episode 200.000000, reward total was -18.000000. running mean: -20.114641\n",
            "resetting env. episode 201.000000, reward total was -19.000000. running mean: -20.103494\n",
            "resetting env. episode 202.000000, reward total was -19.000000. running mean: -20.092459\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.101535\n",
            "resetting env. episode 204.000000, reward total was -18.000000. running mean: -20.080519\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.079714\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.078917\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.088128\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.087247\n",
            "resetting env. episode 209.000000, reward total was -19.000000. running mean: -20.076374\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.075610\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.084854\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.094006\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.103066\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.112035\n",
            "resetting env. episode 215.000000, reward total was -20.000000. running mean: -20.110915\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.119806\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.118608\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.127421\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.136147\n",
            "resetting env. episode 220.000000, reward total was -16.000000. running mean: -20.094786\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.093838\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.092900\n",
            "resetting env. episode 223.000000, reward total was -21.000000. running mean: -20.101971\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.110951\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.109841\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.118743\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.127555\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.126280\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.135017\n",
            "resetting env. episode 230.000000, reward total was -19.000000. running mean: -20.123667\n",
            "resetting env. episode 231.000000, reward total was -16.000000. running mean: -20.082430\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.081606\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.090790\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.099882\n",
            "resetting env. episode 235.000000, reward total was -18.000000. running mean: -20.078883\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.088094\n",
            "resetting env. episode 237.000000, reward total was -21.000000. running mean: -20.097213\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.106241\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.115179\n",
            "resetting env. episode 240.000000, reward total was -19.000000. running mean: -20.104027\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.102987\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.111957\n",
            "resetting env. episode 243.000000, reward total was -19.000000. running mean: -20.100837\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.099829\n",
            "resetting env. episode 245.000000, reward total was -19.000000. running mean: -20.088831\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.087942\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.097063\n",
            "resetting env. episode 248.000000, reward total was -20.000000. running mean: -20.096092\n",
            "resetting env. episode 249.000000, reward total was -19.000000. running mean: -20.085131\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.084280\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.093437\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.102503\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.101478\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.110463\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.109359\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.118265\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.127082\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.125811\n",
            "resetting env. episode 259.000000, reward total was -21.000000. running mean: -20.134553\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.133208\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.141876\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.140457\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.149052\n",
            "resetting env. episode 264.000000, reward total was -21.000000. running mean: -20.157562\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.165986\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.164326\n",
            "resetting env. episode 267.000000, reward total was -19.000000. running mean: -20.152683\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.151156\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.159645\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.158048\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.166468\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.164803\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.173155\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.181424\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.189609\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.187713\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -20.175836\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.184078\n",
            "resetting env. episode 279.000000, reward total was -20.000000. running mean: -20.182237\n",
            "resetting env. episode 280.000000, reward total was -19.000000. running mean: -20.170415\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.178710\n",
            "resetting env. episode 282.000000, reward total was -19.000000. running mean: -20.166923\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.165254\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.173602\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.181866\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.190047\n",
            "resetting env. episode 287.000000, reward total was -21.000000. running mean: -20.198146\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.206165\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.214103\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.221962\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.229743\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.237445\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.245071\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.252620\n",
            "resetting env. episode 295.000000, reward total was -19.000000. running mean: -20.240094\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.237693\n",
            "resetting env. episode 297.000000, reward total was -19.000000. running mean: -20.225316\n",
            "resetting env. episode 298.000000, reward total was -21.000000. running mean: -20.233063\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.230732\n",
            "resetting env. episode 300.000000, reward total was -19.000000. running mean: -20.218425\n",
            "resetting env. episode 301.000000, reward total was -18.000000. running mean: -20.196241\n",
            "resetting env. episode 302.000000, reward total was -20.000000. running mean: -20.194278\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.202335\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.200312\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.208309\n",
            "resetting env. episode 306.000000, reward total was -19.000000. running mean: -20.196226\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.204264\n",
            "resetting env. episode 308.000000, reward total was -20.000000. running mean: -20.202221\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.190199\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.198297\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.206314\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.204251\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.212208\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.220086\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.227885\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.235606\n",
            "resetting env. episode 317.000000, reward total was -20.000000. running mean: -20.233250\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.240918\n",
            "resetting env. episode 319.000000, reward total was -19.000000. running mean: -20.228509\n",
            "resetting env. episode 320.000000, reward total was -19.000000. running mean: -20.216224\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.224061\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.231821\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.229502\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.237207\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.244835\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.252387\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.249863\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.257365\n",
            "resetting env. episode 329.000000, reward total was -19.000000. running mean: -20.244791\n",
            "resetting env. episode 330.000000, reward total was -17.000000. running mean: -20.212343\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.210220\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.218117\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.225936\n",
            "resetting env. episode 334.000000, reward total was -20.000000. running mean: -20.223677\n",
            "resetting env. episode 335.000000, reward total was -18.000000. running mean: -20.201440\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.209426\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.217331\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.225158\n",
            "resetting env. episode 339.000000, reward total was -19.000000. running mean: -20.212906\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.220777\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.228570\n",
            "resetting env. episode 342.000000, reward total was -20.000000. running mean: -20.226284\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.224021\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.231781\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.229463\n",
            "resetting env. episode 346.000000, reward total was -18.000000. running mean: -20.207168\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.205097\n",
            "resetting env. episode 348.000000, reward total was -19.000000. running mean: -20.193046\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.201115\n",
            "resetting env. episode 350.000000, reward total was -20.000000. running mean: -20.199104\n",
            "resetting env. episode 351.000000, reward total was -19.000000. running mean: -20.187113\n",
            "resetting env. episode 352.000000, reward total was -21.000000. running mean: -20.195242\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.193290\n",
            "resetting env. episode 354.000000, reward total was -17.000000. running mean: -20.161357\n",
            "resetting env. episode 355.000000, reward total was -19.000000. running mean: -20.149743\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -20.138246\n",
            "resetting env. episode 357.000000, reward total was -19.000000. running mean: -20.126863\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.125595\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.134339\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.142995\n",
            "resetting env. episode 361.000000, reward total was -21.000000. running mean: -20.151565\n",
            "resetting env. episode 362.000000, reward total was -21.000000. running mean: -20.160050\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.168449\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.166765\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.175097\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.183346\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.191513\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.199597\n",
            "resetting env. episode 369.000000, reward total was -19.000000. running mean: -20.187602\n",
            "resetting env. episode 370.000000, reward total was -19.000000. running mean: -20.175726\n",
            "resetting env. episode 371.000000, reward total was -19.000000. running mean: -20.163968\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.172329\n",
            "resetting env. episode 373.000000, reward total was -19.000000. running mean: -20.160605\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.158999\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.167409\n",
            "resetting env. episode 376.000000, reward total was -20.000000. running mean: -20.165735\n",
            "resetting env. episode 377.000000, reward total was -19.000000. running mean: -20.154078\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.152537\n",
            "resetting env. episode 379.000000, reward total was -20.000000. running mean: -20.151012\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.159502\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.157907\n",
            "resetting env. episode 382.000000, reward total was -20.000000. running mean: -20.156327\n",
            "resetting env. episode 383.000000, reward total was -20.000000. running mean: -20.154764\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.163217\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.171584\n",
            "resetting env. episode 386.000000, reward total was -19.000000. running mean: -20.159869\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.158270\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.156687\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.165120\n",
            "resetting env. episode 390.000000, reward total was -18.000000. running mean: -20.143469\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -20.132034\n",
            "resetting env. episode 392.000000, reward total was -20.000000. running mean: -20.130714\n",
            "resetting env. episode 393.000000, reward total was -20.000000. running mean: -20.129407\n",
            "resetting env. episode 394.000000, reward total was -19.000000. running mean: -20.118113\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.126932\n",
            "resetting env. episode 396.000000, reward total was -20.000000. running mean: -20.125662\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.124406\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.133162\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.131830\n",
            "resetting env. episode 400.000000, reward total was -19.000000. running mean: -20.120512\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.129307\n",
            "resetting env. episode 402.000000, reward total was -21.000000. running mean: -20.138014\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.146633\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.145167\n",
            "resetting env. episode 405.000000, reward total was -21.000000. running mean: -20.153715\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.152178\n",
            "resetting env. episode 407.000000, reward total was -20.000000. running mean: -20.150657\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.159150\n",
            "resetting env. episode 409.000000, reward total was -20.000000. running mean: -20.157558\n",
            "resetting env. episode 410.000000, reward total was -21.000000. running mean: -20.165983\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.174323\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.182580\n",
            "resetting env. episode 413.000000, reward total was -18.000000. running mean: -20.160754\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.169146\n",
            "resetting env. episode 415.000000, reward total was -21.000000. running mean: -20.177455\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.175680\n",
            "resetting env. episode 417.000000, reward total was -19.000000. running mean: -20.163924\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.172284\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.170562\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.168856\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.177167\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.185396\n",
            "resetting env. episode 423.000000, reward total was -19.000000. running mean: -20.173542\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.181806\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.189988\n",
            "resetting env. episode 426.000000, reward total was -20.000000. running mean: -20.188088\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.186208\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.194345\n",
            "resetting env. episode 429.000000, reward total was -20.000000. running mean: -20.192402\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.200478\n",
            "resetting env. episode 431.000000, reward total was -20.000000. running mean: -20.198473\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.206488\n",
            "resetting env. episode 433.000000, reward total was -20.000000. running mean: -20.204424\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.212379\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.220256\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.228053\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.235772\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.233415\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.241081\n",
            "resetting env. episode 440.000000, reward total was -19.000000. running mean: -20.228670\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.236383\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.244019\n",
            "resetting env. episode 443.000000, reward total was -19.000000. running mean: -20.231579\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.239263\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.246871\n",
            "resetting env. episode 446.000000, reward total was -20.000000. running mean: -20.244402\n",
            "resetting env. episode 447.000000, reward total was -19.000000. running mean: -20.231958\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.229638\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.237342\n",
            "resetting env. episode 450.000000, reward total was -20.000000. running mean: -20.234969\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.242619\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.240193\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.247791\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.255313\n",
            "resetting env. episode 455.000000, reward total was -18.000000. running mean: -20.232760\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.230432\n",
            "resetting env. episode 457.000000, reward total was -19.000000. running mean: -20.218128\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.225946\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.233687\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.231350\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.229037\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.236746\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.234379\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.232035\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.229715\n",
            "resetting env. episode 466.000000, reward total was -20.000000. running mean: -20.227418\n",
            "resetting env. episode 467.000000, reward total was -19.000000. running mean: -20.215143\n",
            "resetting env. episode 468.000000, reward total was -19.000000. running mean: -20.202992\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.210962\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.218852\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.216664\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.214497\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.222352\n",
            "resetting env. episode 474.000000, reward total was -19.000000. running mean: -20.210129\n",
            "resetting env. episode 475.000000, reward total was -19.000000. running mean: -20.198027\n",
            "resetting env. episode 476.000000, reward total was -19.000000. running mean: -20.186047\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.194187\n",
            "resetting env. episode 478.000000, reward total was -20.000000. running mean: -20.192245\n",
            "resetting env. episode 479.000000, reward total was -20.000000. running mean: -20.190322\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.198419\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.196435\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.204471\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.212426\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.220302\n",
            "resetting env. episode 485.000000, reward total was -20.000000. running mean: -20.218099\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.225918\n",
            "resetting env. episode 487.000000, reward total was -20.000000. running mean: -20.223658\n",
            "resetting env. episode 488.000000, reward total was -20.000000. running mean: -20.221422\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.229208\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.236916\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.244546\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.252101\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.249580\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.257084\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.264513\n",
            "resetting env. episode 496.000000, reward total was -19.000000. running mean: -20.251868\n",
            "resetting env. episode 497.000000, reward total was -19.000000. running mean: -20.239350\n",
            "resetting env. episode 498.000000, reward total was -21.000000. running mean: -20.246956\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.244486\n",
            "resetting env. episode 500.000000, reward total was -21.000000. running mean: -20.252042\n",
            "CPU times: user 49min 51s, sys: 12min 6s, total: 1h 1min 57s\n",
            "Wall time: 31min 29s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ"
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "bf502133-838d-4eef-d20b-3702549ffe5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.019900\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.019701\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.029504\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.039209\n",
            "resetting env. episode 10.000000, reward total was -18.000000. running mean: -20.018817\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.018629\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.028442\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.038158\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.037776\n",
            "resetting env. episode 15.000000, reward total was -18.000000. running mean: -20.017399\n",
            "resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.017225\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.027052\n",
            "resetting env. episode 18.000000, reward total was -17.000000. running mean: -19.996782\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -19.996814\n",
            "resetting env. episode 20.000000, reward total was -20.000000. running mean: -19.996846\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -19.996877\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -19.996909\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.006940\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.016870\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.026702\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.036434\n",
            "resetting env. episode 27.000000, reward total was -18.000000. running mean: -20.016070\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.025909\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.035650\n",
            "resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.045294\n",
            "resetting env. episode 31.000000, reward total was -16.000000. running mean: -20.004841\n",
            "resetting env. episode 32.000000, reward total was -18.000000. running mean: -19.984792\n",
            "resetting env. episode 33.000000, reward total was -19.000000. running mean: -19.974945\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -19.975195\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -19.985443\n",
            "resetting env. episode 36.000000, reward total was -19.000000. running mean: -19.975589\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -19.985833\n",
            "resetting env. episode 38.000000, reward total was -20.000000. running mean: -19.985975\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -19.986115\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -19.986254\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -19.986391\n",
            "resetting env. episode 42.000000, reward total was -19.000000. running mean: -19.976527\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -19.986762\n",
            "resetting env. episode 44.000000, reward total was -20.000000. running mean: -19.986894\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -19.977025\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -19.987255\n",
            "resetting env. episode 47.000000, reward total was -20.000000. running mean: -19.987383\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -19.997509\n",
            "resetting env. episode 49.000000, reward total was -19.000000. running mean: -19.987534\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -19.997658\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -19.997682\n",
            "resetting env. episode 52.000000, reward total was -18.000000. running mean: -19.977705\n",
            "resetting env. episode 53.000000, reward total was -20.000000. running mean: -19.977928\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -19.988149\n",
            "resetting env. episode 55.000000, reward total was -19.000000. running mean: -19.978267\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -19.988484\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -19.988600\n",
            "resetting env. episode 58.000000, reward total was -19.000000. running mean: -19.978714\n",
            "resetting env. episode 59.000000, reward total was -19.000000. running mean: -19.968926\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -19.979237\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -19.989445\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -19.999550\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.009555\n",
            "resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.009459\n",
            "resetting env. episode 65.000000, reward total was -19.000000. running mean: -19.999365\n",
            "resetting env. episode 66.000000, reward total was -18.000000. running mean: -19.979371\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -19.979577\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -19.989782\n",
            "resetting env. episode 69.000000, reward total was -18.000000. running mean: -19.969884\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -19.980185\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -19.980383\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -19.990579\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.000673\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.010667\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.010560\n",
            "resetting env. episode 76.000000, reward total was -19.000000. running mean: -20.000454\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.010450\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.010345\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.020242\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.030040\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.039739\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -20.039342\n",
            "resetting env. episode 83.000000, reward total was -20.000000. running mean: -20.038948\n",
            "resetting env. episode 84.000000, reward total was -18.000000. running mean: -20.018559\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.028373\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.038090\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.037709\n",
            "resetting env. episode 88.000000, reward total was -19.000000. running mean: -20.027332\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.037058\n",
            "resetting env. episode 90.000000, reward total was -19.000000. running mean: -20.026688\n",
            "resetting env. episode 91.000000, reward total was -18.000000. running mean: -20.006421\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.016357\n",
            "resetting env. episode 93.000000, reward total was -18.000000. running mean: -19.996193\n",
            "resetting env. episode 94.000000, reward total was -20.000000. running mean: -19.996231\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.006269\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.016206\n",
            "resetting env. episode 97.000000, reward total was -19.000000. running mean: -20.006044\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.015984\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -20.005824\n",
            "resetting env. episode 100.000000, reward total was -21.000000. running mean: -20.015765\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.025608\n",
            "resetting env. episode 102.000000, reward total was -20.000000. running mean: -20.025352\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.035098\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.044747\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.054300\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.053757\n",
            "resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.063219\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.062587\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.061961\n",
            "resetting env. episode 110.000000, reward total was -19.000000. running mean: -20.051342\n",
            "resetting env. episode 111.000000, reward total was -18.000000. running mean: -20.030828\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.040520\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.050115\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.059614\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.069017\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.078327\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.087544\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.086668\n",
            "resetting env. episode 119.000000, reward total was -19.000000. running mean: -20.075802\n",
            "resetting env. episode 120.000000, reward total was -19.000000. running mean: -20.065044\n",
            "resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.074393\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.083649\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.092813\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.101885\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.110866\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.119757\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.118560\n",
            "resetting env. episode 128.000000, reward total was -20.000000. running mean: -20.117374\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.126200\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.134938\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.143589\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.152153\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.160632\n",
            "resetting env. episode 134.000000, reward total was -18.000000. running mean: -20.139025\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.137635\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.146259\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.154796\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.163248\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.161616\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.159999\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.158399\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.166815\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.175147\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.183396\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.181562\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.189746\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.187849\n",
            "resetting env. episode 148.000000, reward total was -17.000000. running mean: -20.155970\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.164411\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.162767\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.161139\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.169527\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.177832\n",
            "resetting env. episode 154.000000, reward total was -19.000000. running mean: -20.166054\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.174393\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.172649\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.180923\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.179114\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.187323\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.195449\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.203495\n",
            "resetting env. episode 162.000000, reward total was -17.000000. running mean: -20.171460\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.179745\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.187948\n",
            "resetting env. episode 165.000000, reward total was -18.000000. running mean: -20.166068\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.174408\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.172664\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.170937\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.169228\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.177535\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.175760\n",
            "resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.184002\n",
            "resetting env. episode 173.000000, reward total was -19.000000. running mean: -20.172162\n",
            "resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.170441\n",
            "resetting env. episode 175.000000, reward total was -20.000000. running mean: -20.168736\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.177049\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.185278\n",
            "resetting env. episode 178.000000, reward total was -20.000000. running mean: -20.183426\n",
            "resetting env. episode 179.000000, reward total was -18.000000. running mean: -20.161591\n",
            "resetting env. episode 180.000000, reward total was -20.000000. running mean: -20.159975\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.168376\n",
            "resetting env. episode 182.000000, reward total was -18.000000. running mean: -20.146692\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.155225\n",
            "resetting env. episode 184.000000, reward total was -17.000000. running mean: -20.123673\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.122436\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.131212\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.129900\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.128601\n",
            "resetting env. episode 189.000000, reward total was -20.000000. running mean: -20.127315\n",
            "resetting env. episode 190.000000, reward total was -20.000000. running mean: -20.126041\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.134781\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.143433\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.151999\n",
            "resetting env. episode 194.000000, reward total was -21.000000. running mean: -20.160479\n",
            "resetting env. episode 195.000000, reward total was -19.000000. running mean: -20.148874\n",
            "resetting env. episode 196.000000, reward total was -20.000000. running mean: -20.147385\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.145912\n",
            "resetting env. episode 198.000000, reward total was -20.000000. running mean: -20.144452\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.153008\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.161478\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.169863\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.168164\n",
            "resetting env. episode 203.000000, reward total was -19.000000. running mean: -20.156483\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.164918\n",
            "resetting env. episode 205.000000, reward total was -20.000000. running mean: -20.163269\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.171636\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.179920\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.178120\n",
            "resetting env. episode 209.000000, reward total was -20.000000. running mean: -20.176339\n",
            "resetting env. episode 210.000000, reward total was -19.000000. running mean: -20.164576\n",
            "resetting env. episode 211.000000, reward total was -21.000000. running mean: -20.172930\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.181201\n",
            "resetting env. episode 213.000000, reward total was -19.000000. running mean: -20.169389\n",
            "resetting env. episode 214.000000, reward total was -20.000000. running mean: -20.167695\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.176018\n",
            "resetting env. episode 216.000000, reward total was -19.000000. running mean: -20.164258\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.162615\n",
            "resetting env. episode 218.000000, reward total was -20.000000. running mean: -20.160989\n",
            "resetting env. episode 219.000000, reward total was -19.000000. running mean: -20.149379\n",
            "resetting env. episode 220.000000, reward total was -20.000000. running mean: -20.147885\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.156407\n",
            "resetting env. episode 222.000000, reward total was -19.000000. running mean: -20.144842\n",
            "resetting env. episode 223.000000, reward total was -20.000000. running mean: -20.143394\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.151960\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.150441\n",
            "resetting env. episode 226.000000, reward total was -21.000000. running mean: -20.158936\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.167347\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.165673\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.174017\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.172276\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.180554\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.178748\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.186961\n",
            "resetting env. episode 234.000000, reward total was -20.000000. running mean: -20.185091\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.193240\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.191308\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.189395\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.197501\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.195526\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.203570\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.201535\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.209519\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.217424\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.225250\n",
            "resetting env. episode 245.000000, reward total was -20.000000. running mean: -20.222997\n",
            "resetting env. episode 246.000000, reward total was -19.000000. running mean: -20.210767\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.218660\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.226473\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.234208\n",
            "resetting env. episode 250.000000, reward total was -20.000000. running mean: -20.231866\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.239548\n",
            "resetting env. episode 252.000000, reward total was -20.000000. running mean: -20.237152\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.234781\n",
            "resetting env. episode 254.000000, reward total was -19.000000. running mean: -20.222433\n",
            "resetting env. episode 255.000000, reward total was -20.000000. running mean: -20.220209\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.218006\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.225826\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.223568\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.221332\n",
            "resetting env. episode 260.000000, reward total was -20.000000. running mean: -20.219119\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.226928\n",
            "resetting env. episode 262.000000, reward total was -20.000000. running mean: -20.224659\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.222412\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.220188\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.227986\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.225706\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.223449\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.221215\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.219003\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.216812\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.224644\n",
            "resetting env. episode 272.000000, reward total was -19.000000. running mean: -20.212398\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.220274\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.228071\n",
            "resetting env. episode 275.000000, reward total was -20.000000. running mean: -20.225790\n",
            "resetting env. episode 276.000000, reward total was -19.000000. running mean: -20.213533\n",
            "resetting env. episode 277.000000, reward total was -17.000000. running mean: -20.181397\n",
            "resetting env. episode 278.000000, reward total was -19.000000. running mean: -20.169583\n",
            "resetting env. episode 279.000000, reward total was -19.000000. running mean: -20.157887\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.166309\n",
            "resetting env. episode 281.000000, reward total was -20.000000. running mean: -20.164645\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.162999\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.171369\n",
            "resetting env. episode 284.000000, reward total was -20.000000. running mean: -20.169655\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.167959\n",
            "resetting env. episode 286.000000, reward total was -21.000000. running mean: -20.176279\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -20.164516\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.172871\n",
            "resetting env. episode 289.000000, reward total was -19.000000. running mean: -20.161143\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.159531\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.167936\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.166256\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.174594\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.172848\n",
            "resetting env. episode 295.000000, reward total was -19.000000. running mean: -20.161119\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.159508\n",
            "resetting env. episode 297.000000, reward total was -18.000000. running mean: -20.137913\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.136534\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.135169\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.143817\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.152379\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.160855\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.159247\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.167654\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.175978\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.174218\n",
            "resetting env. episode 307.000000, reward total was -20.000000. running mean: -20.172476\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.180751\n",
            "resetting env. episode 309.000000, reward total was -19.000000. running mean: -20.168943\n",
            "resetting env. episode 310.000000, reward total was -18.000000. running mean: -20.147254\n",
            "resetting env. episode 311.000000, reward total was -18.000000. running mean: -20.125781\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.124524\n",
            "resetting env. episode 313.000000, reward total was -17.000000. running mean: -20.093278\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.092345\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.101422\n",
            "resetting env. episode 316.000000, reward total was -18.000000. running mean: -20.080408\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.089604\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.088708\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.097821\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.106842\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.115774\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.124616\n",
            "resetting env. episode 323.000000, reward total was -20.000000. running mean: -20.123370\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.122136\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.130915\n",
            "resetting env. episode 326.000000, reward total was -21.000000. running mean: -20.139606\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.148210\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.156728\n",
            "resetting env. episode 329.000000, reward total was -19.000000. running mean: -20.145160\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.143709\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.142272\n",
            "resetting env. episode 332.000000, reward total was -20.000000. running mean: -20.140849\n",
            "resetting env. episode 333.000000, reward total was -18.000000. running mean: -20.119441\n",
            "resetting env. episode 334.000000, reward total was -19.000000. running mean: -20.108246\n",
            "resetting env. episode 335.000000, reward total was -19.000000. running mean: -20.097164\n",
            "resetting env. episode 336.000000, reward total was -19.000000. running mean: -20.086192\n",
            "resetting env. episode 337.000000, reward total was -19.000000. running mean: -20.075330\n",
            "resetting env. episode 338.000000, reward total was -19.000000. running mean: -20.064577\n",
            "resetting env. episode 339.000000, reward total was -20.000000. running mean: -20.063931\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.073292\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.082559\n",
            "resetting env. episode 342.000000, reward total was -18.000000. running mean: -20.061733\n",
            "resetting env. episode 343.000000, reward total was -19.000000. running mean: -20.051116\n",
            "resetting env. episode 344.000000, reward total was -19.000000. running mean: -20.040605\n",
            "resetting env. episode 345.000000, reward total was -18.000000. running mean: -20.020199\n",
            "resetting env. episode 346.000000, reward total was -20.000000. running mean: -20.019997\n",
            "resetting env. episode 347.000000, reward total was -20.000000. running mean: -20.019797\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.029599\n",
            "resetting env. episode 349.000000, reward total was -19.000000. running mean: -20.019303\n",
            "resetting env. episode 350.000000, reward total was -19.000000. running mean: -20.009110\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.019019\n",
            "resetting env. episode 352.000000, reward total was -19.000000. running mean: -20.008828\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.018740\n",
            "resetting env. episode 354.000000, reward total was -20.000000. running mean: -20.018553\n",
            "resetting env. episode 355.000000, reward total was -19.000000. running mean: -20.008367\n",
            "resetting env. episode 356.000000, reward total was -18.000000. running mean: -19.988284\n",
            "resetting env. episode 357.000000, reward total was -18.000000. running mean: -19.968401\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -19.978717\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y"
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}