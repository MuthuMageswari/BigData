{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "H=200_le_5 (Original).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cWACPRL869I4"
      },
      "cell_type": "code",
      "source": [
        "!pip install gym >/dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Os6feRY6ec_"
      },
      "cell_type": "code",
      "source": [
        "!pip install JSAnimation >/dev/null"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wotUOa_e6edP"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    HTML(anim.to_jshtml())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R66_INeZ9nYX"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Playing Pong"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ngMhg3fB9aA",
        "outputId": "8d0fa286-28a1-433d-9893-91f3372c63c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 27.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=b1dcd327ab72735f07f792f8d0acbd8c8096f8d623868161dea204ab82b33271\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MtT2GyK_6edc",
        "outputId": "377b43bf-1b93-4c7b-81b5-02b307aefcc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Pong-v0')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  f\"The environment {id} is out of date. You should consider \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oRE6WmXQJ1Z0",
        "outputId": "d0265fa5-311c-40f9-94c7-1e61f17fe4f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "yl_9d4HFJ31W",
        "outputId": "07db806b-7d18-477a-b7a6-19230eeb118c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "trwRXI-h6eeI",
        "outputId": "4ee8d308-37c6-4641-ea2c-580d143ff59c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Run a demo of the environment\n",
        "observation = env.reset()\n",
        "cumulated_reward = 0\n",
        "\n",
        "frames = []\n",
        "for t in range(1000):\n",
        "#     print(observation)\n",
        "    frames.append(env.render(mode = 'rgb_array'))\n",
        "    # very stupid agent, just makes a random action within the allowd action space\n",
        "    action = env.action_space.sample()\n",
        "#     print(\"Action: {}\".format(t+1))    \n",
        "    observation, reward, done, info = env.step(action)\n",
        "#     print(reward)\n",
        "    cumulated_reward += reward\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "        break\n",
        "print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -8.0\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3zZTecVWLLes"
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x): \n",
        "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
        "\n",
        "def prepro(I):\n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  I = I[35:195] # crop\n",
        "  I = I[::2,::2,0] # downsample by factor of 2\n",
        "  I[I == 144] = 0 # erase background (background type 1)\n",
        "  I[I == 109] = 0 # erase background (background type 2)\n",
        "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return I.astype(np.float).ravel()\n",
        "\n",
        "def policy_forward(x):\n",
        "  h = np.dot(model['W1'], x)\n",
        "  h[h<0] = 0 # ReLU nonlinearity\n",
        "  logp = np.dot(model['W2'], h)\n",
        "  p = sigmoid(logp)\n",
        "  return p, h # return probability of taking action 2, and hidden state\n",
        "\n",
        "def model_step(model, observation, prev_x):\n",
        "  # preprocess the observation, set input to network to be difference image\n",
        "  cur_x = prepro(observation)\n",
        "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "  prev_x = cur_x\n",
        "  \n",
        "  # forward the policy network and sample an action from the returned probability\n",
        "  aprob, _ = policy_forward(x)\n",
        "  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n",
        "  \n",
        "  return action, prev_x\n",
        "\n",
        "def play_game(env, model):\n",
        "  observation = env.reset()\n",
        "\n",
        "  frames = []\n",
        "  cumulated_reward = 0\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "\n",
        "  for t in range(1000):\n",
        "      frames.append(env.render(mode = 'rgb_array'))\n",
        "      action, prev_x = model_step(model, observation, prev_x)\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      cumulated_reward += reward\n",
        "      if done:\n",
        "          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n",
        "          break\n",
        "  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
        "  display_frames_as_gif(frames)\n",
        "  env.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gWvZQ7AQLQt"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Policy Gradient from Scratch"
      ]
    },
    {
      "metadata": {
        "id": "eqFm7hqcItWl"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# model initialization\n",
        "H = 200 # number of hidden layer neurons\n",
        "D = 80 * 80 # input dimensionality: 80x80 grid\n",
        "model = {}\n",
        "model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
        "model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
        "\n",
        "# import pickle\n",
        "# model = pickle.load(open('model.pkl', 'rb'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TwjiwKisQM19"
      },
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 10 # every how many episodes to do a param update?\n",
        "learning_rate = 1e-5\n",
        " \n",
        "gamma = 0.99 # discount factor for reward\n",
        "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
        "  \n",
        "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
        "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
        "\n",
        "def discount_rewards(r):\n",
        "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
        "  discounted_r = np.zeros_like(r, dtype=np.float32)\n",
        "  running_add = 0\n",
        "  for t in reversed(range(0, r.size)):\n",
        "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
        "    running_add = running_add * gamma + r[t]\n",
        "    discounted_r[t] = running_add\n",
        "  return discounted_r\n",
        "\n",
        "def policy_backward(epx, eph, epdlogp):\n",
        "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
        "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
        "  dh = np.outer(epdlogp, model['W2'])\n",
        "  dh[eph <= 0] = 0 # backpro prelu\n",
        "  dW1 = np.dot(dh.T, epx)\n",
        "  return {'W1':dW1, 'W2':dW2}\n",
        "\n",
        "def train_model(env, model, total_episodes = 100):\n",
        "  hist = []\n",
        "  observation = env.reset()\n",
        "\n",
        "  prev_x = None # used in computing the difference frame\n",
        "  xs,hs,dlogps,drs = [],[],[],[]\n",
        "  running_reward = None\n",
        "  reward_sum = 0\n",
        "  episode_number = 0\n",
        "\n",
        "  while True:\n",
        "    # preprocess the observation, set input to network to be difference image\n",
        "    cur_x = prepro(observation)\n",
        "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
        "    prev_x = cur_x\n",
        "\n",
        "    # forward the policy network and sample an action from the returned probability\n",
        "    aprob, h = policy_forward(x)\n",
        "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
        "\n",
        "    # record various intermediates (needed later for backprop)\n",
        "    xs.append(x) # observation\n",
        "    hs.append(h) # hidden state\n",
        "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
        "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
        "\n",
        "    # step the environment and get new measurements\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    reward_sum += reward\n",
        "\n",
        "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
        "\n",
        "    if done: # an episode finished\n",
        "      episode_number += 1\n",
        "\n",
        "      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
        "      epx = np.vstack(xs)\n",
        "      eph = np.vstack(hs)\n",
        "      epdlogp = np.vstack(dlogps)\n",
        "      epr = np.vstack(drs)\n",
        "      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
        "\n",
        "      # compute the discounted reward backwards through time\n",
        "      discounted_epr = discount_rewards(epr)\n",
        "      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
        "      discounted_epr -= np.mean(discounted_epr)\n",
        "      discounted_epr /= np.std(discounted_epr)\n",
        "\n",
        "      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
        "      grad = policy_backward(epx, eph, epdlogp)\n",
        "      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
        "\n",
        "      # perform rmsprop parameter update every batch_size episodes\n",
        "      if episode_number % batch_size == 0:\n",
        "        for k,v in model.items():\n",
        "          g = grad_buffer[k] # gradient\n",
        "          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
        "          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
        "          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
        "\n",
        "      # boring book-keeping\n",
        "      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
        "      hist.append((episode_number, reward_sum, running_reward))\n",
        "      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
        "      reward_sum = 0\n",
        "      observation = env.reset() # reset env\n",
        "      prev_x = None\n",
        "      if episode_number == total_episodes: return hist\n",
        "\n",
        "      if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
        "        print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6Ka_5Vl9Orm",
        "outputId": "84161d61-f3ff-4e4c-e346-a30194d8d87d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist1 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 4.000000, reward total was -18.000000. running mean: -19.989900\n",
            "resetting env. episode 5.000000, reward total was -19.000000. running mean: -19.980001\n",
            "resetting env. episode 6.000000, reward total was -18.000000. running mean: -19.960201\n",
            "resetting env. episode 7.000000, reward total was -20.000000. running mean: -19.960599\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -19.970993\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -19.981283\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -19.991470\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -19.991556\n",
            "resetting env. episode 12.000000, reward total was -20.000000. running mean: -19.991640\n",
            "resetting env. episode 13.000000, reward total was -20.000000. running mean: -19.991724\n",
            "resetting env. episode 14.000000, reward total was -21.000000. running mean: -20.001806\n",
            "resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.011788\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.021670\n",
            "resetting env. episode 17.000000, reward total was -20.000000. running mean: -20.021454\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.021239\n",
            "resetting env. episode 19.000000, reward total was -19.000000. running mean: -20.011027\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.020916\n",
            "resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.030707\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.040400\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.039996\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.049596\n",
            "resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.049100\n",
            "resetting env. episode 26.000000, reward total was -20.000000. running mean: -20.048609\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.058123\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.067542\n",
            "resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.076867\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.076098\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.085337\n",
            "resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.094484\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.103539\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.112503\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.121378\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.130165\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.138863\n",
            "resetting env. episode 38.000000, reward total was -19.000000. running mean: -20.127474\n",
            "resetting env. episode 39.000000, reward total was -19.000000. running mean: -20.116200\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.125038\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.133787\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.142449\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.141025\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.149615\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.158118\n",
            "resetting env. episode 46.000000, reward total was -19.000000. running mean: -20.146537\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.155072\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.163521\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.171886\n",
            "resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.180167\n",
            "resetting env. episode 51.000000, reward total was -20.000000. running mean: -20.178365\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.186582\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.194716\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.202769\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.210741\n",
            "resetting env. episode 56.000000, reward total was -19.000000. running mean: -20.198634\n",
            "resetting env. episode 57.000000, reward total was -18.000000. running mean: -20.176647\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.174881\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.183132\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.191301\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.199388\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.207394\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.215320\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.223167\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.230935\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.238626\n",
            "resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.246239\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.253777\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.261239\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.268627\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.265941\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.273281\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.280548\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.287743\n",
            "resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.294865\n",
            "resetting env. episode 76.000000, reward total was -19.000000. running mean: -20.281917\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.289098\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.296207\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.303245\n",
            "resetting env. episode 80.000000, reward total was -19.000000. running mean: -20.290212\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.297310\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -20.294337\n",
            "resetting env. episode 83.000000, reward total was -21.000000. running mean: -20.301394\n",
            "resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.308380\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.315296\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.322143\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.328921\n",
            "resetting env. episode 88.000000, reward total was -20.000000. running mean: -20.325632\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.332376\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.329052\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.335762\n",
            "resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.342404\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.348980\n",
            "resetting env. episode 94.000000, reward total was -19.000000. running mean: -20.335490\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.342135\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.348714\n",
            "resetting env. episode 97.000000, reward total was -20.000000. running mean: -20.345227\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.351774\n",
            "resetting env. episode 99.000000, reward total was -19.000000. running mean: -20.338257\n",
            "resetting env. episode 100.000000, reward total was -19.000000. running mean: -20.324874\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.321625\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.328409\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.325125\n",
            "resetting env. episode 104.000000, reward total was -19.000000. running mean: -20.311874\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.318755\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.315568\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.312412\n",
            "resetting env. episode 108.000000, reward total was -18.000000. running mean: -20.289288\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.286395\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.293531\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -20.290596\n",
            "resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.297690\n",
            "resetting env. episode 113.000000, reward total was -21.000000. running mean: -20.304713\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.311666\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.318549\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.325363\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.332110\n",
            "resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.338789\n",
            "resetting env. episode 119.000000, reward total was -20.000000. running mean: -20.335401\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.342047\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.338626\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.345240\n",
            "resetting env. episode 123.000000, reward total was -17.000000. running mean: -20.311788\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.318670\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.325483\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.332228\n",
            "resetting env. episode 127.000000, reward total was -21.000000. running mean: -20.338906\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.345517\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.352062\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.358541\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.364956\n",
            "resetting env. episode 132.000000, reward total was -19.000000. running mean: -20.351306\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.357793\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.364215\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.360573\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.366967\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.373298\n",
            "resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.369565\n",
            "resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.365869\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.372210\n",
            "resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.378488\n",
            "resetting env. episode 142.000000, reward total was -20.000000. running mean: -20.374703\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.370956\n",
            "resetting env. episode 144.000000, reward total was -19.000000. running mean: -20.357247\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.363674\n",
            "resetting env. episode 146.000000, reward total was -20.000000. running mean: -20.360038\n",
            "resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.356437\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.362873\n",
            "resetting env. episode 149.000000, reward total was -20.000000. running mean: -20.359244\n",
            "resetting env. episode 150.000000, reward total was -20.000000. running mean: -20.355652\n",
            "resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.352095\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.358574\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.364988\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.371339\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.367625\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.373949\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.380209\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.386407\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.392543\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.398618\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.404632\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.410585\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.416479\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.422315\n",
            "resetting env. episode 165.000000, reward total was -19.000000. running mean: -20.408092\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.414011\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.419871\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.425672\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.431415\n",
            "resetting env. episode 170.000000, reward total was -20.000000. running mean: -20.427101\n",
            "resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.422830\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.418602\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.424416\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.430171\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.435870\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.431511\n",
            "resetting env. episode 177.000000, reward total was -17.000000. running mean: -20.397196\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.403224\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.409192\n",
            "resetting env. episode 180.000000, reward total was -21.000000. running mean: -20.415100\n",
            "resetting env. episode 181.000000, reward total was -21.000000. running mean: -20.420949\n",
            "resetting env. episode 182.000000, reward total was -21.000000. running mean: -20.426739\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.432472\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.438147\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.443766\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.449328\n",
            "resetting env. episode 187.000000, reward total was -21.000000. running mean: -20.454835\n",
            "resetting env. episode 188.000000, reward total was -20.000000. running mean: -20.450286\n",
            "resetting env. episode 189.000000, reward total was -19.000000. running mean: -20.435784\n",
            "resetting env. episode 190.000000, reward total was -21.000000. running mean: -20.441426\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.437012\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.442641\n",
            "resetting env. episode 193.000000, reward total was -19.000000. running mean: -20.428215\n",
            "resetting env. episode 194.000000, reward total was -19.000000. running mean: -20.413933\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.409793\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.415696\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.411539\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.417423\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.413249\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.419117\n",
            "resetting env. episode 201.000000, reward total was -20.000000. running mean: -20.414925\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.420776\n",
            "resetting env. episode 203.000000, reward total was -20.000000. running mean: -20.416568\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.422403\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.428179\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.423897\n",
            "resetting env. episode 207.000000, reward total was -20.000000. running mean: -20.419658\n",
            "resetting env. episode 208.000000, reward total was -21.000000. running mean: -20.425461\n",
            "resetting env. episode 209.000000, reward total was -19.000000. running mean: -20.411207\n",
            "resetting env. episode 210.000000, reward total was -21.000000. running mean: -20.417095\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.412924\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.418794\n",
            "resetting env. episode 213.000000, reward total was -18.000000. running mean: -20.394606\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.400660\n",
            "resetting env. episode 215.000000, reward total was -18.000000. running mean: -20.376654\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.372887\n",
            "resetting env. episode 217.000000, reward total was -20.000000. running mean: -20.369158\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.375467\n",
            "resetting env. episode 219.000000, reward total was -21.000000. running mean: -20.381712\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.387895\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.384016\n",
            "resetting env. episode 222.000000, reward total was -20.000000. running mean: -20.380176\n",
            "resetting env. episode 223.000000, reward total was -17.000000. running mean: -20.346374\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.342910\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.339481\n",
            "resetting env. episode 226.000000, reward total was -18.000000. running mean: -20.316086\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.322926\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.329696\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.336399\n",
            "resetting env. episode 230.000000, reward total was -19.000000. running mean: -20.323035\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.329805\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.336507\n",
            "resetting env. episode 233.000000, reward total was -19.000000. running mean: -20.323142\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.329911\n",
            "resetting env. episode 235.000000, reward total was -20.000000. running mean: -20.326611\n",
            "resetting env. episode 236.000000, reward total was -21.000000. running mean: -20.333345\n",
            "resetting env. episode 237.000000, reward total was -19.000000. running mean: -20.320012\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.326812\n",
            "resetting env. episode 239.000000, reward total was -21.000000. running mean: -20.333544\n",
            "resetting env. episode 240.000000, reward total was -21.000000. running mean: -20.340208\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.346806\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.353338\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.349805\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.356307\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.362744\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.369116\n",
            "resetting env. episode 247.000000, reward total was -19.000000. running mean: -20.355425\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.361871\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.368252\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.374569\n",
            "resetting env. episode 251.000000, reward total was -20.000000. running mean: -20.370824\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.377116\n",
            "resetting env. episode 253.000000, reward total was -21.000000. running mean: -20.383344\n",
            "resetting env. episode 254.000000, reward total was -18.000000. running mean: -20.359511\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.365916\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.362257\n",
            "resetting env. episode 257.000000, reward total was -21.000000. running mean: -20.368634\n",
            "resetting env. episode 258.000000, reward total was -19.000000. running mean: -20.354948\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.351398\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.357884\n",
            "resetting env. episode 261.000000, reward total was -20.000000. running mean: -20.354305\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.360762\n",
            "resetting env. episode 263.000000, reward total was -20.000000. running mean: -20.357155\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.353583\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.360047\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.356447\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.362882\n",
            "resetting env. episode 268.000000, reward total was -20.000000. running mean: -20.359254\n",
            "resetting env. episode 269.000000, reward total was -20.000000. running mean: -20.355661\n",
            "resetting env. episode 270.000000, reward total was -20.000000. running mean: -20.352104\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.358583\n",
            "resetting env. episode 272.000000, reward total was -21.000000. running mean: -20.364998\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.371348\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.377634\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.383858\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.380019\n",
            "resetting env. episode 277.000000, reward total was -21.000000. running mean: -20.386219\n",
            "resetting env. episode 278.000000, reward total was -20.000000. running mean: -20.382357\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.388533\n",
            "resetting env. episode 280.000000, reward total was -21.000000. running mean: -20.394648\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.400701\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.396694\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.392728\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.398800\n",
            "resetting env. episode 285.000000, reward total was -20.000000. running mean: -20.394812\n",
            "resetting env. episode 286.000000, reward total was -19.000000. running mean: -20.380864\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.377055\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.373285\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.379552\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.375757\n",
            "resetting env. episode 291.000000, reward total was -20.000000. running mean: -20.371999\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.378279\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.384496\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.390651\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.396745\n",
            "resetting env. episode 296.000000, reward total was -21.000000. running mean: -20.402777\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.408750\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.404662\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.410615\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.416509\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.412344\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.418221\n",
            "resetting env. episode 303.000000, reward total was -21.000000. running mean: -20.424038\n",
            "resetting env. episode 304.000000, reward total was -19.000000. running mean: -20.409798\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.415700\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.421543\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.427328\n",
            "resetting env. episode 308.000000, reward total was -16.000000. running mean: -20.383054\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.389224\n",
            "resetting env. episode 310.000000, reward total was -21.000000. running mean: -20.395332\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.401378\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.397365\n",
            "resetting env. episode 313.000000, reward total was -20.000000. running mean: -20.393391\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.399457\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.395462\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.401508\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.407493\n",
            "resetting env. episode 318.000000, reward total was -21.000000. running mean: -20.413418\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.409284\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.415191\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.411039\n",
            "resetting env. episode 322.000000, reward total was -21.000000. running mean: -20.416928\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.422759\n",
            "resetting env. episode 324.000000, reward total was -21.000000. running mean: -20.428532\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.434246\n",
            "resetting env. episode 326.000000, reward total was -20.000000. running mean: -20.429904\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.435605\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.441249\n",
            "resetting env. episode 329.000000, reward total was -20.000000. running mean: -20.436836\n",
            "resetting env. episode 330.000000, reward total was -21.000000. running mean: -20.442468\n",
            "resetting env. episode 331.000000, reward total was -20.000000. running mean: -20.438043\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.443663\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.449226\n",
            "resetting env. episode 334.000000, reward total was -18.000000. running mean: -20.424734\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.430487\n",
            "resetting env. episode 336.000000, reward total was -20.000000. running mean: -20.426182\n",
            "resetting env. episode 337.000000, reward total was -21.000000. running mean: -20.431920\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.437601\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.443225\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.448792\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.444304\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.449861\n",
            "resetting env. episode 343.000000, reward total was -19.000000. running mean: -20.435363\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.441009\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.436599\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.442233\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.447811\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.453333\n",
            "resetting env. episode 349.000000, reward total was -20.000000. running mean: -20.448799\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.454311\n",
            "resetting env. episode 351.000000, reward total was -21.000000. running mean: -20.459768\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.455171\n",
            "resetting env. episode 353.000000, reward total was -20.000000. running mean: -20.450619\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.456113\n",
            "resetting env. episode 355.000000, reward total was -21.000000. running mean: -20.461552\n",
            "resetting env. episode 356.000000, reward total was -21.000000. running mean: -20.466936\n",
            "resetting env. episode 357.000000, reward total was -19.000000. running mean: -20.452267\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.447744\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.453267\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.458734\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.454147\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.449605\n",
            "resetting env. episode 363.000000, reward total was -18.000000. running mean: -20.425109\n",
            "resetting env. episode 364.000000, reward total was -18.000000. running mean: -20.400858\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.406849\n",
            "resetting env. episode 366.000000, reward total was -19.000000. running mean: -20.392781\n",
            "resetting env. episode 367.000000, reward total was -20.000000. running mean: -20.388853\n",
            "resetting env. episode 368.000000, reward total was -20.000000. running mean: -20.384965\n",
            "resetting env. episode 369.000000, reward total was -20.000000. running mean: -20.381115\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.387304\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.393431\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.399496\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.405501\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.411446\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.417332\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.423159\n",
            "resetting env. episode 377.000000, reward total was -20.000000. running mean: -20.418927\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.424738\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.430490\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.436185\n",
            "resetting env. episode 381.000000, reward total was -21.000000. running mean: -20.441824\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.447405\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.452931\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.458402\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.463818\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.459180\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.464588\n",
            "resetting env. episode 388.000000, reward total was -20.000000. running mean: -20.459942\n",
            "resetting env. episode 389.000000, reward total was -21.000000. running mean: -20.465343\n",
            "resetting env. episode 390.000000, reward total was -20.000000. running mean: -20.460689\n",
            "resetting env. episode 391.000000, reward total was -21.000000. running mean: -20.466082\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.471422\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.476707\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.481940\n",
            "resetting env. episode 395.000000, reward total was -21.000000. running mean: -20.487121\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.492250\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.497327\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.502354\n",
            "resetting env. episode 399.000000, reward total was -20.000000. running mean: -20.497330\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.492357\n",
            "resetting env. episode 401.000000, reward total was -20.000000. running mean: -20.487434\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.482559\n",
            "resetting env. episode 403.000000, reward total was -19.000000. running mean: -20.467734\n",
            "resetting env. episode 404.000000, reward total was -17.000000. running mean: -20.433056\n",
            "resetting env. episode 405.000000, reward total was -20.000000. running mean: -20.428726\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.434438\n",
            "resetting env. episode 407.000000, reward total was -21.000000. running mean: -20.440094\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.445693\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.451236\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.446724\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.442257\n",
            "resetting env. episode 412.000000, reward total was -21.000000. running mean: -20.447834\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.453356\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.458822\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.454234\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.459692\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.465095\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.470444\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.465739\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.471082\n",
            "resetting env. episode 421.000000, reward total was -19.000000. running mean: -20.456371\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.461807\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.467189\n",
            "resetting env. episode 424.000000, reward total was -21.000000. running mean: -20.472517\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.477792\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.483014\n",
            "resetting env. episode 427.000000, reward total was -20.000000. running mean: -20.478184\n",
            "resetting env. episode 428.000000, reward total was -18.000000. running mean: -20.453402\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.458868\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.464280\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.469637\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.474940\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.480191\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.485389\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.480535\n",
            "resetting env. episode 436.000000, reward total was -21.000000. running mean: -20.485730\n",
            "resetting env. episode 437.000000, reward total was -19.000000. running mean: -20.470873\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.476164\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.481402\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.486588\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.481722\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.486905\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.482036\n",
            "resetting env. episode 444.000000, reward total was -20.000000. running mean: -20.477216\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.482443\n",
            "resetting env. episode 446.000000, reward total was -19.000000. running mean: -20.467619\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.472943\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.478213\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.473431\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.478697\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.483910\n",
            "resetting env. episode 452.000000, reward total was -19.000000. running mean: -20.469071\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.474380\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.479636\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.484840\n",
            "resetting env. episode 456.000000, reward total was -19.000000. running mean: -20.469992\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.475292\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.480539\n",
            "resetting env. episode 459.000000, reward total was -20.000000. running mean: -20.475733\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.470976\n",
            "resetting env. episode 461.000000, reward total was -20.000000. running mean: -20.466266\n",
            "resetting env. episode 462.000000, reward total was -20.000000. running mean: -20.461604\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.466988\n",
            "resetting env. episode 464.000000, reward total was -20.000000. running mean: -20.462318\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.467695\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.473018\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.478287\n",
            "resetting env. episode 468.000000, reward total was -21.000000. running mean: -20.483505\n",
            "resetting env. episode 469.000000, reward total was -20.000000. running mean: -20.478670\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.483883\n",
            "resetting env. episode 471.000000, reward total was -20.000000. running mean: -20.479044\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.474254\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.479511\n",
            "resetting env. episode 474.000000, reward total was -17.000000. running mean: -20.444716\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.440269\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.445866\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.451407\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.456893\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.462324\n",
            "resetting env. episode 480.000000, reward total was -19.000000. running mean: -20.447701\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.453224\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.458692\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.464105\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.469464\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.474769\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.480022\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.485221\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.490369\n",
            "resetting env. episode 489.000000, reward total was -20.000000. running mean: -20.485465\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.490611\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.495705\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.500748\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.495740\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.500783\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.495775\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.500817\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.505809\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -20.490751\n",
            "resetting env. episode 499.000000, reward total was -20.000000. running mean: -20.485843\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -20.470985\n",
            "CPU times: user 22min 30s, sys: 10min 9s, total: 32min 40s\n",
            "Wall time: 16min 54s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cHYCDYwhlVLV",
        "outputId": "35758fe9-ddc8-45c7-86a4-f3cd36a1293b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist2 = train_model(env, model, total_episodes=500)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n",
            "resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.009900\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.019801\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.029603\n",
            "resetting env. episode 6.000000, reward total was -20.000000. running mean: -20.029307\n",
            "resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.039014\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.048624\n",
            "resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.048138\n",
            "resetting env. episode 10.000000, reward total was -20.000000. running mean: -20.047656\n",
            "resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.057180\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.066608\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.075942\n",
            "resetting env. episode 14.000000, reward total was -19.000000. running mean: -20.065182\n",
            "resetting env. episode 15.000000, reward total was -19.000000. running mean: -20.054530\n",
            "resetting env. episode 16.000000, reward total was -21.000000. running mean: -20.063985\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.073345\n",
            "resetting env. episode 18.000000, reward total was -20.000000. running mean: -20.072612\n",
            "resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.081886\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.091067\n",
            "resetting env. episode 21.000000, reward total was -20.000000. running mean: -20.090156\n",
            "resetting env. episode 22.000000, reward total was -20.000000. running mean: -20.089255\n",
            "resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.098362\n",
            "resetting env. episode 24.000000, reward total was -19.000000. running mean: -20.087378\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.096505\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.105540\n",
            "resetting env. episode 27.000000, reward total was -21.000000. running mean: -20.114484\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.123339\n",
            "resetting env. episode 29.000000, reward total was -19.000000. running mean: -20.112106\n",
            "resetting env. episode 30.000000, reward total was -20.000000. running mean: -20.110985\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.119875\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.118676\n",
            "resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.127490\n",
            "resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.126215\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.134953\n",
            "resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.133603\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.142267\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.150844\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.149336\n",
            "resetting env. episode 40.000000, reward total was -20.000000. running mean: -20.147843\n",
            "resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.156364\n",
            "resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.164800\n",
            "resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.173152\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.181421\n",
            "resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.189607\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.197711\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.205734\n",
            "resetting env. episode 48.000000, reward total was -20.000000. running mean: -20.203676\n",
            "resetting env. episode 49.000000, reward total was -20.000000. running mean: -20.201639\n",
            "resetting env. episode 50.000000, reward total was -19.000000. running mean: -20.189623\n",
            "resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.197727\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.205750\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.213692\n",
            "resetting env. episode 54.000000, reward total was -21.000000. running mean: -20.221555\n",
            "resetting env. episode 55.000000, reward total was -19.000000. running mean: -20.209340\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.217246\n",
            "resetting env. episode 57.000000, reward total was -20.000000. running mean: -20.215074\n",
            "resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.222923\n",
            "resetting env. episode 59.000000, reward total was -21.000000. running mean: -20.230694\n",
            "resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.238387\n",
            "resetting env. episode 61.000000, reward total was -21.000000. running mean: -20.246003\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.253543\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.261007\n",
            "resetting env. episode 64.000000, reward total was -21.000000. running mean: -20.268397\n",
            "resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.275713\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.282956\n",
            "resetting env. episode 67.000000, reward total was -20.000000. running mean: -20.280127\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.287325\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.294452\n",
            "resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.301508\n",
            "resetting env. episode 71.000000, reward total was -20.000000. running mean: -20.298493\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.305508\n",
            "resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.302453\n",
            "resetting env. episode 74.000000, reward total was -20.000000. running mean: -20.299428\n",
            "resetting env. episode 75.000000, reward total was -19.000000. running mean: -20.286434\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.293569\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.300634\n",
            "resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.297627\n",
            "resetting env. episode 79.000000, reward total was -20.000000. running mean: -20.294651\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.301705\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.308688\n",
            "resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.315601\n",
            "resetting env. episode 83.000000, reward total was -19.000000. running mean: -20.302445\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.299420\n",
            "resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.296426\n",
            "resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.303462\n",
            "resetting env. episode 87.000000, reward total was -21.000000. running mean: -20.310427\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.317323\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.324150\n",
            "resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.320908\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.327699\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.324422\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.331178\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.337866\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.344487\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.351043\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.357532\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.363957\n",
            "resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.370317\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.366614\n",
            "resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.372948\n",
            "resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.379218\n",
            "resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.375426\n",
            "resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.381672\n",
            "resetting env. episode 105.000000, reward total was -19.000000. running mean: -20.367855\n",
            "resetting env. episode 106.000000, reward total was -20.000000. running mean: -20.364177\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.360535\n",
            "resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.356930\n",
            "resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.363360\n",
            "resetting env. episode 110.000000, reward total was -20.000000. running mean: -20.359727\n",
            "resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.366129\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.362468\n",
            "resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.358844\n",
            "resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.365255\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.371603\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.377886\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.384108\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.380267\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.386464\n",
            "resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.392599\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.388673\n",
            "resetting env. episode 122.000000, reward total was -21.000000. running mean: -20.394787\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.400839\n",
            "resetting env. episode 124.000000, reward total was -21.000000. running mean: -20.406830\n",
            "resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.412762\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.418634\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.414448\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.420304\n",
            "resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.426100\n",
            "resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.431839\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.437521\n",
            "resetting env. episode 132.000000, reward total was -21.000000. running mean: -20.443146\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.448714\n",
            "resetting env. episode 134.000000, reward total was -19.000000. running mean: -20.434227\n",
            "resetting env. episode 135.000000, reward total was -19.000000. running mean: -20.419885\n",
            "resetting env. episode 136.000000, reward total was -21.000000. running mean: -20.425686\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.431429\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.437115\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.442744\n",
            "resetting env. episode 140.000000, reward total was -21.000000. running mean: -20.448316\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.443833\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.449395\n",
            "resetting env. episode 143.000000, reward total was -20.000000. running mean: -20.444901\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.450452\n",
            "resetting env. episode 145.000000, reward total was -21.000000. running mean: -20.455947\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.461388\n",
            "resetting env. episode 147.000000, reward total was -18.000000. running mean: -20.436774\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.442406\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.447982\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.453502\n",
            "resetting env. episode 151.000000, reward total was -19.000000. running mean: -20.438967\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.444578\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.450132\n",
            "resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.455631\n",
            "resetting env. episode 155.000000, reward total was -21.000000. running mean: -20.461074\n",
            "resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.456464\n",
            "resetting env. episode 157.000000, reward total was -19.000000. running mean: -20.441899\n",
            "resetting env. episode 158.000000, reward total was -21.000000. running mean: -20.447480\n",
            "resetting env. episode 159.000000, reward total was -21.000000. running mean: -20.453005\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.458475\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.463890\n",
            "resetting env. episode 162.000000, reward total was -21.000000. running mean: -20.469251\n",
            "resetting env. episode 163.000000, reward total was -20.000000. running mean: -20.464559\n",
            "resetting env. episode 164.000000, reward total was -20.000000. running mean: -20.459913\n",
            "resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.465314\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.470661\n",
            "resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.465954\n",
            "resetting env. episode 168.000000, reward total was -20.000000. running mean: -20.461295\n",
            "resetting env. episode 169.000000, reward total was -21.000000. running mean: -20.466682\n",
            "resetting env. episode 170.000000, reward total was -19.000000. running mean: -20.452015\n",
            "resetting env. episode 171.000000, reward total was -21.000000. running mean: -20.457495\n",
            "resetting env. episode 172.000000, reward total was -20.000000. running mean: -20.452920\n",
            "resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.448391\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.453907\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.459368\n",
            "resetting env. episode 176.000000, reward total was -21.000000. running mean: -20.464774\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.470126\n",
            "resetting env. episode 178.000000, reward total was -18.000000. running mean: -20.445425\n",
            "resetting env. episode 179.000000, reward total was -20.000000. running mean: -20.440971\n",
            "resetting env. episode 180.000000, reward total was -18.000000. running mean: -20.416561\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.412396\n",
            "resetting env. episode 182.000000, reward total was -19.000000. running mean: -20.398272\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.404289\n",
            "resetting env. episode 184.000000, reward total was -21.000000. running mean: -20.410246\n",
            "resetting env. episode 185.000000, reward total was -20.000000. running mean: -20.406144\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.412082\n",
            "resetting env. episode 187.000000, reward total was -20.000000. running mean: -20.407961\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.413882\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.419743\n",
            "resetting env. episode 190.000000, reward total was -19.000000. running mean: -20.405545\n",
            "resetting env. episode 191.000000, reward total was -20.000000. running mean: -20.401490\n",
            "resetting env. episode 192.000000, reward total was -20.000000. running mean: -20.397475\n",
            "resetting env. episode 193.000000, reward total was -21.000000. running mean: -20.403500\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.399465\n",
            "resetting env. episode 195.000000, reward total was -20.000000. running mean: -20.395471\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.401516\n",
            "resetting env. episode 197.000000, reward total was -20.000000. running mean: -20.397501\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.403526\n",
            "resetting env. episode 199.000000, reward total was -21.000000. running mean: -20.409491\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.415396\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.421242\n",
            "resetting env. episode 202.000000, reward total was -21.000000. running mean: -20.427029\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.432759\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.438431\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.444047\n",
            "resetting env. episode 206.000000, reward total was -20.000000. running mean: -20.439607\n",
            "resetting env. episode 207.000000, reward total was -21.000000. running mean: -20.445211\n",
            "resetting env. episode 208.000000, reward total was -19.000000. running mean: -20.430758\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.436451\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.432086\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.427766\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.433488\n",
            "resetting env. episode 213.000000, reward total was -21.000000. running mean: -20.439153\n",
            "resetting env. episode 214.000000, reward total was -21.000000. running mean: -20.444761\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.450314\n",
            "resetting env. episode 216.000000, reward total was -21.000000. running mean: -20.455811\n",
            "resetting env. episode 217.000000, reward total was -19.000000. running mean: -20.441253\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.446840\n",
            "resetting env. episode 219.000000, reward total was -19.000000. running mean: -20.432372\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.438048\n",
            "resetting env. episode 221.000000, reward total was -20.000000. running mean: -20.433667\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.439331\n",
            "resetting env. episode 223.000000, reward total was -19.000000. running mean: -20.424937\n",
            "resetting env. episode 224.000000, reward total was -21.000000. running mean: -20.430688\n",
            "resetting env. episode 225.000000, reward total was -20.000000. running mean: -20.426381\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.422117\n",
            "resetting env. episode 227.000000, reward total was -19.000000. running mean: -20.407896\n",
            "resetting env. episode 228.000000, reward total was -20.000000. running mean: -20.403817\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.409779\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.405681\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.411625\n",
            "resetting env. episode 232.000000, reward total was -20.000000. running mean: -20.407508\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.413433\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.419299\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.425106\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.420855\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.416646\n",
            "resetting env. episode 238.000000, reward total was -21.000000. running mean: -20.422480\n",
            "resetting env. episode 239.000000, reward total was -19.000000. running mean: -20.408255\n",
            "resetting env. episode 240.000000, reward total was -20.000000. running mean: -20.404172\n",
            "resetting env. episode 241.000000, reward total was -20.000000. running mean: -20.400131\n",
            "resetting env. episode 242.000000, reward total was -20.000000. running mean: -20.396129\n",
            "resetting env. episode 243.000000, reward total was -20.000000. running mean: -20.392168\n",
            "resetting env. episode 244.000000, reward total was -21.000000. running mean: -20.398246\n",
            "resetting env. episode 245.000000, reward total was -21.000000. running mean: -20.404264\n",
            "resetting env. episode 246.000000, reward total was -21.000000. running mean: -20.410221\n",
            "resetting env. episode 247.000000, reward total was -20.000000. running mean: -20.406119\n",
            "resetting env. episode 248.000000, reward total was -21.000000. running mean: -20.412058\n",
            "resetting env. episode 249.000000, reward total was -19.000000. running mean: -20.397937\n",
            "resetting env. episode 250.000000, reward total was -19.000000. running mean: -20.383958\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.390118\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.396217\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.392255\n",
            "resetting env. episode 254.000000, reward total was -20.000000. running mean: -20.388332\n",
            "resetting env. episode 255.000000, reward total was -21.000000. running mean: -20.394449\n",
            "resetting env. episode 256.000000, reward total was -21.000000. running mean: -20.400505\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.396500\n",
            "resetting env. episode 258.000000, reward total was -20.000000. running mean: -20.392535\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.388609\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.394723\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.400776\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.406768\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.412701\n",
            "resetting env. episode 264.000000, reward total was -19.000000. running mean: -20.398574\n",
            "resetting env. episode 265.000000, reward total was -18.000000. running mean: -20.374588\n",
            "resetting env. episode 266.000000, reward total was -20.000000. running mean: -20.370842\n",
            "resetting env. episode 267.000000, reward total was -21.000000. running mean: -20.377133\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.383362\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.389529\n",
            "resetting env. episode 270.000000, reward total was -21.000000. running mean: -20.395633\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.401677\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.397660\n",
            "resetting env. episode 273.000000, reward total was -19.000000. running mean: -20.383684\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.389847\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.395948\n",
            "resetting env. episode 276.000000, reward total was -19.000000. running mean: -20.381989\n",
            "resetting env. episode 277.000000, reward total was -20.000000. running mean: -20.378169\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.384387\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.390543\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.386638\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.392771\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.388844\n",
            "resetting env. episode 283.000000, reward total was -20.000000. running mean: -20.384955\n",
            "resetting env. episode 284.000000, reward total was -18.000000. running mean: -20.361106\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.367495\n",
            "resetting env. episode 286.000000, reward total was -20.000000. running mean: -20.363820\n",
            "resetting env. episode 287.000000, reward total was -19.000000. running mean: -20.350182\n",
            "resetting env. episode 288.000000, reward total was -21.000000. running mean: -20.356680\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.363113\n",
            "resetting env. episode 290.000000, reward total was -21.000000. running mean: -20.369482\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.375787\n",
            "resetting env. episode 292.000000, reward total was -21.000000. running mean: -20.382029\n",
            "resetting env. episode 293.000000, reward total was -20.000000. running mean: -20.378209\n",
            "resetting env. episode 294.000000, reward total was -21.000000. running mean: -20.384427\n",
            "resetting env. episode 295.000000, reward total was -21.000000. running mean: -20.390582\n",
            "resetting env. episode 296.000000, reward total was -20.000000. running mean: -20.386677\n",
            "resetting env. episode 297.000000, reward total was -21.000000. running mean: -20.392810\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.388882\n",
            "resetting env. episode 299.000000, reward total was -20.000000. running mean: -20.384993\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.391143\n",
            "resetting env. episode 301.000000, reward total was -20.000000. running mean: -20.387232\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.393359\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.389426\n",
            "resetting env. episode 304.000000, reward total was -21.000000. running mean: -20.395531\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.401576\n",
            "resetting env. episode 306.000000, reward total was -21.000000. running mean: -20.407560\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.413485\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.419350\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.425156\n",
            "resetting env. episode 310.000000, reward total was -19.000000. running mean: -20.410905\n",
            "resetting env. episode 311.000000, reward total was -21.000000. running mean: -20.416796\n",
            "resetting env. episode 312.000000, reward total was -21.000000. running mean: -20.422628\n",
            "resetting env. episode 313.000000, reward total was -21.000000. running mean: -20.428402\n",
            "resetting env. episode 314.000000, reward total was -21.000000. running mean: -20.434118\n",
            "resetting env. episode 315.000000, reward total was -20.000000. running mean: -20.429776\n",
            "resetting env. episode 316.000000, reward total was -21.000000. running mean: -20.435479\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.441124\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.436713\n",
            "resetting env. episode 319.000000, reward total was -21.000000. running mean: -20.442345\n",
            "resetting env. episode 320.000000, reward total was -21.000000. running mean: -20.447922\n",
            "resetting env. episode 321.000000, reward total was -21.000000. running mean: -20.453443\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.448908\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.454419\n",
            "resetting env. episode 324.000000, reward total was -19.000000. running mean: -20.439875\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.445476\n",
            "resetting env. episode 326.000000, reward total was -18.000000. running mean: -20.421022\n",
            "resetting env. episode 327.000000, reward total was -20.000000. running mean: -20.416811\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.422643\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.428417\n",
            "resetting env. episode 330.000000, reward total was -17.000000. running mean: -20.394133\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.400191\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.406189\n",
            "resetting env. episode 333.000000, reward total was -21.000000. running mean: -20.412128\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.418006\n",
            "resetting env. episode 335.000000, reward total was -21.000000. running mean: -20.423826\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.429588\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.425292\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.431039\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.436729\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.442361\n",
            "resetting env. episode 341.000000, reward total was -20.000000. running mean: -20.437938\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.443558\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.439123\n",
            "resetting env. episode 344.000000, reward total was -20.000000. running mean: -20.434732\n",
            "resetting env. episode 345.000000, reward total was -20.000000. running mean: -20.430384\n",
            "resetting env. episode 346.000000, reward total was -21.000000. running mean: -20.436080\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.441720\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.447302\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.452829\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.458301\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.453718\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.449181\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.454689\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.460142\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.455541\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -20.440985\n",
            "resetting env. episode 357.000000, reward total was -20.000000. running mean: -20.436576\n",
            "resetting env. episode 358.000000, reward total was -20.000000. running mean: -20.432210\n",
            "resetting env. episode 359.000000, reward total was -21.000000. running mean: -20.437888\n",
            "resetting env. episode 360.000000, reward total was -19.000000. running mean: -20.423509\n",
            "resetting env. episode 361.000000, reward total was -19.000000. running mean: -20.409274\n",
            "resetting env. episode 362.000000, reward total was -18.000000. running mean: -20.385181\n",
            "resetting env. episode 363.000000, reward total was -20.000000. running mean: -20.381329\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.377516\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.383741\n",
            "resetting env. episode 366.000000, reward total was -21.000000. running mean: -20.389903\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.396004\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.402044\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.408024\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.413944\n",
            "resetting env. episode 371.000000, reward total was -19.000000. running mean: -20.399804\n",
            "resetting env. episode 372.000000, reward total was -21.000000. running mean: -20.405806\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.411748\n",
            "resetting env. episode 374.000000, reward total was -21.000000. running mean: -20.417631\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.423454\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.429220\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.434928\n",
            "resetting env. episode 378.000000, reward total was -21.000000. running mean: -20.440578\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.446172\n",
            "resetting env. episode 380.000000, reward total was -20.000000. running mean: -20.441711\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.437294\n",
            "resetting env. episode 382.000000, reward total was -21.000000. running mean: -20.442921\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.448491\n",
            "resetting env. episode 384.000000, reward total was -19.000000. running mean: -20.434007\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.439667\n",
            "resetting env. episode 386.000000, reward total was -20.000000. running mean: -20.435270\n",
            "resetting env. episode 387.000000, reward total was -20.000000. running mean: -20.430917\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.436608\n",
            "resetting env. episode 389.000000, reward total was -19.000000. running mean: -20.422242\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.428019\n",
            "resetting env. episode 391.000000, reward total was -19.000000. running mean: -20.413739\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.419602\n",
            "resetting env. episode 393.000000, reward total was -19.000000. running mean: -20.405406\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.411352\n",
            "resetting env. episode 395.000000, reward total was -20.000000. running mean: -20.407238\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.413166\n",
            "resetting env. episode 397.000000, reward total was -21.000000. running mean: -20.419034\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.424844\n",
            "resetting env. episode 399.000000, reward total was -18.000000. running mean: -20.400595\n",
            "resetting env. episode 400.000000, reward total was -21.000000. running mean: -20.406590\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.412524\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.408398\n",
            "resetting env. episode 403.000000, reward total was -21.000000. running mean: -20.414314\n",
            "resetting env. episode 404.000000, reward total was -21.000000. running mean: -20.420171\n",
            "resetting env. episode 405.000000, reward total was -19.000000. running mean: -20.405970\n",
            "resetting env. episode 406.000000, reward total was -21.000000. running mean: -20.411910\n",
            "resetting env. episode 407.000000, reward total was -19.000000. running mean: -20.397791\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.403813\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.409775\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.405677\n",
            "resetting env. episode 411.000000, reward total was -21.000000. running mean: -20.411620\n",
            "resetting env. episode 412.000000, reward total was -19.000000. running mean: -20.397504\n",
            "resetting env. episode 413.000000, reward total was -21.000000. running mean: -20.403529\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.409494\n",
            "resetting env. episode 415.000000, reward total was -20.000000. running mean: -20.405399\n",
            "resetting env. episode 416.000000, reward total was -21.000000. running mean: -20.411345\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.417231\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.423059\n",
            "resetting env. episode 419.000000, reward total was -21.000000. running mean: -20.428828\n",
            "resetting env. episode 420.000000, reward total was -21.000000. running mean: -20.434540\n",
            "resetting env. episode 421.000000, reward total was -19.000000. running mean: -20.420195\n",
            "resetting env. episode 422.000000, reward total was -21.000000. running mean: -20.425993\n",
            "resetting env. episode 423.000000, reward total was -21.000000. running mean: -20.431733\n",
            "resetting env. episode 424.000000, reward total was -19.000000. running mean: -20.417416\n",
            "resetting env. episode 425.000000, reward total was -21.000000. running mean: -20.423241\n",
            "resetting env. episode 426.000000, reward total was -17.000000. running mean: -20.389009\n",
            "resetting env. episode 427.000000, reward total was -21.000000. running mean: -20.395119\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.401168\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.407156\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.413084\n",
            "resetting env. episode 431.000000, reward total was -21.000000. running mean: -20.418954\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.424764\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.430516\n",
            "resetting env. episode 434.000000, reward total was -20.000000. running mean: -20.426211\n",
            "resetting env. episode 435.000000, reward total was -21.000000. running mean: -20.431949\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.427630\n",
            "resetting env. episode 437.000000, reward total was -18.000000. running mean: -20.403353\n",
            "resetting env. episode 438.000000, reward total was -21.000000. running mean: -20.409320\n",
            "resetting env. episode 439.000000, reward total was -21.000000. running mean: -20.415227\n",
            "resetting env. episode 440.000000, reward total was -19.000000. running mean: -20.401074\n",
            "resetting env. episode 441.000000, reward total was -20.000000. running mean: -20.397064\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.403093\n",
            "resetting env. episode 443.000000, reward total was -20.000000. running mean: -20.399062\n",
            "resetting env. episode 444.000000, reward total was -19.000000. running mean: -20.385071\n",
            "resetting env. episode 445.000000, reward total was -21.000000. running mean: -20.391221\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.397308\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.403335\n",
            "resetting env. episode 448.000000, reward total was -21.000000. running mean: -20.409302\n",
            "resetting env. episode 449.000000, reward total was -21.000000. running mean: -20.415209\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.421057\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.426846\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.422578\n",
            "resetting env. episode 453.000000, reward total was -21.000000. running mean: -20.428352\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.434069\n",
            "resetting env. episode 455.000000, reward total was -19.000000. running mean: -20.419728\n",
            "resetting env. episode 456.000000, reward total was -20.000000. running mean: -20.415531\n",
            "resetting env. episode 457.000000, reward total was -20.000000. running mean: -20.411375\n",
            "resetting env. episode 458.000000, reward total was -21.000000. running mean: -20.417262\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.423089\n",
            "resetting env. episode 460.000000, reward total was -21.000000. running mean: -20.428858\n",
            "resetting env. episode 461.000000, reward total was -21.000000. running mean: -20.434569\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.440224\n",
            "resetting env. episode 463.000000, reward total was -20.000000. running mean: -20.435822\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.441463\n",
            "resetting env. episode 465.000000, reward total was -21.000000. running mean: -20.447049\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.452578\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.458052\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.453472\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.458937\n",
            "resetting env. episode 470.000000, reward total was -21.000000. running mean: -20.464348\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.469704\n",
            "resetting env. episode 472.000000, reward total was -21.000000. running mean: -20.475007\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.480257\n",
            "resetting env. episode 474.000000, reward total was -20.000000. running mean: -20.475455\n",
            "resetting env. episode 475.000000, reward total was -21.000000. running mean: -20.480700\n",
            "resetting env. episode 476.000000, reward total was -21.000000. running mean: -20.485893\n",
            "resetting env. episode 477.000000, reward total was -18.000000. running mean: -20.461034\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.466424\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.471760\n",
            "resetting env. episode 480.000000, reward total was -21.000000. running mean: -20.477042\n",
            "resetting env. episode 481.000000, reward total was -21.000000. running mean: -20.482272\n",
            "resetting env. episode 482.000000, reward total was -20.000000. running mean: -20.477449\n",
            "resetting env. episode 483.000000, reward total was -21.000000. running mean: -20.482674\n",
            "resetting env. episode 484.000000, reward total was -20.000000. running mean: -20.477848\n",
            "resetting env. episode 485.000000, reward total was -21.000000. running mean: -20.483069\n",
            "resetting env. episode 486.000000, reward total was -19.000000. running mean: -20.468238\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.473556\n",
            "resetting env. episode 488.000000, reward total was -19.000000. running mean: -20.458821\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.464232\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.469590\n",
            "resetting env. episode 491.000000, reward total was -21.000000. running mean: -20.474894\n",
            "resetting env. episode 492.000000, reward total was -20.000000. running mean: -20.470145\n",
            "resetting env. episode 493.000000, reward total was -20.000000. running mean: -20.465444\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.470789\n",
            "resetting env. episode 495.000000, reward total was -21.000000. running mean: -20.476081\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.481321\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.486507\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -20.471642\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.476926\n",
            "resetting env. episode 500.000000, reward total was -19.000000. running mean: -20.462157\n",
            "CPU times: user 22min 41s, sys: 10min 13s, total: 32min 54s\n",
            "Wall time: 16min 57s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8fheN9DRlWXQ",
        "outputId": "b5cde746-9b5a-4e59-9e9a-bd2b3ce3733c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9AxOcQhIsKow",
        "outputId": "cadbeefa-978c-45b1-cfa9-aee2c76ef198",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "%time hist3 = train_model(env, model, total_episodes=1500)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 3.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 4.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 5.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 6.000000, reward total was -21.000000. running mean: -21.000000\n",
            "resetting env. episode 7.000000, reward total was -19.000000. running mean: -20.980000\n",
            "resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.980200\n",
            "resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.980398\n",
            "resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.980594\n",
            "resetting env. episode 11.000000, reward total was -20.000000. running mean: -20.970788\n",
            "resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.971080\n",
            "resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.971369\n",
            "resetting env. episode 14.000000, reward total was -20.000000. running mean: -20.961656\n",
            "resetting env. episode 15.000000, reward total was -20.000000. running mean: -20.952039\n",
            "resetting env. episode 16.000000, reward total was -17.000000. running mean: -20.912519\n",
            "resetting env. episode 17.000000, reward total was -21.000000. running mean: -20.913394\n",
            "resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.914260\n",
            "resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.905117\n",
            "resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.906066\n",
            "resetting env. episode 21.000000, reward total was -18.000000. running mean: -20.877005\n",
            "resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.878235\n",
            "resetting env. episode 23.000000, reward total was -20.000000. running mean: -20.869453\n",
            "resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.870758\n",
            "resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.872051\n",
            "resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.873330\n",
            "resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.864597\n",
            "resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.865951\n",
            "resetting env. episode 29.000000, reward total was -19.000000. running mean: -20.847291\n",
            "resetting env. episode 30.000000, reward total was -19.000000. running mean: -20.828818\n",
            "resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.830530\n",
            "resetting env. episode 32.000000, reward total was -20.000000. running mean: -20.822225\n",
            "resetting env. episode 33.000000, reward total was -20.000000. running mean: -20.814003\n",
            "resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.815863\n",
            "resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.817704\n",
            "resetting env. episode 36.000000, reward total was -21.000000. running mean: -20.819527\n",
            "resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.821332\n",
            "resetting env. episode 38.000000, reward total was -21.000000. running mean: -20.823118\n",
            "resetting env. episode 39.000000, reward total was -20.000000. running mean: -20.814887\n",
            "resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.816738\n",
            "resetting env. episode 41.000000, reward total was -20.000000. running mean: -20.808571\n",
            "resetting env. episode 42.000000, reward total was -20.000000. running mean: -20.800485\n",
            "resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.792480\n",
            "resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.794556\n",
            "resetting env. episode 45.000000, reward total was -19.000000. running mean: -20.776610\n",
            "resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.778844\n",
            "resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.781056\n",
            "resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.783245\n",
            "resetting env. episode 49.000000, reward total was -21.000000. running mean: -20.785413\n",
            "resetting env. episode 50.000000, reward total was -20.000000. running mean: -20.777558\n",
            "resetting env. episode 51.000000, reward total was -19.000000. running mean: -20.759783\n",
            "resetting env. episode 52.000000, reward total was -21.000000. running mean: -20.762185\n",
            "resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.764563\n",
            "resetting env. episode 54.000000, reward total was -20.000000. running mean: -20.756918\n",
            "resetting env. episode 55.000000, reward total was -21.000000. running mean: -20.759348\n",
            "resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.761755\n",
            "resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.764137\n",
            "resetting env. episode 58.000000, reward total was -20.000000. running mean: -20.756496\n",
            "resetting env. episode 59.000000, reward total was -20.000000. running mean: -20.748931\n",
            "resetting env. episode 60.000000, reward total was -20.000000. running mean: -20.741442\n",
            "resetting env. episode 61.000000, reward total was -19.000000. running mean: -20.724027\n",
            "resetting env. episode 62.000000, reward total was -21.000000. running mean: -20.726787\n",
            "resetting env. episode 63.000000, reward total was -21.000000. running mean: -20.729519\n",
            "resetting env. episode 64.000000, reward total was -19.000000. running mean: -20.712224\n",
            "resetting env. episode 65.000000, reward total was -19.000000. running mean: -20.695102\n",
            "resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.698151\n",
            "resetting env. episode 67.000000, reward total was -19.000000. running mean: -20.681169\n",
            "resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.684357\n",
            "resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.687514\n",
            "resetting env. episode 70.000000, reward total was -20.000000. running mean: -20.680639\n",
            "resetting env. episode 71.000000, reward total was -19.000000. running mean: -20.663832\n",
            "resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.667194\n",
            "resetting env. episode 73.000000, reward total was -21.000000. running mean: -20.670522\n",
            "resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.673817\n",
            "resetting env. episode 75.000000, reward total was -20.000000. running mean: -20.667079\n",
            "resetting env. episode 76.000000, reward total was -21.000000. running mean: -20.670408\n",
            "resetting env. episode 77.000000, reward total was -21.000000. running mean: -20.673704\n",
            "resetting env. episode 78.000000, reward total was -21.000000. running mean: -20.676967\n",
            "resetting env. episode 79.000000, reward total was -21.000000. running mean: -20.680197\n",
            "resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.683395\n",
            "resetting env. episode 81.000000, reward total was -21.000000. running mean: -20.686561\n",
            "resetting env. episode 82.000000, reward total was -20.000000. running mean: -20.679696\n",
            "resetting env. episode 83.000000, reward total was -19.000000. running mean: -20.662899\n",
            "resetting env. episode 84.000000, reward total was -20.000000. running mean: -20.656270\n",
            "resetting env. episode 85.000000, reward total was -21.000000. running mean: -20.659707\n",
            "resetting env. episode 86.000000, reward total was -20.000000. running mean: -20.653110\n",
            "resetting env. episode 87.000000, reward total was -20.000000. running mean: -20.646579\n",
            "resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.650113\n",
            "resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.653612\n",
            "resetting env. episode 90.000000, reward total was -21.000000. running mean: -20.657076\n",
            "resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.660505\n",
            "resetting env. episode 92.000000, reward total was -20.000000. running mean: -20.653900\n",
            "resetting env. episode 93.000000, reward total was -21.000000. running mean: -20.657361\n",
            "resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.660787\n",
            "resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.664179\n",
            "resetting env. episode 96.000000, reward total was -21.000000. running mean: -20.667538\n",
            "resetting env. episode 97.000000, reward total was -21.000000. running mean: -20.670862\n",
            "resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.674154\n",
            "resetting env. episode 99.000000, reward total was -20.000000. running mean: -20.667412\n",
            "resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.660738\n",
            "resetting env. episode 101.000000, reward total was -20.000000. running mean: -20.654131\n",
            "resetting env. episode 102.000000, reward total was -19.000000. running mean: -20.637589\n",
            "resetting env. episode 103.000000, reward total was -21.000000. running mean: -20.641213\n",
            "resetting env. episode 104.000000, reward total was -20.000000. running mean: -20.634801\n",
            "resetting env. episode 105.000000, reward total was -21.000000. running mean: -20.638453\n",
            "resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.642069\n",
            "resetting env. episode 107.000000, reward total was -20.000000. running mean: -20.635648\n",
            "resetting env. episode 108.000000, reward total was -21.000000. running mean: -20.639292\n",
            "resetting env. episode 109.000000, reward total was -20.000000. running mean: -20.632899\n",
            "resetting env. episode 110.000000, reward total was -21.000000. running mean: -20.636570\n",
            "resetting env. episode 111.000000, reward total was -20.000000. running mean: -20.630204\n",
            "resetting env. episode 112.000000, reward total was -20.000000. running mean: -20.623902\n",
            "resetting env. episode 113.000000, reward total was -19.000000. running mean: -20.607663\n",
            "resetting env. episode 114.000000, reward total was -19.000000. running mean: -20.591586\n",
            "resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.595670\n",
            "resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.599714\n",
            "resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.603717\n",
            "resetting env. episode 118.000000, reward total was -20.000000. running mean: -20.597679\n",
            "resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.601703\n",
            "resetting env. episode 120.000000, reward total was -19.000000. running mean: -20.585686\n",
            "resetting env. episode 121.000000, reward total was -20.000000. running mean: -20.579829\n",
            "resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.574030\n",
            "resetting env. episode 123.000000, reward total was -21.000000. running mean: -20.578290\n",
            "resetting env. episode 124.000000, reward total was -18.000000. running mean: -20.552507\n",
            "resetting env. episode 125.000000, reward total was -20.000000. running mean: -20.546982\n",
            "resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.551512\n",
            "resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.545997\n",
            "resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.550537\n",
            "resetting env. episode 129.000000, reward total was -19.000000. running mean: -20.535032\n",
            "resetting env. episode 130.000000, reward total was -20.000000. running mean: -20.529682\n",
            "resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.534385\n",
            "resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.529041\n",
            "resetting env. episode 133.000000, reward total was -21.000000. running mean: -20.533751\n",
            "resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.538413\n",
            "resetting env. episode 135.000000, reward total was -20.000000. running mean: -20.533029\n",
            "resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.527699\n",
            "resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.532422\n",
            "resetting env. episode 138.000000, reward total was -21.000000. running mean: -20.537097\n",
            "resetting env. episode 139.000000, reward total was -21.000000. running mean: -20.541726\n",
            "resetting env. episode 140.000000, reward total was -20.000000. running mean: -20.536309\n",
            "resetting env. episode 141.000000, reward total was -20.000000. running mean: -20.530946\n",
            "resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.535637\n",
            "resetting env. episode 143.000000, reward total was -21.000000. running mean: -20.540280\n",
            "resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.544877\n",
            "resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.539429\n",
            "resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.544034\n",
            "resetting env. episode 147.000000, reward total was -19.000000. running mean: -20.528594\n",
            "resetting env. episode 148.000000, reward total was -21.000000. running mean: -20.533308\n",
            "resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.537975\n",
            "resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.542595\n",
            "resetting env. episode 151.000000, reward total was -17.000000. running mean: -20.507169\n",
            "resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.512098\n",
            "resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.516977\n",
            "resetting env. episode 154.000000, reward total was -19.000000. running mean: -20.501807\n",
            "resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.496789\n",
            "resetting env. episode 156.000000, reward total was -21.000000. running mean: -20.501821\n",
            "resetting env. episode 157.000000, reward total was -21.000000. running mean: -20.506803\n",
            "resetting env. episode 158.000000, reward total was -20.000000. running mean: -20.501735\n",
            "resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.496717\n",
            "resetting env. episode 160.000000, reward total was -21.000000. running mean: -20.501750\n",
            "resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.506733\n",
            "resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.501665\n",
            "resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.506649\n",
            "resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.511582\n",
            "resetting env. episode 165.000000, reward total was -20.000000. running mean: -20.506466\n",
            "resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.511402\n",
            "resetting env. episode 167.000000, reward total was -21.000000. running mean: -20.516288\n",
            "resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.521125\n",
            "resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.515914\n",
            "resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.520754\n",
            "resetting env. episode 171.000000, reward total was -19.000000. running mean: -20.505547\n",
            "resetting env. episode 172.000000, reward total was -18.000000. running mean: -20.480491\n",
            "resetting env. episode 173.000000, reward total was -21.000000. running mean: -20.485686\n",
            "resetting env. episode 174.000000, reward total was -21.000000. running mean: -20.490830\n",
            "resetting env. episode 175.000000, reward total was -21.000000. running mean: -20.495921\n",
            "resetting env. episode 176.000000, reward total was -20.000000. running mean: -20.490962\n",
            "resetting env. episode 177.000000, reward total was -21.000000. running mean: -20.496052\n",
            "resetting env. episode 178.000000, reward total was -21.000000. running mean: -20.501092\n",
            "resetting env. episode 179.000000, reward total was -21.000000. running mean: -20.506081\n",
            "resetting env. episode 180.000000, reward total was -19.000000. running mean: -20.491020\n",
            "resetting env. episode 181.000000, reward total was -20.000000. running mean: -20.486110\n",
            "resetting env. episode 182.000000, reward total was -20.000000. running mean: -20.481249\n",
            "resetting env. episode 183.000000, reward total was -21.000000. running mean: -20.486436\n",
            "resetting env. episode 184.000000, reward total was -20.000000. running mean: -20.481572\n",
            "resetting env. episode 185.000000, reward total was -21.000000. running mean: -20.486756\n",
            "resetting env. episode 186.000000, reward total was -21.000000. running mean: -20.491889\n",
            "resetting env. episode 187.000000, reward total was -19.000000. running mean: -20.476970\n",
            "resetting env. episode 188.000000, reward total was -21.000000. running mean: -20.482200\n",
            "resetting env. episode 189.000000, reward total was -21.000000. running mean: -20.487378\n",
            "resetting env. episode 190.000000, reward total was -19.000000. running mean: -20.472504\n",
            "resetting env. episode 191.000000, reward total was -21.000000. running mean: -20.477779\n",
            "resetting env. episode 192.000000, reward total was -21.000000. running mean: -20.483002\n",
            "resetting env. episode 193.000000, reward total was -20.000000. running mean: -20.478172\n",
            "resetting env. episode 194.000000, reward total was -20.000000. running mean: -20.473390\n",
            "resetting env. episode 195.000000, reward total was -21.000000. running mean: -20.478656\n",
            "resetting env. episode 196.000000, reward total was -21.000000. running mean: -20.483869\n",
            "resetting env. episode 197.000000, reward total was -18.000000. running mean: -20.459031\n",
            "resetting env. episode 198.000000, reward total was -21.000000. running mean: -20.464440\n",
            "resetting env. episode 199.000000, reward total was -20.000000. running mean: -20.459796\n",
            "resetting env. episode 200.000000, reward total was -21.000000. running mean: -20.465198\n",
            "resetting env. episode 201.000000, reward total was -21.000000. running mean: -20.470546\n",
            "resetting env. episode 202.000000, reward total was -20.000000. running mean: -20.465841\n",
            "resetting env. episode 203.000000, reward total was -21.000000. running mean: -20.471182\n",
            "resetting env. episode 204.000000, reward total was -21.000000. running mean: -20.476470\n",
            "resetting env. episode 205.000000, reward total was -21.000000. running mean: -20.481706\n",
            "resetting env. episode 206.000000, reward total was -21.000000. running mean: -20.486889\n",
            "resetting env. episode 207.000000, reward total was -19.000000. running mean: -20.472020\n",
            "resetting env. episode 208.000000, reward total was -20.000000. running mean: -20.467300\n",
            "resetting env. episode 209.000000, reward total was -21.000000. running mean: -20.472627\n",
            "resetting env. episode 210.000000, reward total was -20.000000. running mean: -20.467900\n",
            "resetting env. episode 211.000000, reward total was -20.000000. running mean: -20.463221\n",
            "resetting env. episode 212.000000, reward total was -21.000000. running mean: -20.468589\n",
            "resetting env. episode 213.000000, reward total was -19.000000. running mean: -20.453903\n",
            "resetting env. episode 214.000000, reward total was -18.000000. running mean: -20.429364\n",
            "resetting env. episode 215.000000, reward total was -21.000000. running mean: -20.435070\n",
            "resetting env. episode 216.000000, reward total was -20.000000. running mean: -20.430720\n",
            "resetting env. episode 217.000000, reward total was -19.000000. running mean: -20.416413\n",
            "resetting env. episode 218.000000, reward total was -21.000000. running mean: -20.422248\n",
            "resetting env. episode 219.000000, reward total was -20.000000. running mean: -20.418026\n",
            "resetting env. episode 220.000000, reward total was -21.000000. running mean: -20.423846\n",
            "resetting env. episode 221.000000, reward total was -21.000000. running mean: -20.429607\n",
            "resetting env. episode 222.000000, reward total was -21.000000. running mean: -20.435311\n",
            "resetting env. episode 223.000000, reward total was -17.000000. running mean: -20.400958\n",
            "resetting env. episode 224.000000, reward total was -20.000000. running mean: -20.396948\n",
            "resetting env. episode 225.000000, reward total was -21.000000. running mean: -20.402979\n",
            "resetting env. episode 226.000000, reward total was -20.000000. running mean: -20.398949\n",
            "resetting env. episode 227.000000, reward total was -21.000000. running mean: -20.404960\n",
            "resetting env. episode 228.000000, reward total was -21.000000. running mean: -20.410910\n",
            "resetting env. episode 229.000000, reward total was -21.000000. running mean: -20.416801\n",
            "resetting env. episode 230.000000, reward total was -20.000000. running mean: -20.412633\n",
            "resetting env. episode 231.000000, reward total was -21.000000. running mean: -20.418507\n",
            "resetting env. episode 232.000000, reward total was -21.000000. running mean: -20.424322\n",
            "resetting env. episode 233.000000, reward total was -21.000000. running mean: -20.430078\n",
            "resetting env. episode 234.000000, reward total was -21.000000. running mean: -20.435778\n",
            "resetting env. episode 235.000000, reward total was -21.000000. running mean: -20.441420\n",
            "resetting env. episode 236.000000, reward total was -20.000000. running mean: -20.437006\n",
            "resetting env. episode 237.000000, reward total was -20.000000. running mean: -20.432636\n",
            "resetting env. episode 238.000000, reward total was -20.000000. running mean: -20.428309\n",
            "resetting env. episode 239.000000, reward total was -20.000000. running mean: -20.424026\n",
            "resetting env. episode 240.000000, reward total was -19.000000. running mean: -20.409786\n",
            "resetting env. episode 241.000000, reward total was -21.000000. running mean: -20.415688\n",
            "resetting env. episode 242.000000, reward total was -21.000000. running mean: -20.421531\n",
            "resetting env. episode 243.000000, reward total was -21.000000. running mean: -20.427316\n",
            "resetting env. episode 244.000000, reward total was -20.000000. running mean: -20.423043\n",
            "resetting env. episode 245.000000, reward total was -19.000000. running mean: -20.408812\n",
            "resetting env. episode 246.000000, reward total was -20.000000. running mean: -20.404724\n",
            "resetting env. episode 247.000000, reward total was -21.000000. running mean: -20.410677\n",
            "resetting env. episode 248.000000, reward total was -19.000000. running mean: -20.396570\n",
            "resetting env. episode 249.000000, reward total was -21.000000. running mean: -20.402604\n",
            "resetting env. episode 250.000000, reward total was -21.000000. running mean: -20.408578\n",
            "resetting env. episode 251.000000, reward total was -21.000000. running mean: -20.414493\n",
            "resetting env. episode 252.000000, reward total was -21.000000. running mean: -20.420348\n",
            "resetting env. episode 253.000000, reward total was -20.000000. running mean: -20.416144\n",
            "resetting env. episode 254.000000, reward total was -21.000000. running mean: -20.421983\n",
            "resetting env. episode 255.000000, reward total was -18.000000. running mean: -20.397763\n",
            "resetting env. episode 256.000000, reward total was -20.000000. running mean: -20.393785\n",
            "resetting env. episode 257.000000, reward total was -20.000000. running mean: -20.389847\n",
            "resetting env. episode 258.000000, reward total was -21.000000. running mean: -20.395949\n",
            "resetting env. episode 259.000000, reward total was -20.000000. running mean: -20.391989\n",
            "resetting env. episode 260.000000, reward total was -21.000000. running mean: -20.398070\n",
            "resetting env. episode 261.000000, reward total was -21.000000. running mean: -20.404089\n",
            "resetting env. episode 262.000000, reward total was -21.000000. running mean: -20.410048\n",
            "resetting env. episode 263.000000, reward total was -21.000000. running mean: -20.415948\n",
            "resetting env. episode 264.000000, reward total was -20.000000. running mean: -20.411788\n",
            "resetting env. episode 265.000000, reward total was -21.000000. running mean: -20.417670\n",
            "resetting env. episode 266.000000, reward total was -21.000000. running mean: -20.423493\n",
            "resetting env. episode 267.000000, reward total was -20.000000. running mean: -20.419259\n",
            "resetting env. episode 268.000000, reward total was -21.000000. running mean: -20.425066\n",
            "resetting env. episode 269.000000, reward total was -21.000000. running mean: -20.430815\n",
            "resetting env. episode 270.000000, reward total was -19.000000. running mean: -20.416507\n",
            "resetting env. episode 271.000000, reward total was -21.000000. running mean: -20.422342\n",
            "resetting env. episode 272.000000, reward total was -20.000000. running mean: -20.418119\n",
            "resetting env. episode 273.000000, reward total was -21.000000. running mean: -20.423937\n",
            "resetting env. episode 274.000000, reward total was -21.000000. running mean: -20.429698\n",
            "resetting env. episode 275.000000, reward total was -21.000000. running mean: -20.435401\n",
            "resetting env. episode 276.000000, reward total was -20.000000. running mean: -20.431047\n",
            "resetting env. episode 277.000000, reward total was -19.000000. running mean: -20.416737\n",
            "resetting env. episode 278.000000, reward total was -21.000000. running mean: -20.422569\n",
            "resetting env. episode 279.000000, reward total was -21.000000. running mean: -20.428344\n",
            "resetting env. episode 280.000000, reward total was -20.000000. running mean: -20.424060\n",
            "resetting env. episode 281.000000, reward total was -21.000000. running mean: -20.429820\n",
            "resetting env. episode 282.000000, reward total was -20.000000. running mean: -20.425521\n",
            "resetting env. episode 283.000000, reward total was -21.000000. running mean: -20.431266\n",
            "resetting env. episode 284.000000, reward total was -21.000000. running mean: -20.436953\n",
            "resetting env. episode 285.000000, reward total was -21.000000. running mean: -20.442584\n",
            "resetting env. episode 286.000000, reward total was -17.000000. running mean: -20.408158\n",
            "resetting env. episode 287.000000, reward total was -20.000000. running mean: -20.404076\n",
            "resetting env. episode 288.000000, reward total was -20.000000. running mean: -20.400036\n",
            "resetting env. episode 289.000000, reward total was -21.000000. running mean: -20.406035\n",
            "resetting env. episode 290.000000, reward total was -20.000000. running mean: -20.401975\n",
            "resetting env. episode 291.000000, reward total was -21.000000. running mean: -20.407955\n",
            "resetting env. episode 292.000000, reward total was -20.000000. running mean: -20.403876\n",
            "resetting env. episode 293.000000, reward total was -21.000000. running mean: -20.409837\n",
            "resetting env. episode 294.000000, reward total was -20.000000. running mean: -20.405739\n",
            "resetting env. episode 295.000000, reward total was -20.000000. running mean: -20.401681\n",
            "resetting env. episode 296.000000, reward total was -19.000000. running mean: -20.387664\n",
            "resetting env. episode 297.000000, reward total was -19.000000. running mean: -20.373788\n",
            "resetting env. episode 298.000000, reward total was -20.000000. running mean: -20.370050\n",
            "resetting env. episode 299.000000, reward total was -21.000000. running mean: -20.376349\n",
            "resetting env. episode 300.000000, reward total was -21.000000. running mean: -20.382586\n",
            "resetting env. episode 301.000000, reward total was -21.000000. running mean: -20.388760\n",
            "resetting env. episode 302.000000, reward total was -21.000000. running mean: -20.394872\n",
            "resetting env. episode 303.000000, reward total was -20.000000. running mean: -20.390924\n",
            "resetting env. episode 304.000000, reward total was -20.000000. running mean: -20.387014\n",
            "resetting env. episode 305.000000, reward total was -21.000000. running mean: -20.393144\n",
            "resetting env. episode 306.000000, reward total was -20.000000. running mean: -20.389213\n",
            "resetting env. episode 307.000000, reward total was -21.000000. running mean: -20.395321\n",
            "resetting env. episode 308.000000, reward total was -21.000000. running mean: -20.401368\n",
            "resetting env. episode 309.000000, reward total was -21.000000. running mean: -20.407354\n",
            "resetting env. episode 310.000000, reward total was -19.000000. running mean: -20.393280\n",
            "resetting env. episode 311.000000, reward total was -19.000000. running mean: -20.379348\n",
            "resetting env. episode 312.000000, reward total was -20.000000. running mean: -20.375554\n",
            "resetting env. episode 313.000000, reward total was -19.000000. running mean: -20.361798\n",
            "resetting env. episode 314.000000, reward total was -20.000000. running mean: -20.358181\n",
            "resetting env. episode 315.000000, reward total was -21.000000. running mean: -20.364599\n",
            "resetting env. episode 316.000000, reward total was -20.000000. running mean: -20.360953\n",
            "resetting env. episode 317.000000, reward total was -21.000000. running mean: -20.367343\n",
            "resetting env. episode 318.000000, reward total was -20.000000. running mean: -20.363670\n",
            "resetting env. episode 319.000000, reward total was -20.000000. running mean: -20.360033\n",
            "resetting env. episode 320.000000, reward total was -20.000000. running mean: -20.356433\n",
            "resetting env. episode 321.000000, reward total was -20.000000. running mean: -20.352868\n",
            "resetting env. episode 322.000000, reward total was -20.000000. running mean: -20.349340\n",
            "resetting env. episode 323.000000, reward total was -21.000000. running mean: -20.355846\n",
            "resetting env. episode 324.000000, reward total was -20.000000. running mean: -20.352288\n",
            "resetting env. episode 325.000000, reward total was -21.000000. running mean: -20.358765\n",
            "resetting env. episode 326.000000, reward total was -19.000000. running mean: -20.345177\n",
            "resetting env. episode 327.000000, reward total was -21.000000. running mean: -20.351726\n",
            "resetting env. episode 328.000000, reward total was -21.000000. running mean: -20.358208\n",
            "resetting env. episode 329.000000, reward total was -21.000000. running mean: -20.364626\n",
            "resetting env. episode 330.000000, reward total was -20.000000. running mean: -20.360980\n",
            "resetting env. episode 331.000000, reward total was -21.000000. running mean: -20.367370\n",
            "resetting env. episode 332.000000, reward total was -21.000000. running mean: -20.373696\n",
            "resetting env. episode 333.000000, reward total was -20.000000. running mean: -20.369959\n",
            "resetting env. episode 334.000000, reward total was -21.000000. running mean: -20.376260\n",
            "resetting env. episode 335.000000, reward total was -20.000000. running mean: -20.372497\n",
            "resetting env. episode 336.000000, reward total was -21.000000. running mean: -20.378772\n",
            "resetting env. episode 337.000000, reward total was -20.000000. running mean: -20.374985\n",
            "resetting env. episode 338.000000, reward total was -21.000000. running mean: -20.381235\n",
            "resetting env. episode 339.000000, reward total was -21.000000. running mean: -20.387422\n",
            "resetting env. episode 340.000000, reward total was -21.000000. running mean: -20.393548\n",
            "resetting env. episode 341.000000, reward total was -21.000000. running mean: -20.399613\n",
            "resetting env. episode 342.000000, reward total was -21.000000. running mean: -20.405617\n",
            "resetting env. episode 343.000000, reward total was -20.000000. running mean: -20.401560\n",
            "resetting env. episode 344.000000, reward total was -21.000000. running mean: -20.407545\n",
            "resetting env. episode 345.000000, reward total was -18.000000. running mean: -20.383469\n",
            "resetting env. episode 346.000000, reward total was -19.000000. running mean: -20.369635\n",
            "resetting env. episode 347.000000, reward total was -21.000000. running mean: -20.375938\n",
            "resetting env. episode 348.000000, reward total was -21.000000. running mean: -20.382179\n",
            "resetting env. episode 349.000000, reward total was -21.000000. running mean: -20.388357\n",
            "resetting env. episode 350.000000, reward total was -21.000000. running mean: -20.394474\n",
            "resetting env. episode 351.000000, reward total was -20.000000. running mean: -20.390529\n",
            "resetting env. episode 352.000000, reward total was -20.000000. running mean: -20.386624\n",
            "resetting env. episode 353.000000, reward total was -21.000000. running mean: -20.392757\n",
            "resetting env. episode 354.000000, reward total was -21.000000. running mean: -20.398830\n",
            "resetting env. episode 355.000000, reward total was -20.000000. running mean: -20.394841\n",
            "resetting env. episode 356.000000, reward total was -19.000000. running mean: -20.380893\n",
            "resetting env. episode 357.000000, reward total was -21.000000. running mean: -20.387084\n",
            "resetting env. episode 358.000000, reward total was -21.000000. running mean: -20.393213\n",
            "resetting env. episode 359.000000, reward total was -20.000000. running mean: -20.389281\n",
            "resetting env. episode 360.000000, reward total was -21.000000. running mean: -20.395388\n",
            "resetting env. episode 361.000000, reward total was -20.000000. running mean: -20.391434\n",
            "resetting env. episode 362.000000, reward total was -20.000000. running mean: -20.387520\n",
            "resetting env. episode 363.000000, reward total was -21.000000. running mean: -20.393645\n",
            "resetting env. episode 364.000000, reward total was -20.000000. running mean: -20.389708\n",
            "resetting env. episode 365.000000, reward total was -21.000000. running mean: -20.395811\n",
            "resetting env. episode 366.000000, reward total was -20.000000. running mean: -20.391853\n",
            "resetting env. episode 367.000000, reward total was -21.000000. running mean: -20.397935\n",
            "resetting env. episode 368.000000, reward total was -21.000000. running mean: -20.403955\n",
            "resetting env. episode 369.000000, reward total was -21.000000. running mean: -20.409916\n",
            "resetting env. episode 370.000000, reward total was -21.000000. running mean: -20.415817\n",
            "resetting env. episode 371.000000, reward total was -21.000000. running mean: -20.421658\n",
            "resetting env. episode 372.000000, reward total was -20.000000. running mean: -20.417442\n",
            "resetting env. episode 373.000000, reward total was -21.000000. running mean: -20.423267\n",
            "resetting env. episode 374.000000, reward total was -20.000000. running mean: -20.419035\n",
            "resetting env. episode 375.000000, reward total was -21.000000. running mean: -20.424844\n",
            "resetting env. episode 376.000000, reward total was -21.000000. running mean: -20.430596\n",
            "resetting env. episode 377.000000, reward total was -21.000000. running mean: -20.436290\n",
            "resetting env. episode 378.000000, reward total was -20.000000. running mean: -20.431927\n",
            "resetting env. episode 379.000000, reward total was -21.000000. running mean: -20.437608\n",
            "resetting env. episode 380.000000, reward total was -21.000000. running mean: -20.443232\n",
            "resetting env. episode 381.000000, reward total was -20.000000. running mean: -20.438799\n",
            "resetting env. episode 382.000000, reward total was -19.000000. running mean: -20.424411\n",
            "resetting env. episode 383.000000, reward total was -21.000000. running mean: -20.430167\n",
            "resetting env. episode 384.000000, reward total was -21.000000. running mean: -20.435866\n",
            "resetting env. episode 385.000000, reward total was -21.000000. running mean: -20.441507\n",
            "resetting env. episode 386.000000, reward total was -21.000000. running mean: -20.447092\n",
            "resetting env. episode 387.000000, reward total was -21.000000. running mean: -20.452621\n",
            "resetting env. episode 388.000000, reward total was -21.000000. running mean: -20.458095\n",
            "resetting env. episode 389.000000, reward total was -19.000000. running mean: -20.443514\n",
            "resetting env. episode 390.000000, reward total was -21.000000. running mean: -20.449079\n",
            "resetting env. episode 391.000000, reward total was -20.000000. running mean: -20.444588\n",
            "resetting env. episode 392.000000, reward total was -21.000000. running mean: -20.450142\n",
            "resetting env. episode 393.000000, reward total was -21.000000. running mean: -20.455641\n",
            "resetting env. episode 394.000000, reward total was -21.000000. running mean: -20.461084\n",
            "resetting env. episode 395.000000, reward total was -19.000000. running mean: -20.446473\n",
            "resetting env. episode 396.000000, reward total was -21.000000. running mean: -20.452009\n",
            "resetting env. episode 397.000000, reward total was -20.000000. running mean: -20.447489\n",
            "resetting env. episode 398.000000, reward total was -21.000000. running mean: -20.453014\n",
            "resetting env. episode 399.000000, reward total was -21.000000. running mean: -20.458484\n",
            "resetting env. episode 400.000000, reward total was -20.000000. running mean: -20.453899\n",
            "resetting env. episode 401.000000, reward total was -21.000000. running mean: -20.459360\n",
            "resetting env. episode 402.000000, reward total was -20.000000. running mean: -20.454766\n",
            "resetting env. episode 403.000000, reward total was -20.000000. running mean: -20.450218\n",
            "resetting env. episode 404.000000, reward total was -20.000000. running mean: -20.445716\n",
            "resetting env. episode 405.000000, reward total was -18.000000. running mean: -20.421259\n",
            "resetting env. episode 406.000000, reward total was -20.000000. running mean: -20.417047\n",
            "resetting env. episode 407.000000, reward total was -19.000000. running mean: -20.402876\n",
            "resetting env. episode 408.000000, reward total was -21.000000. running mean: -20.408847\n",
            "resetting env. episode 409.000000, reward total was -21.000000. running mean: -20.414759\n",
            "resetting env. episode 410.000000, reward total was -20.000000. running mean: -20.410611\n",
            "resetting env. episode 411.000000, reward total was -20.000000. running mean: -20.406505\n",
            "resetting env. episode 412.000000, reward total was -19.000000. running mean: -20.392440\n",
            "resetting env. episode 413.000000, reward total was -19.000000. running mean: -20.378516\n",
            "resetting env. episode 414.000000, reward total was -21.000000. running mean: -20.384731\n",
            "resetting env. episode 415.000000, reward total was -18.000000. running mean: -20.360883\n",
            "resetting env. episode 416.000000, reward total was -20.000000. running mean: -20.357274\n",
            "resetting env. episode 417.000000, reward total was -21.000000. running mean: -20.363702\n",
            "resetting env. episode 418.000000, reward total was -21.000000. running mean: -20.370065\n",
            "resetting env. episode 419.000000, reward total was -20.000000. running mean: -20.366364\n",
            "resetting env. episode 420.000000, reward total was -20.000000. running mean: -20.362700\n",
            "resetting env. episode 421.000000, reward total was -21.000000. running mean: -20.369073\n",
            "resetting env. episode 422.000000, reward total was -20.000000. running mean: -20.365383\n",
            "resetting env. episode 423.000000, reward total was -20.000000. running mean: -20.361729\n",
            "resetting env. episode 424.000000, reward total was -19.000000. running mean: -20.348112\n",
            "resetting env. episode 425.000000, reward total was -20.000000. running mean: -20.344630\n",
            "resetting env. episode 426.000000, reward total was -21.000000. running mean: -20.351184\n",
            "resetting env. episode 427.000000, reward total was -19.000000. running mean: -20.337672\n",
            "resetting env. episode 428.000000, reward total was -21.000000. running mean: -20.344296\n",
            "resetting env. episode 429.000000, reward total was -21.000000. running mean: -20.350853\n",
            "resetting env. episode 430.000000, reward total was -21.000000. running mean: -20.357344\n",
            "resetting env. episode 431.000000, reward total was -19.000000. running mean: -20.343771\n",
            "resetting env. episode 432.000000, reward total was -21.000000. running mean: -20.350333\n",
            "resetting env. episode 433.000000, reward total was -21.000000. running mean: -20.356830\n",
            "resetting env. episode 434.000000, reward total was -21.000000. running mean: -20.363261\n",
            "resetting env. episode 435.000000, reward total was -20.000000. running mean: -20.359629\n",
            "resetting env. episode 436.000000, reward total was -20.000000. running mean: -20.356032\n",
            "resetting env. episode 437.000000, reward total was -21.000000. running mean: -20.362472\n",
            "resetting env. episode 438.000000, reward total was -20.000000. running mean: -20.358847\n",
            "resetting env. episode 439.000000, reward total was -20.000000. running mean: -20.355259\n",
            "resetting env. episode 440.000000, reward total was -21.000000. running mean: -20.361706\n",
            "resetting env. episode 441.000000, reward total was -21.000000. running mean: -20.368089\n",
            "resetting env. episode 442.000000, reward total was -21.000000. running mean: -20.374408\n",
            "resetting env. episode 443.000000, reward total was -21.000000. running mean: -20.380664\n",
            "resetting env. episode 444.000000, reward total was -21.000000. running mean: -20.386858\n",
            "resetting env. episode 445.000000, reward total was -20.000000. running mean: -20.382989\n",
            "resetting env. episode 446.000000, reward total was -21.000000. running mean: -20.389159\n",
            "resetting env. episode 447.000000, reward total was -21.000000. running mean: -20.395268\n",
            "resetting env. episode 448.000000, reward total was -20.000000. running mean: -20.391315\n",
            "resetting env. episode 449.000000, reward total was -20.000000. running mean: -20.387402\n",
            "resetting env. episode 450.000000, reward total was -21.000000. running mean: -20.393528\n",
            "resetting env. episode 451.000000, reward total was -21.000000. running mean: -20.399592\n",
            "resetting env. episode 452.000000, reward total was -20.000000. running mean: -20.395596\n",
            "resetting env. episode 453.000000, reward total was -17.000000. running mean: -20.361641\n",
            "resetting env. episode 454.000000, reward total was -21.000000. running mean: -20.368024\n",
            "resetting env. episode 455.000000, reward total was -21.000000. running mean: -20.374344\n",
            "resetting env. episode 456.000000, reward total was -21.000000. running mean: -20.380600\n",
            "resetting env. episode 457.000000, reward total was -21.000000. running mean: -20.386794\n",
            "resetting env. episode 458.000000, reward total was -19.000000. running mean: -20.372926\n",
            "resetting env. episode 459.000000, reward total was -21.000000. running mean: -20.379197\n",
            "resetting env. episode 460.000000, reward total was -20.000000. running mean: -20.375405\n",
            "resetting env. episode 461.000000, reward total was -19.000000. running mean: -20.361651\n",
            "resetting env. episode 462.000000, reward total was -21.000000. running mean: -20.368035\n",
            "resetting env. episode 463.000000, reward total was -21.000000. running mean: -20.374354\n",
            "resetting env. episode 464.000000, reward total was -21.000000. running mean: -20.380611\n",
            "resetting env. episode 465.000000, reward total was -20.000000. running mean: -20.376805\n",
            "resetting env. episode 466.000000, reward total was -21.000000. running mean: -20.383037\n",
            "resetting env. episode 467.000000, reward total was -21.000000. running mean: -20.389206\n",
            "resetting env. episode 468.000000, reward total was -20.000000. running mean: -20.385314\n",
            "resetting env. episode 469.000000, reward total was -21.000000. running mean: -20.391461\n",
            "resetting env. episode 470.000000, reward total was -17.000000. running mean: -20.357546\n",
            "resetting env. episode 471.000000, reward total was -21.000000. running mean: -20.363971\n",
            "resetting env. episode 472.000000, reward total was -20.000000. running mean: -20.360331\n",
            "resetting env. episode 473.000000, reward total was -21.000000. running mean: -20.366728\n",
            "resetting env. episode 474.000000, reward total was -21.000000. running mean: -20.373061\n",
            "resetting env. episode 475.000000, reward total was -20.000000. running mean: -20.369330\n",
            "resetting env. episode 476.000000, reward total was -20.000000. running mean: -20.365637\n",
            "resetting env. episode 477.000000, reward total was -21.000000. running mean: -20.371980\n",
            "resetting env. episode 478.000000, reward total was -21.000000. running mean: -20.378261\n",
            "resetting env. episode 479.000000, reward total was -21.000000. running mean: -20.384478\n",
            "resetting env. episode 480.000000, reward total was -20.000000. running mean: -20.380633\n",
            "resetting env. episode 481.000000, reward total was -20.000000. running mean: -20.376827\n",
            "resetting env. episode 482.000000, reward total was -21.000000. running mean: -20.383059\n",
            "resetting env. episode 483.000000, reward total was -18.000000. running mean: -20.359228\n",
            "resetting env. episode 484.000000, reward total was -21.000000. running mean: -20.365636\n",
            "resetting env. episode 485.000000, reward total was -19.000000. running mean: -20.351979\n",
            "resetting env. episode 486.000000, reward total was -21.000000. running mean: -20.358460\n",
            "resetting env. episode 487.000000, reward total was -21.000000. running mean: -20.364875\n",
            "resetting env. episode 488.000000, reward total was -21.000000. running mean: -20.371226\n",
            "resetting env. episode 489.000000, reward total was -21.000000. running mean: -20.377514\n",
            "resetting env. episode 490.000000, reward total was -21.000000. running mean: -20.383739\n",
            "resetting env. episode 491.000000, reward total was -19.000000. running mean: -20.369901\n",
            "resetting env. episode 492.000000, reward total was -21.000000. running mean: -20.376202\n",
            "resetting env. episode 493.000000, reward total was -21.000000. running mean: -20.382440\n",
            "resetting env. episode 494.000000, reward total was -21.000000. running mean: -20.388616\n",
            "resetting env. episode 495.000000, reward total was -20.000000. running mean: -20.384730\n",
            "resetting env. episode 496.000000, reward total was -21.000000. running mean: -20.390883\n",
            "resetting env. episode 497.000000, reward total was -21.000000. running mean: -20.396974\n",
            "resetting env. episode 498.000000, reward total was -19.000000. running mean: -20.383004\n",
            "resetting env. episode 499.000000, reward total was -21.000000. running mean: -20.389174\n",
            "resetting env. episode 500.000000, reward total was -18.000000. running mean: -20.365282\n",
            "resetting env. episode 501.000000, reward total was -17.000000. running mean: -20.331629\n",
            "resetting env. episode 502.000000, reward total was -21.000000. running mean: -20.338313\n",
            "resetting env. episode 503.000000, reward total was -20.000000. running mean: -20.334930\n",
            "resetting env. episode 504.000000, reward total was -21.000000. running mean: -20.341581\n",
            "resetting env. episode 505.000000, reward total was -21.000000. running mean: -20.348165\n",
            "resetting env. episode 506.000000, reward total was -20.000000. running mean: -20.344683\n",
            "resetting env. episode 507.000000, reward total was -20.000000. running mean: -20.341236\n",
            "resetting env. episode 508.000000, reward total was -20.000000. running mean: -20.337824\n",
            "resetting env. episode 509.000000, reward total was -21.000000. running mean: -20.344446\n",
            "resetting env. episode 510.000000, reward total was -21.000000. running mean: -20.351001\n",
            "resetting env. episode 511.000000, reward total was -21.000000. running mean: -20.357491\n",
            "resetting env. episode 512.000000, reward total was -20.000000. running mean: -20.353916\n",
            "resetting env. episode 513.000000, reward total was -20.000000. running mean: -20.350377\n",
            "resetting env. episode 514.000000, reward total was -20.000000. running mean: -20.346873\n",
            "resetting env. episode 515.000000, reward total was -20.000000. running mean: -20.343405\n",
            "resetting env. episode 516.000000, reward total was -20.000000. running mean: -20.339971\n",
            "resetting env. episode 517.000000, reward total was -21.000000. running mean: -20.346571\n",
            "resetting env. episode 518.000000, reward total was -20.000000. running mean: -20.343105\n",
            "resetting env. episode 519.000000, reward total was -20.000000. running mean: -20.339674\n",
            "resetting env. episode 520.000000, reward total was -20.000000. running mean: -20.336277\n",
            "resetting env. episode 521.000000, reward total was -21.000000. running mean: -20.342915\n",
            "resetting env. episode 522.000000, reward total was -20.000000. running mean: -20.339486\n",
            "resetting env. episode 523.000000, reward total was -21.000000. running mean: -20.346091\n",
            "resetting env. episode 524.000000, reward total was -20.000000. running mean: -20.342630\n",
            "resetting env. episode 525.000000, reward total was -20.000000. running mean: -20.339203\n",
            "resetting env. episode 526.000000, reward total was -21.000000. running mean: -20.345811\n",
            "resetting env. episode 527.000000, reward total was -21.000000. running mean: -20.352353\n",
            "resetting env. episode 528.000000, reward total was -21.000000. running mean: -20.358830\n",
            "resetting env. episode 529.000000, reward total was -20.000000. running mean: -20.355242\n",
            "resetting env. episode 530.000000, reward total was -20.000000. running mean: -20.351689\n",
            "resetting env. episode 531.000000, reward total was -20.000000. running mean: -20.348172\n",
            "resetting env. episode 532.000000, reward total was -19.000000. running mean: -20.334690\n",
            "resetting env. episode 533.000000, reward total was -20.000000. running mean: -20.331344\n",
            "resetting env. episode 534.000000, reward total was -20.000000. running mean: -20.328030\n",
            "resetting env. episode 535.000000, reward total was -20.000000. running mean: -20.324750\n",
            "resetting env. episode 536.000000, reward total was -21.000000. running mean: -20.331502\n",
            "resetting env. episode 537.000000, reward total was -21.000000. running mean: -20.338187\n",
            "resetting env. episode 538.000000, reward total was -21.000000. running mean: -20.344805\n",
            "resetting env. episode 539.000000, reward total was -21.000000. running mean: -20.351357\n",
            "resetting env. episode 540.000000, reward total was -21.000000. running mean: -20.357844\n",
            "resetting env. episode 541.000000, reward total was -21.000000. running mean: -20.364265\n",
            "resetting env. episode 542.000000, reward total was -21.000000. running mean: -20.370623\n",
            "resetting env. episode 543.000000, reward total was -18.000000. running mean: -20.346916\n",
            "resetting env. episode 544.000000, reward total was -19.000000. running mean: -20.333447\n",
            "resetting env. episode 545.000000, reward total was -21.000000. running mean: -20.340113\n",
            "resetting env. episode 546.000000, reward total was -21.000000. running mean: -20.346712\n",
            "resetting env. episode 547.000000, reward total was -20.000000. running mean: -20.343245\n",
            "resetting env. episode 548.000000, reward total was -21.000000. running mean: -20.349812\n",
            "resetting env. episode 549.000000, reward total was -20.000000. running mean: -20.346314\n",
            "resetting env. episode 550.000000, reward total was -20.000000. running mean: -20.342851\n",
            "resetting env. episode 551.000000, reward total was -20.000000. running mean: -20.339422\n",
            "resetting env. episode 552.000000, reward total was -20.000000. running mean: -20.336028\n",
            "resetting env. episode 553.000000, reward total was -20.000000. running mean: -20.332668\n",
            "resetting env. episode 554.000000, reward total was -21.000000. running mean: -20.339341\n",
            "resetting env. episode 555.000000, reward total was -21.000000. running mean: -20.345948\n",
            "resetting env. episode 556.000000, reward total was -20.000000. running mean: -20.342488\n",
            "resetting env. episode 557.000000, reward total was -19.000000. running mean: -20.329063\n",
            "resetting env. episode 558.000000, reward total was -18.000000. running mean: -20.305773\n",
            "resetting env. episode 559.000000, reward total was -19.000000. running mean: -20.292715\n",
            "resetting env. episode 560.000000, reward total was -20.000000. running mean: -20.289788\n",
            "resetting env. episode 561.000000, reward total was -19.000000. running mean: -20.276890\n",
            "resetting env. episode 562.000000, reward total was -21.000000. running mean: -20.284121\n",
            "resetting env. episode 563.000000, reward total was -18.000000. running mean: -20.261280\n",
            "resetting env. episode 564.000000, reward total was -21.000000. running mean: -20.268667\n",
            "resetting env. episode 565.000000, reward total was -21.000000. running mean: -20.275980\n",
            "resetting env. episode 566.000000, reward total was -21.000000. running mean: -20.283221\n",
            "resetting env. episode 567.000000, reward total was -21.000000. running mean: -20.290388\n",
            "resetting env. episode 568.000000, reward total was -20.000000. running mean: -20.287485\n",
            "resetting env. episode 569.000000, reward total was -20.000000. running mean: -20.284610\n",
            "resetting env. episode 570.000000, reward total was -21.000000. running mean: -20.291764\n",
            "resetting env. episode 571.000000, reward total was -19.000000. running mean: -20.278846\n",
            "resetting env. episode 572.000000, reward total was -19.000000. running mean: -20.266058\n",
            "resetting env. episode 573.000000, reward total was -21.000000. running mean: -20.273397\n",
            "resetting env. episode 574.000000, reward total was -21.000000. running mean: -20.280663\n",
            "resetting env. episode 575.000000, reward total was -21.000000. running mean: -20.287856\n",
            "resetting env. episode 576.000000, reward total was -21.000000. running mean: -20.294978\n",
            "resetting env. episode 577.000000, reward total was -20.000000. running mean: -20.292028\n",
            "resetting env. episode 578.000000, reward total was -20.000000. running mean: -20.289108\n",
            "resetting env. episode 579.000000, reward total was -21.000000. running mean: -20.296217\n",
            "resetting env. episode 580.000000, reward total was -20.000000. running mean: -20.293254\n",
            "resetting env. episode 581.000000, reward total was -21.000000. running mean: -20.300322\n",
            "resetting env. episode 582.000000, reward total was -21.000000. running mean: -20.307319\n",
            "resetting env. episode 583.000000, reward total was -20.000000. running mean: -20.304246\n",
            "resetting env. episode 584.000000, reward total was -20.000000. running mean: -20.301203\n",
            "resetting env. episode 585.000000, reward total was -21.000000. running mean: -20.308191\n",
            "resetting env. episode 586.000000, reward total was -21.000000. running mean: -20.315109\n",
            "resetting env. episode 587.000000, reward total was -21.000000. running mean: -20.321958\n",
            "resetting env. episode 588.000000, reward total was -19.000000. running mean: -20.308738\n",
            "resetting env. episode 589.000000, reward total was -20.000000. running mean: -20.305651\n",
            "resetting env. episode 590.000000, reward total was -21.000000. running mean: -20.312595\n",
            "resetting env. episode 591.000000, reward total was -21.000000. running mean: -20.319469\n",
            "resetting env. episode 592.000000, reward total was -21.000000. running mean: -20.326274\n",
            "resetting env. episode 593.000000, reward total was -20.000000. running mean: -20.323011\n",
            "resetting env. episode 594.000000, reward total was -21.000000. running mean: -20.329781\n",
            "resetting env. episode 595.000000, reward total was -21.000000. running mean: -20.336483\n",
            "resetting env. episode 596.000000, reward total was -21.000000. running mean: -20.343118\n",
            "resetting env. episode 597.000000, reward total was -20.000000. running mean: -20.339687\n",
            "resetting env. episode 598.000000, reward total was -20.000000. running mean: -20.336290\n",
            "resetting env. episode 599.000000, reward total was -21.000000. running mean: -20.342927\n",
            "resetting env. episode 600.000000, reward total was -21.000000. running mean: -20.349498\n",
            "resetting env. episode 601.000000, reward total was -20.000000. running mean: -20.346003\n",
            "resetting env. episode 602.000000, reward total was -21.000000. running mean: -20.352543\n",
            "resetting env. episode 603.000000, reward total was -20.000000. running mean: -20.349018\n",
            "resetting env. episode 604.000000, reward total was -21.000000. running mean: -20.355528\n",
            "resetting env. episode 605.000000, reward total was -20.000000. running mean: -20.351972\n",
            "resetting env. episode 606.000000, reward total was -20.000000. running mean: -20.348453\n",
            "resetting env. episode 607.000000, reward total was -21.000000. running mean: -20.354968\n",
            "resetting env. episode 608.000000, reward total was -21.000000. running mean: -20.361418\n",
            "resetting env. episode 609.000000, reward total was -21.000000. running mean: -20.367804\n",
            "resetting env. episode 610.000000, reward total was -21.000000. running mean: -20.374126\n",
            "resetting env. episode 611.000000, reward total was -20.000000. running mean: -20.370385\n",
            "resetting env. episode 612.000000, reward total was -19.000000. running mean: -20.356681\n",
            "resetting env. episode 613.000000, reward total was -21.000000. running mean: -20.363114\n",
            "resetting env. episode 614.000000, reward total was -21.000000. running mean: -20.369483\n",
            "resetting env. episode 615.000000, reward total was -19.000000. running mean: -20.355788\n",
            "resetting env. episode 616.000000, reward total was -19.000000. running mean: -20.342230\n",
            "resetting env. episode 617.000000, reward total was -20.000000. running mean: -20.338808\n",
            "resetting env. episode 618.000000, reward total was -21.000000. running mean: -20.345420\n",
            "resetting env. episode 619.000000, reward total was -20.000000. running mean: -20.341966\n",
            "resetting env. episode 620.000000, reward total was -20.000000. running mean: -20.338546\n",
            "resetting env. episode 621.000000, reward total was -20.000000. running mean: -20.335161\n",
            "resetting env. episode 622.000000, reward total was -21.000000. running mean: -20.341809\n",
            "resetting env. episode 623.000000, reward total was -20.000000. running mean: -20.338391\n",
            "resetting env. episode 624.000000, reward total was -19.000000. running mean: -20.325007\n",
            "resetting env. episode 625.000000, reward total was -21.000000. running mean: -20.331757\n",
            "resetting env. episode 626.000000, reward total was -20.000000. running mean: -20.328439\n",
            "resetting env. episode 627.000000, reward total was -20.000000. running mean: -20.325155\n",
            "resetting env. episode 628.000000, reward total was -21.000000. running mean: -20.331903\n",
            "resetting env. episode 629.000000, reward total was -21.000000. running mean: -20.338584\n",
            "resetting env. episode 630.000000, reward total was -21.000000. running mean: -20.345199\n",
            "resetting env. episode 631.000000, reward total was -18.000000. running mean: -20.321747\n",
            "resetting env. episode 632.000000, reward total was -18.000000. running mean: -20.298529\n",
            "resetting env. episode 633.000000, reward total was -21.000000. running mean: -20.305544\n",
            "resetting env. episode 634.000000, reward total was -20.000000. running mean: -20.302488\n",
            "resetting env. episode 635.000000, reward total was -20.000000. running mean: -20.299464\n",
            "resetting env. episode 636.000000, reward total was -21.000000. running mean: -20.306469\n",
            "resetting env. episode 637.000000, reward total was -21.000000. running mean: -20.313404\n",
            "resetting env. episode 638.000000, reward total was -20.000000. running mean: -20.310270\n",
            "resetting env. episode 639.000000, reward total was -21.000000. running mean: -20.317167\n",
            "resetting env. episode 640.000000, reward total was -20.000000. running mean: -20.313996\n",
            "resetting env. episode 641.000000, reward total was -20.000000. running mean: -20.310856\n",
            "resetting env. episode 642.000000, reward total was -21.000000. running mean: -20.317747\n",
            "resetting env. episode 643.000000, reward total was -21.000000. running mean: -20.324570\n",
            "resetting env. episode 644.000000, reward total was -20.000000. running mean: -20.321324\n",
            "resetting env. episode 645.000000, reward total was -21.000000. running mean: -20.328111\n",
            "resetting env. episode 646.000000, reward total was -21.000000. running mean: -20.334830\n",
            "resetting env. episode 647.000000, reward total was -21.000000. running mean: -20.341481\n",
            "resetting env. episode 648.000000, reward total was -21.000000. running mean: -20.348067\n",
            "resetting env. episode 649.000000, reward total was -21.000000. running mean: -20.354586\n",
            "resetting env. episode 650.000000, reward total was -21.000000. running mean: -20.361040\n",
            "resetting env. episode 651.000000, reward total was -21.000000. running mean: -20.367430\n",
            "resetting env. episode 652.000000, reward total was -21.000000. running mean: -20.373755\n",
            "resetting env. episode 653.000000, reward total was -21.000000. running mean: -20.380018\n",
            "resetting env. episode 654.000000, reward total was -21.000000. running mean: -20.386218\n",
            "resetting env. episode 655.000000, reward total was -20.000000. running mean: -20.382356\n",
            "resetting env. episode 656.000000, reward total was -20.000000. running mean: -20.378532\n",
            "resetting env. episode 657.000000, reward total was -20.000000. running mean: -20.374747\n",
            "resetting env. episode 658.000000, reward total was -19.000000. running mean: -20.360999\n",
            "resetting env. episode 659.000000, reward total was -18.000000. running mean: -20.337389\n",
            "resetting env. episode 660.000000, reward total was -21.000000. running mean: -20.344015\n",
            "resetting env. episode 661.000000, reward total was -21.000000. running mean: -20.350575\n",
            "resetting env. episode 662.000000, reward total was -20.000000. running mean: -20.347069\n",
            "resetting env. episode 663.000000, reward total was -20.000000. running mean: -20.343599\n",
            "resetting env. episode 664.000000, reward total was -21.000000. running mean: -20.350163\n",
            "resetting env. episode 665.000000, reward total was -18.000000. running mean: -20.326661\n",
            "resetting env. episode 666.000000, reward total was -21.000000. running mean: -20.333394\n",
            "resetting env. episode 667.000000, reward total was -20.000000. running mean: -20.330061\n",
            "resetting env. episode 668.000000, reward total was -20.000000. running mean: -20.326760\n",
            "resetting env. episode 669.000000, reward total was -20.000000. running mean: -20.323492\n",
            "resetting env. episode 670.000000, reward total was -21.000000. running mean: -20.330257\n",
            "resetting env. episode 671.000000, reward total was -21.000000. running mean: -20.336955\n",
            "resetting env. episode 672.000000, reward total was -21.000000. running mean: -20.343585\n",
            "resetting env. episode 673.000000, reward total was -21.000000. running mean: -20.350149\n",
            "resetting env. episode 674.000000, reward total was -21.000000. running mean: -20.356648\n",
            "resetting env. episode 675.000000, reward total was -21.000000. running mean: -20.363081\n",
            "resetting env. episode 676.000000, reward total was -21.000000. running mean: -20.369451\n",
            "resetting env. episode 677.000000, reward total was -21.000000. running mean: -20.375756\n",
            "resetting env. episode 678.000000, reward total was -21.000000. running mean: -20.381999\n",
            "resetting env. episode 679.000000, reward total was -21.000000. running mean: -20.388179\n",
            "resetting env. episode 680.000000, reward total was -21.000000. running mean: -20.394297\n",
            "resetting env. episode 681.000000, reward total was -18.000000. running mean: -20.370354\n",
            "resetting env. episode 682.000000, reward total was -20.000000. running mean: -20.366650\n",
            "resetting env. episode 683.000000, reward total was -21.000000. running mean: -20.372984\n",
            "resetting env. episode 684.000000, reward total was -20.000000. running mean: -20.369254\n",
            "resetting env. episode 685.000000, reward total was -19.000000. running mean: -20.355561\n",
            "resetting env. episode 686.000000, reward total was -20.000000. running mean: -20.352006\n",
            "resetting env. episode 687.000000, reward total was -20.000000. running mean: -20.348486\n",
            "resetting env. episode 688.000000, reward total was -20.000000. running mean: -20.345001\n",
            "resetting env. episode 689.000000, reward total was -21.000000. running mean: -20.351551\n",
            "resetting env. episode 690.000000, reward total was -20.000000. running mean: -20.348035\n",
            "resetting env. episode 691.000000, reward total was -21.000000. running mean: -20.354555\n",
            "resetting env. episode 692.000000, reward total was -21.000000. running mean: -20.361009\n",
            "resetting env. episode 693.000000, reward total was -19.000000. running mean: -20.347399\n",
            "resetting env. episode 694.000000, reward total was -20.000000. running mean: -20.343925\n",
            "resetting env. episode 695.000000, reward total was -21.000000. running mean: -20.350486\n",
            "resetting env. episode 696.000000, reward total was -21.000000. running mean: -20.356981\n",
            "resetting env. episode 697.000000, reward total was -21.000000. running mean: -20.363411\n",
            "resetting env. episode 698.000000, reward total was -21.000000. running mean: -20.369777\n",
            "resetting env. episode 699.000000, reward total was -21.000000. running mean: -20.376080\n",
            "resetting env. episode 700.000000, reward total was -20.000000. running mean: -20.372319\n",
            "resetting env. episode 701.000000, reward total was -21.000000. running mean: -20.378596\n",
            "resetting env. episode 702.000000, reward total was -20.000000. running mean: -20.374810\n",
            "resetting env. episode 703.000000, reward total was -21.000000. running mean: -20.381062\n",
            "resetting env. episode 704.000000, reward total was -21.000000. running mean: -20.387251\n",
            "resetting env. episode 705.000000, reward total was -20.000000. running mean: -20.383378\n",
            "resetting env. episode 706.000000, reward total was -21.000000. running mean: -20.389545\n",
            "resetting env. episode 707.000000, reward total was -19.000000. running mean: -20.375649\n",
            "resetting env. episode 708.000000, reward total was -19.000000. running mean: -20.361893\n",
            "resetting env. episode 709.000000, reward total was -21.000000. running mean: -20.368274\n",
            "resetting env. episode 710.000000, reward total was -21.000000. running mean: -20.374591\n",
            "resetting env. episode 711.000000, reward total was -21.000000. running mean: -20.380845\n",
            "resetting env. episode 712.000000, reward total was -20.000000. running mean: -20.377037\n",
            "resetting env. episode 713.000000, reward total was -21.000000. running mean: -20.383266\n",
            "resetting env. episode 714.000000, reward total was -21.000000. running mean: -20.389434\n",
            "resetting env. episode 715.000000, reward total was -21.000000. running mean: -20.395539\n",
            "resetting env. episode 716.000000, reward total was -21.000000. running mean: -20.401584\n",
            "resetting env. episode 717.000000, reward total was -20.000000. running mean: -20.397568\n",
            "resetting env. episode 718.000000, reward total was -20.000000. running mean: -20.393592\n",
            "resetting env. episode 719.000000, reward total was -19.000000. running mean: -20.379656\n",
            "resetting env. episode 720.000000, reward total was -20.000000. running mean: -20.375860\n",
            "resetting env. episode 721.000000, reward total was -21.000000. running mean: -20.382101\n",
            "resetting env. episode 722.000000, reward total was -16.000000. running mean: -20.338280\n",
            "resetting env. episode 723.000000, reward total was -21.000000. running mean: -20.344897\n",
            "resetting env. episode 724.000000, reward total was -21.000000. running mean: -20.351448\n",
            "resetting env. episode 725.000000, reward total was -19.000000. running mean: -20.337934\n",
            "resetting env. episode 726.000000, reward total was -18.000000. running mean: -20.314555\n",
            "resetting env. episode 727.000000, reward total was -20.000000. running mean: -20.311409\n",
            "resetting env. episode 728.000000, reward total was -21.000000. running mean: -20.318295\n",
            "resetting env. episode 729.000000, reward total was -21.000000. running mean: -20.325112\n",
            "resetting env. episode 730.000000, reward total was -19.000000. running mean: -20.311861\n",
            "resetting env. episode 731.000000, reward total was -20.000000. running mean: -20.308742\n",
            "resetting env. episode 732.000000, reward total was -21.000000. running mean: -20.315655\n",
            "resetting env. episode 733.000000, reward total was -20.000000. running mean: -20.312498\n",
            "resetting env. episode 734.000000, reward total was -21.000000. running mean: -20.319373\n",
            "resetting env. episode 735.000000, reward total was -21.000000. running mean: -20.326180\n",
            "resetting env. episode 736.000000, reward total was -20.000000. running mean: -20.322918\n",
            "resetting env. episode 737.000000, reward total was -21.000000. running mean: -20.329689\n",
            "resetting env. episode 738.000000, reward total was -21.000000. running mean: -20.336392\n",
            "resetting env. episode 739.000000, reward total was -21.000000. running mean: -20.343028\n",
            "resetting env. episode 740.000000, reward total was -20.000000. running mean: -20.339598\n",
            "resetting env. episode 741.000000, reward total was -21.000000. running mean: -20.346202\n",
            "resetting env. episode 742.000000, reward total was -21.000000. running mean: -20.352740\n",
            "resetting env. episode 743.000000, reward total was -21.000000. running mean: -20.359212\n",
            "resetting env. episode 744.000000, reward total was -20.000000. running mean: -20.355620\n",
            "resetting env. episode 745.000000, reward total was -19.000000. running mean: -20.342064\n",
            "resetting env. episode 746.000000, reward total was -20.000000. running mean: -20.338643\n",
            "resetting env. episode 747.000000, reward total was -20.000000. running mean: -20.335257\n",
            "resetting env. episode 748.000000, reward total was -18.000000. running mean: -20.311904\n",
            "resetting env. episode 749.000000, reward total was -21.000000. running mean: -20.318785\n",
            "resetting env. episode 750.000000, reward total was -18.000000. running mean: -20.295597\n",
            "resetting env. episode 751.000000, reward total was -21.000000. running mean: -20.302641\n",
            "resetting env. episode 752.000000, reward total was -18.000000. running mean: -20.279615\n",
            "resetting env. episode 753.000000, reward total was -21.000000. running mean: -20.286819\n",
            "resetting env. episode 754.000000, reward total was -21.000000. running mean: -20.293951\n",
            "resetting env. episode 755.000000, reward total was -20.000000. running mean: -20.291011\n",
            "resetting env. episode 756.000000, reward total was -19.000000. running mean: -20.278101\n",
            "resetting env. episode 757.000000, reward total was -21.000000. running mean: -20.285320\n",
            "resetting env. episode 758.000000, reward total was -19.000000. running mean: -20.272467\n",
            "resetting env. episode 759.000000, reward total was -21.000000. running mean: -20.279742\n",
            "resetting env. episode 760.000000, reward total was -21.000000. running mean: -20.286945\n",
            "resetting env. episode 761.000000, reward total was -20.000000. running mean: -20.284075\n",
            "resetting env. episode 762.000000, reward total was -21.000000. running mean: -20.291235\n",
            "resetting env. episode 763.000000, reward total was -19.000000. running mean: -20.278322\n",
            "resetting env. episode 764.000000, reward total was -19.000000. running mean: -20.265539\n",
            "resetting env. episode 765.000000, reward total was -21.000000. running mean: -20.272884\n",
            "resetting env. episode 766.000000, reward total was -20.000000. running mean: -20.270155\n",
            "resetting env. episode 767.000000, reward total was -21.000000. running mean: -20.277453\n",
            "resetting env. episode 768.000000, reward total was -21.000000. running mean: -20.284679\n",
            "resetting env. episode 769.000000, reward total was -21.000000. running mean: -20.291832\n",
            "resetting env. episode 770.000000, reward total was -21.000000. running mean: -20.298914\n",
            "resetting env. episode 771.000000, reward total was -20.000000. running mean: -20.295924\n",
            "resetting env. episode 772.000000, reward total was -20.000000. running mean: -20.292965\n",
            "resetting env. episode 773.000000, reward total was -21.000000. running mean: -20.300036\n",
            "resetting env. episode 774.000000, reward total was -21.000000. running mean: -20.307035\n",
            "resetting env. episode 775.000000, reward total was -20.000000. running mean: -20.303965\n",
            "resetting env. episode 776.000000, reward total was -20.000000. running mean: -20.300925\n",
            "resetting env. episode 777.000000, reward total was -20.000000. running mean: -20.297916\n",
            "resetting env. episode 778.000000, reward total was -20.000000. running mean: -20.294937\n",
            "resetting env. episode 779.000000, reward total was -21.000000. running mean: -20.301987\n",
            "resetting env. episode 780.000000, reward total was -21.000000. running mean: -20.308967\n",
            "resetting env. episode 781.000000, reward total was -21.000000. running mean: -20.315878\n",
            "resetting env. episode 782.000000, reward total was -19.000000. running mean: -20.302719\n",
            "resetting env. episode 783.000000, reward total was -21.000000. running mean: -20.309692\n",
            "resetting env. episode 784.000000, reward total was -20.000000. running mean: -20.306595\n",
            "resetting env. episode 785.000000, reward total was -20.000000. running mean: -20.303529\n",
            "resetting env. episode 786.000000, reward total was -21.000000. running mean: -20.310494\n",
            "resetting env. episode 787.000000, reward total was -21.000000. running mean: -20.317389\n",
            "resetting env. episode 788.000000, reward total was -21.000000. running mean: -20.324215\n",
            "resetting env. episode 789.000000, reward total was -20.000000. running mean: -20.320973\n",
            "resetting env. episode 790.000000, reward total was -20.000000. running mean: -20.317763\n",
            "resetting env. episode 791.000000, reward total was -19.000000. running mean: -20.304585\n",
            "resetting env. episode 792.000000, reward total was -21.000000. running mean: -20.311540\n",
            "resetting env. episode 793.000000, reward total was -21.000000. running mean: -20.318424\n",
            "resetting env. episode 794.000000, reward total was -20.000000. running mean: -20.315240\n",
            "resetting env. episode 795.000000, reward total was -21.000000. running mean: -20.322087\n",
            "resetting env. episode 796.000000, reward total was -21.000000. running mean: -20.328867\n",
            "resetting env. episode 797.000000, reward total was -20.000000. running mean: -20.325578\n",
            "resetting env. episode 798.000000, reward total was -20.000000. running mean: -20.322322\n",
            "resetting env. episode 799.000000, reward total was -21.000000. running mean: -20.329099\n",
            "resetting env. episode 800.000000, reward total was -21.000000. running mean: -20.335808\n",
            "resetting env. episode 801.000000, reward total was -21.000000. running mean: -20.342450\n",
            "resetting env. episode 802.000000, reward total was -21.000000. running mean: -20.349025\n",
            "resetting env. episode 803.000000, reward total was -21.000000. running mean: -20.355535\n",
            "resetting env. episode 804.000000, reward total was -21.000000. running mean: -20.361980\n",
            "resetting env. episode 805.000000, reward total was -21.000000. running mean: -20.368360\n",
            "resetting env. episode 806.000000, reward total was -18.000000. running mean: -20.344676\n",
            "resetting env. episode 807.000000, reward total was -21.000000. running mean: -20.351230\n",
            "resetting env. episode 808.000000, reward total was -20.000000. running mean: -20.347717\n",
            "resetting env. episode 809.000000, reward total was -21.000000. running mean: -20.354240\n",
            "resetting env. episode 810.000000, reward total was -21.000000. running mean: -20.360698\n",
            "resetting env. episode 811.000000, reward total was -21.000000. running mean: -20.367091\n",
            "resetting env. episode 812.000000, reward total was -21.000000. running mean: -20.373420\n",
            "resetting env. episode 813.000000, reward total was -20.000000. running mean: -20.369686\n",
            "resetting env. episode 814.000000, reward total was -21.000000. running mean: -20.375989\n",
            "resetting env. episode 815.000000, reward total was -19.000000. running mean: -20.362229\n",
            "resetting env. episode 816.000000, reward total was -21.000000. running mean: -20.368607\n",
            "resetting env. episode 817.000000, reward total was -21.000000. running mean: -20.374921\n",
            "resetting env. episode 818.000000, reward total was -21.000000. running mean: -20.381171\n",
            "resetting env. episode 819.000000, reward total was -21.000000. running mean: -20.387360\n",
            "resetting env. episode 820.000000, reward total was -21.000000. running mean: -20.393486\n",
            "resetting env. episode 821.000000, reward total was -21.000000. running mean: -20.399551\n",
            "resetting env. episode 822.000000, reward total was -21.000000. running mean: -20.405556\n",
            "resetting env. episode 823.000000, reward total was -21.000000. running mean: -20.411500\n",
            "resetting env. episode 824.000000, reward total was -20.000000. running mean: -20.407385\n",
            "resetting env. episode 825.000000, reward total was -20.000000. running mean: -20.403311\n",
            "resetting env. episode 826.000000, reward total was -21.000000. running mean: -20.409278\n",
            "resetting env. episode 827.000000, reward total was -21.000000. running mean: -20.415185\n",
            "resetting env. episode 828.000000, reward total was -20.000000. running mean: -20.411034\n",
            "resetting env. episode 829.000000, reward total was -21.000000. running mean: -20.416923\n",
            "resetting env. episode 830.000000, reward total was -19.000000. running mean: -20.402754\n",
            "resetting env. episode 831.000000, reward total was -21.000000. running mean: -20.408726\n",
            "resetting env. episode 832.000000, reward total was -21.000000. running mean: -20.414639\n",
            "resetting env. episode 833.000000, reward total was -21.000000. running mean: -20.420493\n",
            "resetting env. episode 834.000000, reward total was -20.000000. running mean: -20.416288\n",
            "resetting env. episode 835.000000, reward total was -21.000000. running mean: -20.422125\n",
            "resetting env. episode 836.000000, reward total was -20.000000. running mean: -20.417904\n",
            "resetting env. episode 837.000000, reward total was -20.000000. running mean: -20.413725\n",
            "resetting env. episode 838.000000, reward total was -19.000000. running mean: -20.399587\n",
            "resetting env. episode 839.000000, reward total was -21.000000. running mean: -20.405592\n",
            "resetting env. episode 840.000000, reward total was -21.000000. running mean: -20.411536\n",
            "resetting env. episode 841.000000, reward total was -21.000000. running mean: -20.417420\n",
            "resetting env. episode 842.000000, reward total was -21.000000. running mean: -20.423246\n",
            "resetting env. episode 843.000000, reward total was -21.000000. running mean: -20.429014\n",
            "resetting env. episode 844.000000, reward total was -20.000000. running mean: -20.424723\n",
            "resetting env. episode 845.000000, reward total was -20.000000. running mean: -20.420476\n",
            "resetting env. episode 846.000000, reward total was -21.000000. running mean: -20.426271\n",
            "resetting env. episode 847.000000, reward total was -21.000000. running mean: -20.432009\n",
            "resetting env. episode 848.000000, reward total was -20.000000. running mean: -20.427689\n",
            "resetting env. episode 849.000000, reward total was -21.000000. running mean: -20.433412\n",
            "resetting env. episode 850.000000, reward total was -21.000000. running mean: -20.439078\n",
            "resetting env. episode 851.000000, reward total was -21.000000. running mean: -20.444687\n",
            "resetting env. episode 852.000000, reward total was -20.000000. running mean: -20.440240\n",
            "resetting env. episode 853.000000, reward total was -21.000000. running mean: -20.445838\n",
            "resetting env. episode 854.000000, reward total was -18.000000. running mean: -20.421379\n",
            "resetting env. episode 855.000000, reward total was -20.000000. running mean: -20.417165\n",
            "resetting env. episode 856.000000, reward total was -20.000000. running mean: -20.412994\n",
            "resetting env. episode 857.000000, reward total was -19.000000. running mean: -20.398864\n",
            "resetting env. episode 858.000000, reward total was -21.000000. running mean: -20.404875\n",
            "resetting env. episode 859.000000, reward total was -20.000000. running mean: -20.400826\n",
            "resetting env. episode 860.000000, reward total was -21.000000. running mean: -20.406818\n",
            "resetting env. episode 861.000000, reward total was -21.000000. running mean: -20.412750\n",
            "resetting env. episode 862.000000, reward total was -20.000000. running mean: -20.408623\n",
            "resetting env. episode 863.000000, reward total was -20.000000. running mean: -20.404536\n",
            "resetting env. episode 864.000000, reward total was -21.000000. running mean: -20.410491\n",
            "resetting env. episode 865.000000, reward total was -21.000000. running mean: -20.416386\n",
            "resetting env. episode 866.000000, reward total was -21.000000. running mean: -20.422222\n",
            "resetting env. episode 867.000000, reward total was -21.000000. running mean: -20.428000\n",
            "resetting env. episode 868.000000, reward total was -21.000000. running mean: -20.433720\n",
            "resetting env. episode 869.000000, reward total was -21.000000. running mean: -20.439383\n",
            "resetting env. episode 870.000000, reward total was -20.000000. running mean: -20.434989\n",
            "resetting env. episode 871.000000, reward total was -21.000000. running mean: -20.440639\n",
            "resetting env. episode 872.000000, reward total was -21.000000. running mean: -20.446233\n",
            "resetting env. episode 873.000000, reward total was -21.000000. running mean: -20.451770\n",
            "resetting env. episode 874.000000, reward total was -20.000000. running mean: -20.447253\n",
            "resetting env. episode 875.000000, reward total was -21.000000. running mean: -20.452780\n",
            "resetting env. episode 876.000000, reward total was -20.000000. running mean: -20.448252\n",
            "resetting env. episode 877.000000, reward total was -21.000000. running mean: -20.453770\n",
            "resetting env. episode 878.000000, reward total was -21.000000. running mean: -20.459232\n",
            "resetting env. episode 879.000000, reward total was -21.000000. running mean: -20.464640\n",
            "resetting env. episode 880.000000, reward total was -21.000000. running mean: -20.469993\n",
            "resetting env. episode 881.000000, reward total was -21.000000. running mean: -20.475293\n",
            "resetting env. episode 882.000000, reward total was -20.000000. running mean: -20.470540\n",
            "resetting env. episode 883.000000, reward total was -21.000000. running mean: -20.475835\n",
            "resetting env. episode 884.000000, reward total was -21.000000. running mean: -20.481077\n",
            "resetting env. episode 885.000000, reward total was -21.000000. running mean: -20.486266\n",
            "resetting env. episode 886.000000, reward total was -19.000000. running mean: -20.471403\n",
            "resetting env. episode 887.000000, reward total was -21.000000. running mean: -20.476689\n",
            "resetting env. episode 888.000000, reward total was -21.000000. running mean: -20.481922\n",
            "resetting env. episode 889.000000, reward total was -21.000000. running mean: -20.487103\n",
            "resetting env. episode 890.000000, reward total was -21.000000. running mean: -20.492232\n",
            "resetting env. episode 891.000000, reward total was -19.000000. running mean: -20.477310\n",
            "resetting env. episode 892.000000, reward total was -20.000000. running mean: -20.472537\n",
            "resetting env. episode 893.000000, reward total was -20.000000. running mean: -20.467811\n",
            "resetting env. episode 894.000000, reward total was -21.000000. running mean: -20.473133\n",
            "resetting env. episode 895.000000, reward total was -20.000000. running mean: -20.468402\n",
            "resetting env. episode 896.000000, reward total was -19.000000. running mean: -20.453718\n",
            "resetting env. episode 897.000000, reward total was -21.000000. running mean: -20.459181\n",
            "resetting env. episode 898.000000, reward total was -21.000000. running mean: -20.464589\n",
            "resetting env. episode 899.000000, reward total was -21.000000. running mean: -20.469943\n",
            "resetting env. episode 900.000000, reward total was -16.000000. running mean: -20.425244\n",
            "resetting env. episode 901.000000, reward total was -21.000000. running mean: -20.430991\n",
            "resetting env. episode 902.000000, reward total was -18.000000. running mean: -20.406681\n",
            "resetting env. episode 903.000000, reward total was -21.000000. running mean: -20.412614\n",
            "resetting env. episode 904.000000, reward total was -20.000000. running mean: -20.408488\n",
            "resetting env. episode 905.000000, reward total was -19.000000. running mean: -20.394403\n",
            "resetting env. episode 906.000000, reward total was -21.000000. running mean: -20.400459\n",
            "resetting env. episode 907.000000, reward total was -21.000000. running mean: -20.406455\n",
            "resetting env. episode 908.000000, reward total was -21.000000. running mean: -20.412390\n",
            "resetting env. episode 909.000000, reward total was -21.000000. running mean: -20.418266\n",
            "resetting env. episode 910.000000, reward total was -20.000000. running mean: -20.414084\n",
            "resetting env. episode 911.000000, reward total was -21.000000. running mean: -20.419943\n",
            "resetting env. episode 912.000000, reward total was -21.000000. running mean: -20.425743\n",
            "resetting env. episode 913.000000, reward total was -20.000000. running mean: -20.421486\n",
            "resetting env. episode 914.000000, reward total was -21.000000. running mean: -20.427271\n",
            "resetting env. episode 915.000000, reward total was -19.000000. running mean: -20.412998\n",
            "resetting env. episode 916.000000, reward total was -21.000000. running mean: -20.418868\n",
            "resetting env. episode 917.000000, reward total was -21.000000. running mean: -20.424680\n",
            "resetting env. episode 918.000000, reward total was -21.000000. running mean: -20.430433\n",
            "resetting env. episode 919.000000, reward total was -20.000000. running mean: -20.426129\n",
            "resetting env. episode 920.000000, reward total was -21.000000. running mean: -20.431867\n",
            "resetting env. episode 921.000000, reward total was -21.000000. running mean: -20.437549\n",
            "resetting env. episode 922.000000, reward total was -21.000000. running mean: -20.443173\n",
            "resetting env. episode 923.000000, reward total was -20.000000. running mean: -20.438741\n",
            "resetting env. episode 924.000000, reward total was -21.000000. running mean: -20.444354\n",
            "resetting env. episode 925.000000, reward total was -20.000000. running mean: -20.439910\n",
            "resetting env. episode 926.000000, reward total was -20.000000. running mean: -20.435511\n",
            "resetting env. episode 927.000000, reward total was -21.000000. running mean: -20.441156\n",
            "resetting env. episode 928.000000, reward total was -20.000000. running mean: -20.436745\n",
            "resetting env. episode 929.000000, reward total was -21.000000. running mean: -20.442377\n",
            "resetting env. episode 930.000000, reward total was -21.000000. running mean: -20.447953\n",
            "resetting env. episode 931.000000, reward total was -20.000000. running mean: -20.443474\n",
            "resetting env. episode 932.000000, reward total was -20.000000. running mean: -20.439039\n",
            "resetting env. episode 933.000000, reward total was -19.000000. running mean: -20.424649\n",
            "resetting env. episode 934.000000, reward total was -21.000000. running mean: -20.430402\n",
            "resetting env. episode 935.000000, reward total was -20.000000. running mean: -20.426098\n",
            "resetting env. episode 936.000000, reward total was -21.000000. running mean: -20.431837\n",
            "resetting env. episode 937.000000, reward total was -20.000000. running mean: -20.427519\n",
            "resetting env. episode 938.000000, reward total was -20.000000. running mean: -20.423244\n",
            "resetting env. episode 939.000000, reward total was -20.000000. running mean: -20.419011\n",
            "resetting env. episode 940.000000, reward total was -20.000000. running mean: -20.414821\n",
            "resetting env. episode 941.000000, reward total was -20.000000. running mean: -20.410673\n",
            "resetting env. episode 942.000000, reward total was -21.000000. running mean: -20.416566\n",
            "resetting env. episode 943.000000, reward total was -19.000000. running mean: -20.402401\n",
            "resetting env. episode 944.000000, reward total was -18.000000. running mean: -20.378377\n",
            "resetting env. episode 945.000000, reward total was -20.000000. running mean: -20.374593\n",
            "resetting env. episode 946.000000, reward total was -21.000000. running mean: -20.380847\n",
            "resetting env. episode 947.000000, reward total was -21.000000. running mean: -20.387038\n",
            "resetting env. episode 948.000000, reward total was -20.000000. running mean: -20.383168\n",
            "resetting env. episode 949.000000, reward total was -21.000000. running mean: -20.389336\n",
            "resetting env. episode 950.000000, reward total was -19.000000. running mean: -20.375443\n",
            "resetting env. episode 951.000000, reward total was -21.000000. running mean: -20.381689\n",
            "resetting env. episode 952.000000, reward total was -20.000000. running mean: -20.377872\n",
            "resetting env. episode 953.000000, reward total was -21.000000. running mean: -20.384093\n",
            "resetting env. episode 954.000000, reward total was -21.000000. running mean: -20.390252\n",
            "resetting env. episode 955.000000, reward total was -21.000000. running mean: -20.396349\n",
            "resetting env. episode 956.000000, reward total was -21.000000. running mean: -20.402386\n",
            "resetting env. episode 957.000000, reward total was -20.000000. running mean: -20.398362\n",
            "resetting env. episode 958.000000, reward total was -21.000000. running mean: -20.404379\n",
            "resetting env. episode 959.000000, reward total was -21.000000. running mean: -20.410335\n",
            "resetting env. episode 960.000000, reward total was -20.000000. running mean: -20.406231\n",
            "resetting env. episode 961.000000, reward total was -20.000000. running mean: -20.402169\n",
            "resetting env. episode 962.000000, reward total was -19.000000. running mean: -20.388147\n",
            "resetting env. episode 963.000000, reward total was -21.000000. running mean: -20.394266\n",
            "resetting env. episode 964.000000, reward total was -20.000000. running mean: -20.390323\n",
            "resetting env. episode 965.000000, reward total was -21.000000. running mean: -20.396420\n",
            "resetting env. episode 966.000000, reward total was -20.000000. running mean: -20.392456\n",
            "resetting env. episode 967.000000, reward total was -19.000000. running mean: -20.378531\n",
            "resetting env. episode 968.000000, reward total was -21.000000. running mean: -20.384746\n",
            "resetting env. episode 969.000000, reward total was -21.000000. running mean: -20.390898\n",
            "resetting env. episode 970.000000, reward total was -18.000000. running mean: -20.366989\n",
            "resetting env. episode 971.000000, reward total was -21.000000. running mean: -20.373320\n",
            "resetting env. episode 972.000000, reward total was -20.000000. running mean: -20.369586\n",
            "resetting env. episode 973.000000, reward total was -18.000000. running mean: -20.345891\n",
            "resetting env. episode 974.000000, reward total was -20.000000. running mean: -20.342432\n",
            "resetting env. episode 975.000000, reward total was -20.000000. running mean: -20.339007\n",
            "resetting env. episode 976.000000, reward total was -21.000000. running mean: -20.345617\n",
            "resetting env. episode 977.000000, reward total was -19.000000. running mean: -20.332161\n",
            "resetting env. episode 978.000000, reward total was -19.000000. running mean: -20.318839\n",
            "resetting env. episode 979.000000, reward total was -21.000000. running mean: -20.325651\n",
            "resetting env. episode 980.000000, reward total was -17.000000. running mean: -20.292395\n",
            "resetting env. episode 981.000000, reward total was -20.000000. running mean: -20.289471\n",
            "resetting env. episode 982.000000, reward total was -21.000000. running mean: -20.296576\n",
            "resetting env. episode 983.000000, reward total was -21.000000. running mean: -20.303610\n",
            "resetting env. episode 984.000000, reward total was -21.000000. running mean: -20.310574\n",
            "resetting env. episode 985.000000, reward total was -20.000000. running mean: -20.307468\n",
            "resetting env. episode 986.000000, reward total was -20.000000. running mean: -20.304394\n",
            "resetting env. episode 987.000000, reward total was -20.000000. running mean: -20.301350\n",
            "resetting env. episode 988.000000, reward total was -20.000000. running mean: -20.298336\n",
            "resetting env. episode 989.000000, reward total was -21.000000. running mean: -20.305353\n",
            "resetting env. episode 990.000000, reward total was -21.000000. running mean: -20.312299\n",
            "resetting env. episode 991.000000, reward total was -20.000000. running mean: -20.309176\n",
            "resetting env. episode 992.000000, reward total was -21.000000. running mean: -20.316085\n",
            "resetting env. episode 993.000000, reward total was -21.000000. running mean: -20.322924\n",
            "resetting env. episode 994.000000, reward total was -21.000000. running mean: -20.329694\n",
            "resetting env. episode 995.000000, reward total was -20.000000. running mean: -20.326398\n",
            "resetting env. episode 996.000000, reward total was -21.000000. running mean: -20.333134\n",
            "resetting env. episode 997.000000, reward total was -21.000000. running mean: -20.339802\n",
            "resetting env. episode 998.000000, reward total was -21.000000. running mean: -20.346404\n",
            "resetting env. episode 999.000000, reward total was -21.000000. running mean: -20.352940\n",
            "resetting env. episode 1000.000000, reward total was -20.000000. running mean: -20.349411\n",
            "resetting env. episode 1001.000000, reward total was -20.000000. running mean: -20.345917\n",
            "resetting env. episode 1002.000000, reward total was -21.000000. running mean: -20.352457\n",
            "resetting env. episode 1003.000000, reward total was -19.000000. running mean: -20.338933\n",
            "resetting env. episode 1004.000000, reward total was -21.000000. running mean: -20.345544\n",
            "resetting env. episode 1005.000000, reward total was -21.000000. running mean: -20.352088\n",
            "resetting env. episode 1006.000000, reward total was -21.000000. running mean: -20.358567\n",
            "resetting env. episode 1007.000000, reward total was -21.000000. running mean: -20.364982\n",
            "resetting env. episode 1008.000000, reward total was -21.000000. running mean: -20.371332\n",
            "resetting env. episode 1009.000000, reward total was -21.000000. running mean: -20.377618\n",
            "resetting env. episode 1010.000000, reward total was -21.000000. running mean: -20.383842\n",
            "resetting env. episode 1011.000000, reward total was -20.000000. running mean: -20.380004\n",
            "resetting env. episode 1012.000000, reward total was -20.000000. running mean: -20.376204\n",
            "resetting env. episode 1013.000000, reward total was -21.000000. running mean: -20.382442\n",
            "resetting env. episode 1014.000000, reward total was -21.000000. running mean: -20.388617\n",
            "resetting env. episode 1015.000000, reward total was -21.000000. running mean: -20.394731\n",
            "resetting env. episode 1016.000000, reward total was -20.000000. running mean: -20.390784\n",
            "resetting env. episode 1017.000000, reward total was -20.000000. running mean: -20.386876\n",
            "resetting env. episode 1018.000000, reward total was -20.000000. running mean: -20.383007\n",
            "resetting env. episode 1019.000000, reward total was -18.000000. running mean: -20.359177\n",
            "resetting env. episode 1020.000000, reward total was -21.000000. running mean: -20.365585\n",
            "resetting env. episode 1021.000000, reward total was -20.000000. running mean: -20.361930\n",
            "resetting env. episode 1022.000000, reward total was -21.000000. running mean: -20.368310\n",
            "resetting env. episode 1023.000000, reward total was -21.000000. running mean: -20.374627\n",
            "resetting env. episode 1024.000000, reward total was -21.000000. running mean: -20.380881\n",
            "resetting env. episode 1025.000000, reward total was -21.000000. running mean: -20.387072\n",
            "resetting env. episode 1026.000000, reward total was -21.000000. running mean: -20.393201\n",
            "resetting env. episode 1027.000000, reward total was -21.000000. running mean: -20.399269\n",
            "resetting env. episode 1028.000000, reward total was -21.000000. running mean: -20.405277\n",
            "resetting env. episode 1029.000000, reward total was -18.000000. running mean: -20.381224\n",
            "resetting env. episode 1030.000000, reward total was -21.000000. running mean: -20.387412\n",
            "resetting env. episode 1031.000000, reward total was -21.000000. running mean: -20.393538\n",
            "resetting env. episode 1032.000000, reward total was -21.000000. running mean: -20.399602\n",
            "resetting env. episode 1033.000000, reward total was -21.000000. running mean: -20.405606\n",
            "resetting env. episode 1034.000000, reward total was -21.000000. running mean: -20.411550\n",
            "resetting env. episode 1035.000000, reward total was -21.000000. running mean: -20.417435\n",
            "resetting env. episode 1036.000000, reward total was -21.000000. running mean: -20.423260\n",
            "resetting env. episode 1037.000000, reward total was -21.000000. running mean: -20.429028\n",
            "resetting env. episode 1038.000000, reward total was -21.000000. running mean: -20.434737\n",
            "resetting env. episode 1039.000000, reward total was -21.000000. running mean: -20.440390\n",
            "resetting env. episode 1040.000000, reward total was -20.000000. running mean: -20.435986\n",
            "resetting env. episode 1041.000000, reward total was -21.000000. running mean: -20.441626\n",
            "resetting env. episode 1042.000000, reward total was -19.000000. running mean: -20.427210\n",
            "resetting env. episode 1043.000000, reward total was -21.000000. running mean: -20.432938\n",
            "resetting env. episode 1044.000000, reward total was -21.000000. running mean: -20.438608\n",
            "resetting env. episode 1045.000000, reward total was -21.000000. running mean: -20.444222\n",
            "resetting env. episode 1046.000000, reward total was -21.000000. running mean: -20.449780\n",
            "resetting env. episode 1047.000000, reward total was -19.000000. running mean: -20.435282\n",
            "resetting env. episode 1048.000000, reward total was -19.000000. running mean: -20.420930\n",
            "resetting env. episode 1049.000000, reward total was -19.000000. running mean: -20.406720\n",
            "resetting env. episode 1050.000000, reward total was -17.000000. running mean: -20.372653\n",
            "resetting env. episode 1051.000000, reward total was -21.000000. running mean: -20.378927\n",
            "resetting env. episode 1052.000000, reward total was -21.000000. running mean: -20.385137\n",
            "resetting env. episode 1053.000000, reward total was -21.000000. running mean: -20.391286\n",
            "resetting env. episode 1054.000000, reward total was -21.000000. running mean: -20.397373\n",
            "resetting env. episode 1055.000000, reward total was -20.000000. running mean: -20.393399\n",
            "resetting env. episode 1056.000000, reward total was -21.000000. running mean: -20.399465\n",
            "resetting env. episode 1057.000000, reward total was -21.000000. running mean: -20.405471\n",
            "resetting env. episode 1058.000000, reward total was -20.000000. running mean: -20.401416\n",
            "resetting env. episode 1059.000000, reward total was -21.000000. running mean: -20.407402\n",
            "resetting env. episode 1060.000000, reward total was -21.000000. running mean: -20.413328\n",
            "resetting env. episode 1061.000000, reward total was -21.000000. running mean: -20.419194\n",
            "resetting env. episode 1062.000000, reward total was -20.000000. running mean: -20.415003\n",
            "resetting env. episode 1063.000000, reward total was -21.000000. running mean: -20.420853\n",
            "resetting env. episode 1064.000000, reward total was -21.000000. running mean: -20.426644\n",
            "resetting env. episode 1065.000000, reward total was -20.000000. running mean: -20.422378\n",
            "resetting env. episode 1066.000000, reward total was -21.000000. running mean: -20.428154\n",
            "resetting env. episode 1067.000000, reward total was -21.000000. running mean: -20.433872\n",
            "resetting env. episode 1068.000000, reward total was -21.000000. running mean: -20.439534\n",
            "resetting env. episode 1069.000000, reward total was -21.000000. running mean: -20.445138\n",
            "resetting env. episode 1070.000000, reward total was -21.000000. running mean: -20.450687\n",
            "resetting env. episode 1071.000000, reward total was -21.000000. running mean: -20.456180\n",
            "resetting env. episode 1072.000000, reward total was -21.000000. running mean: -20.461618\n",
            "resetting env. episode 1073.000000, reward total was -21.000000. running mean: -20.467002\n",
            "resetting env. episode 1074.000000, reward total was -21.000000. running mean: -20.472332\n",
            "resetting env. episode 1075.000000, reward total was -19.000000. running mean: -20.457609\n",
            "resetting env. episode 1076.000000, reward total was -20.000000. running mean: -20.453033\n",
            "resetting env. episode 1077.000000, reward total was -21.000000. running mean: -20.458502\n",
            "resetting env. episode 1078.000000, reward total was -19.000000. running mean: -20.443917\n",
            "resetting env. episode 1079.000000, reward total was -21.000000. running mean: -20.449478\n",
            "resetting env. episode 1080.000000, reward total was -21.000000. running mean: -20.454983\n",
            "resetting env. episode 1081.000000, reward total was -21.000000. running mean: -20.460433\n",
            "resetting env. episode 1082.000000, reward total was -19.000000. running mean: -20.445829\n",
            "resetting env. episode 1083.000000, reward total was -19.000000. running mean: -20.431371\n",
            "resetting env. episode 1084.000000, reward total was -21.000000. running mean: -20.437057\n",
            "resetting env. episode 1085.000000, reward total was -21.000000. running mean: -20.442686\n",
            "resetting env. episode 1086.000000, reward total was -19.000000. running mean: -20.428260\n",
            "resetting env. episode 1087.000000, reward total was -21.000000. running mean: -20.433977\n",
            "resetting env. episode 1088.000000, reward total was -21.000000. running mean: -20.439637\n",
            "resetting env. episode 1089.000000, reward total was -19.000000. running mean: -20.425241\n",
            "resetting env. episode 1090.000000, reward total was -21.000000. running mean: -20.430988\n",
            "resetting env. episode 1091.000000, reward total was -21.000000. running mean: -20.436679\n",
            "resetting env. episode 1092.000000, reward total was -18.000000. running mean: -20.412312\n",
            "resetting env. episode 1093.000000, reward total was -20.000000. running mean: -20.408189\n",
            "resetting env. episode 1094.000000, reward total was -20.000000. running mean: -20.404107\n",
            "resetting env. episode 1095.000000, reward total was -21.000000. running mean: -20.410066\n",
            "resetting env. episode 1096.000000, reward total was -21.000000. running mean: -20.415965\n",
            "resetting env. episode 1097.000000, reward total was -21.000000. running mean: -20.421805\n",
            "resetting env. episode 1098.000000, reward total was -21.000000. running mean: -20.427587\n",
            "resetting env. episode 1099.000000, reward total was -21.000000. running mean: -20.433311\n",
            "resetting env. episode 1100.000000, reward total was -20.000000. running mean: -20.428978\n",
            "resetting env. episode 1101.000000, reward total was -21.000000. running mean: -20.434689\n",
            "resetting env. episode 1102.000000, reward total was -21.000000. running mean: -20.440342\n",
            "resetting env. episode 1103.000000, reward total was -21.000000. running mean: -20.445938\n",
            "resetting env. episode 1104.000000, reward total was -21.000000. running mean: -20.451479\n",
            "resetting env. episode 1105.000000, reward total was -20.000000. running mean: -20.446964\n",
            "resetting env. episode 1106.000000, reward total was -21.000000. running mean: -20.452494\n",
            "resetting env. episode 1107.000000, reward total was -21.000000. running mean: -20.457970\n",
            "resetting env. episode 1108.000000, reward total was -21.000000. running mean: -20.463390\n",
            "resetting env. episode 1109.000000, reward total was -21.000000. running mean: -20.468756\n",
            "resetting env. episode 1110.000000, reward total was -21.000000. running mean: -20.474068\n",
            "resetting env. episode 1111.000000, reward total was -20.000000. running mean: -20.469328\n",
            "resetting env. episode 1112.000000, reward total was -21.000000. running mean: -20.474634\n",
            "resetting env. episode 1113.000000, reward total was -21.000000. running mean: -20.479888\n",
            "resetting env. episode 1114.000000, reward total was -21.000000. running mean: -20.485089\n",
            "resetting env. episode 1115.000000, reward total was -21.000000. running mean: -20.490238\n",
            "resetting env. episode 1116.000000, reward total was -21.000000. running mean: -20.495336\n",
            "resetting env. episode 1117.000000, reward total was -21.000000. running mean: -20.500383\n",
            "resetting env. episode 1118.000000, reward total was -21.000000. running mean: -20.505379\n",
            "resetting env. episode 1119.000000, reward total was -21.000000. running mean: -20.510325\n",
            "resetting env. episode 1120.000000, reward total was -21.000000. running mean: -20.515222\n",
            "resetting env. episode 1121.000000, reward total was -21.000000. running mean: -20.520069\n",
            "resetting env. episode 1122.000000, reward total was -21.000000. running mean: -20.524869\n",
            "resetting env. episode 1123.000000, reward total was -21.000000. running mean: -20.529620\n",
            "resetting env. episode 1124.000000, reward total was -21.000000. running mean: -20.534324\n",
            "resetting env. episode 1125.000000, reward total was -21.000000. running mean: -20.538981\n",
            "resetting env. episode 1126.000000, reward total was -19.000000. running mean: -20.523591\n",
            "resetting env. episode 1127.000000, reward total was -21.000000. running mean: -20.528355\n",
            "resetting env. episode 1128.000000, reward total was -20.000000. running mean: -20.523071\n",
            "resetting env. episode 1129.000000, reward total was -21.000000. running mean: -20.527841\n",
            "resetting env. episode 1130.000000, reward total was -19.000000. running mean: -20.512562\n",
            "resetting env. episode 1131.000000, reward total was -21.000000. running mean: -20.517437\n",
            "resetting env. episode 1132.000000, reward total was -21.000000. running mean: -20.522262\n",
            "resetting env. episode 1133.000000, reward total was -21.000000. running mean: -20.527040\n",
            "resetting env. episode 1134.000000, reward total was -20.000000. running mean: -20.521769\n",
            "resetting env. episode 1135.000000, reward total was -20.000000. running mean: -20.516552\n",
            "resetting env. episode 1136.000000, reward total was -20.000000. running mean: -20.511386\n",
            "resetting env. episode 1137.000000, reward total was -18.000000. running mean: -20.486272\n",
            "resetting env. episode 1138.000000, reward total was -19.000000. running mean: -20.471409\n",
            "resetting env. episode 1139.000000, reward total was -21.000000. running mean: -20.476695\n",
            "resetting env. episode 1140.000000, reward total was -21.000000. running mean: -20.481928\n",
            "resetting env. episode 1141.000000, reward total was -20.000000. running mean: -20.477109\n",
            "resetting env. episode 1142.000000, reward total was -21.000000. running mean: -20.482338\n",
            "resetting env. episode 1143.000000, reward total was -21.000000. running mean: -20.487515\n",
            "resetting env. episode 1144.000000, reward total was -21.000000. running mean: -20.492640\n",
            "resetting env. episode 1145.000000, reward total was -21.000000. running mean: -20.497713\n",
            "resetting env. episode 1146.000000, reward total was -21.000000. running mean: -20.502736\n",
            "resetting env. episode 1147.000000, reward total was -17.000000. running mean: -20.467709\n",
            "resetting env. episode 1148.000000, reward total was -21.000000. running mean: -20.473032\n",
            "resetting env. episode 1149.000000, reward total was -21.000000. running mean: -20.478301\n",
            "resetting env. episode 1150.000000, reward total was -21.000000. running mean: -20.483518\n",
            "resetting env. episode 1151.000000, reward total was -21.000000. running mean: -20.488683\n",
            "resetting env. episode 1152.000000, reward total was -20.000000. running mean: -20.483796\n",
            "resetting env. episode 1153.000000, reward total was -20.000000. running mean: -20.478958\n",
            "resetting env. episode 1154.000000, reward total was -21.000000. running mean: -20.484169\n",
            "resetting env. episode 1155.000000, reward total was -21.000000. running mean: -20.489327\n",
            "resetting env. episode 1156.000000, reward total was -21.000000. running mean: -20.494434\n",
            "resetting env. episode 1157.000000, reward total was -20.000000. running mean: -20.489489\n",
            "resetting env. episode 1158.000000, reward total was -21.000000. running mean: -20.494594\n",
            "resetting env. episode 1159.000000, reward total was -19.000000. running mean: -20.479649\n",
            "resetting env. episode 1160.000000, reward total was -20.000000. running mean: -20.474852\n",
            "resetting env. episode 1161.000000, reward total was -20.000000. running mean: -20.470104\n",
            "resetting env. episode 1162.000000, reward total was -20.000000. running mean: -20.465402\n",
            "resetting env. episode 1163.000000, reward total was -21.000000. running mean: -20.470748\n",
            "resetting env. episode 1164.000000, reward total was -19.000000. running mean: -20.456041\n",
            "resetting env. episode 1165.000000, reward total was -20.000000. running mean: -20.451481\n",
            "resetting env. episode 1166.000000, reward total was -21.000000. running mean: -20.456966\n",
            "resetting env. episode 1167.000000, reward total was -21.000000. running mean: -20.462396\n",
            "resetting env. episode 1168.000000, reward total was -19.000000. running mean: -20.447772\n",
            "resetting env. episode 1169.000000, reward total was -21.000000. running mean: -20.453294\n",
            "resetting env. episode 1170.000000, reward total was -21.000000. running mean: -20.458761\n",
            "resetting env. episode 1171.000000, reward total was -18.000000. running mean: -20.434174\n",
            "resetting env. episode 1172.000000, reward total was -21.000000. running mean: -20.439832\n",
            "resetting env. episode 1173.000000, reward total was -21.000000. running mean: -20.445434\n",
            "resetting env. episode 1174.000000, reward total was -21.000000. running mean: -20.450979\n",
            "resetting env. episode 1175.000000, reward total was -21.000000. running mean: -20.456470\n",
            "resetting env. episode 1176.000000, reward total was -21.000000. running mean: -20.461905\n",
            "resetting env. episode 1177.000000, reward total was -20.000000. running mean: -20.457286\n",
            "resetting env. episode 1178.000000, reward total was -21.000000. running mean: -20.462713\n",
            "resetting env. episode 1179.000000, reward total was -20.000000. running mean: -20.458086\n",
            "resetting env. episode 1180.000000, reward total was -21.000000. running mean: -20.463505\n",
            "resetting env. episode 1181.000000, reward total was -19.000000. running mean: -20.448870\n",
            "resetting env. episode 1182.000000, reward total was -20.000000. running mean: -20.444381\n",
            "resetting env. episode 1183.000000, reward total was -21.000000. running mean: -20.449938\n",
            "resetting env. episode 1184.000000, reward total was -21.000000. running mean: -20.455438\n",
            "resetting env. episode 1185.000000, reward total was -20.000000. running mean: -20.450884\n",
            "resetting env. episode 1186.000000, reward total was -21.000000. running mean: -20.456375\n",
            "resetting env. episode 1187.000000, reward total was -21.000000. running mean: -20.461811\n",
            "resetting env. episode 1188.000000, reward total was -21.000000. running mean: -20.467193\n",
            "resetting env. episode 1189.000000, reward total was -21.000000. running mean: -20.472521\n",
            "resetting env. episode 1190.000000, reward total was -21.000000. running mean: -20.477796\n",
            "resetting env. episode 1191.000000, reward total was -20.000000. running mean: -20.473018\n",
            "resetting env. episode 1192.000000, reward total was -21.000000. running mean: -20.478288\n",
            "resetting env. episode 1193.000000, reward total was -21.000000. running mean: -20.483505\n",
            "resetting env. episode 1194.000000, reward total was -20.000000. running mean: -20.478670\n",
            "resetting env. episode 1195.000000, reward total was -21.000000. running mean: -20.483883\n",
            "resetting env. episode 1196.000000, reward total was -21.000000. running mean: -20.489044\n",
            "resetting env. episode 1197.000000, reward total was -21.000000. running mean: -20.494154\n",
            "resetting env. episode 1198.000000, reward total was -21.000000. running mean: -20.499212\n",
            "resetting env. episode 1199.000000, reward total was -21.000000. running mean: -20.504220\n",
            "resetting env. episode 1200.000000, reward total was -21.000000. running mean: -20.509178\n",
            "resetting env. episode 1201.000000, reward total was -19.000000. running mean: -20.494086\n",
            "resetting env. episode 1202.000000, reward total was -21.000000. running mean: -20.499145\n",
            "resetting env. episode 1203.000000, reward total was -21.000000. running mean: -20.504154\n",
            "resetting env. episode 1204.000000, reward total was -21.000000. running mean: -20.509112\n",
            "resetting env. episode 1205.000000, reward total was -21.000000. running mean: -20.514021\n",
            "resetting env. episode 1206.000000, reward total was -21.000000. running mean: -20.518881\n",
            "resetting env. episode 1207.000000, reward total was -21.000000. running mean: -20.523692\n",
            "resetting env. episode 1208.000000, reward total was -21.000000. running mean: -20.528455\n",
            "resetting env. episode 1209.000000, reward total was -21.000000. running mean: -20.533171\n",
            "resetting env. episode 1210.000000, reward total was -21.000000. running mean: -20.537839\n",
            "resetting env. episode 1211.000000, reward total was -21.000000. running mean: -20.542461\n",
            "resetting env. episode 1212.000000, reward total was -21.000000. running mean: -20.547036\n",
            "resetting env. episode 1213.000000, reward total was -21.000000. running mean: -20.551566\n",
            "resetting env. episode 1214.000000, reward total was -20.000000. running mean: -20.546050\n",
            "resetting env. episode 1215.000000, reward total was -19.000000. running mean: -20.530590\n",
            "resetting env. episode 1216.000000, reward total was -19.000000. running mean: -20.515284\n",
            "resetting env. episode 1217.000000, reward total was -20.000000. running mean: -20.510131\n",
            "resetting env. episode 1218.000000, reward total was -21.000000. running mean: -20.515029\n",
            "resetting env. episode 1219.000000, reward total was -21.000000. running mean: -20.519879\n",
            "resetting env. episode 1220.000000, reward total was -21.000000. running mean: -20.524680\n",
            "resetting env. episode 1221.000000, reward total was -20.000000. running mean: -20.519434\n",
            "resetting env. episode 1222.000000, reward total was -21.000000. running mean: -20.524239\n",
            "resetting env. episode 1223.000000, reward total was -21.000000. running mean: -20.528997\n",
            "resetting env. episode 1224.000000, reward total was -20.000000. running mean: -20.523707\n",
            "resetting env. episode 1225.000000, reward total was -21.000000. running mean: -20.528470\n",
            "resetting env. episode 1226.000000, reward total was -21.000000. running mean: -20.533185\n",
            "resetting env. episode 1227.000000, reward total was -21.000000. running mean: -20.537853\n",
            "resetting env. episode 1228.000000, reward total was -19.000000. running mean: -20.522475\n",
            "resetting env. episode 1229.000000, reward total was -19.000000. running mean: -20.507250\n",
            "resetting env. episode 1230.000000, reward total was -20.000000. running mean: -20.502178\n",
            "resetting env. episode 1231.000000, reward total was -21.000000. running mean: -20.507156\n",
            "resetting env. episode 1232.000000, reward total was -18.000000. running mean: -20.482084\n",
            "resetting env. episode 1233.000000, reward total was -20.000000. running mean: -20.477263\n",
            "resetting env. episode 1234.000000, reward total was -20.000000. running mean: -20.472491\n",
            "resetting env. episode 1235.000000, reward total was -21.000000. running mean: -20.477766\n",
            "resetting env. episode 1236.000000, reward total was -21.000000. running mean: -20.482988\n",
            "resetting env. episode 1237.000000, reward total was -20.000000. running mean: -20.478158\n",
            "resetting env. episode 1238.000000, reward total was -21.000000. running mean: -20.483377\n",
            "resetting env. episode 1239.000000, reward total was -20.000000. running mean: -20.478543\n",
            "resetting env. episode 1240.000000, reward total was -21.000000. running mean: -20.483757\n",
            "resetting env. episode 1241.000000, reward total was -21.000000. running mean: -20.488920\n",
            "resetting env. episode 1242.000000, reward total was -17.000000. running mean: -20.454031\n",
            "resetting env. episode 1243.000000, reward total was -20.000000. running mean: -20.449490\n",
            "resetting env. episode 1244.000000, reward total was -20.000000. running mean: -20.444995\n",
            "resetting env. episode 1245.000000, reward total was -21.000000. running mean: -20.450546\n",
            "resetting env. episode 1246.000000, reward total was -21.000000. running mean: -20.456040\n",
            "resetting env. episode 1247.000000, reward total was -21.000000. running mean: -20.461480\n",
            "resetting env. episode 1248.000000, reward total was -19.000000. running mean: -20.446865\n",
            "resetting env. episode 1249.000000, reward total was -21.000000. running mean: -20.452396\n",
            "resetting env. episode 1250.000000, reward total was -19.000000. running mean: -20.437872\n",
            "resetting env. episode 1251.000000, reward total was -20.000000. running mean: -20.433494\n",
            "resetting env. episode 1252.000000, reward total was -21.000000. running mean: -20.439159\n",
            "resetting env. episode 1253.000000, reward total was -20.000000. running mean: -20.434767\n",
            "resetting env. episode 1254.000000, reward total was -21.000000. running mean: -20.440419\n",
            "resetting env. episode 1255.000000, reward total was -19.000000. running mean: -20.426015\n",
            "resetting env. episode 1256.000000, reward total was -21.000000. running mean: -20.431755\n",
            "resetting env. episode 1257.000000, reward total was -20.000000. running mean: -20.427437\n",
            "resetting env. episode 1258.000000, reward total was -21.000000. running mean: -20.433163\n",
            "resetting env. episode 1259.000000, reward total was -20.000000. running mean: -20.428831\n",
            "resetting env. episode 1260.000000, reward total was -21.000000. running mean: -20.434543\n",
            "resetting env. episode 1261.000000, reward total was -21.000000. running mean: -20.440198\n",
            "resetting env. episode 1262.000000, reward total was -20.000000. running mean: -20.435796\n",
            "resetting env. episode 1263.000000, reward total was -21.000000. running mean: -20.441438\n",
            "resetting env. episode 1264.000000, reward total was -20.000000. running mean: -20.437023\n",
            "resetting env. episode 1265.000000, reward total was -21.000000. running mean: -20.442653\n",
            "resetting env. episode 1266.000000, reward total was -21.000000. running mean: -20.448227\n",
            "resetting env. episode 1267.000000, reward total was -16.000000. running mean: -20.403744\n",
            "resetting env. episode 1268.000000, reward total was -20.000000. running mean: -20.399707\n",
            "resetting env. episode 1269.000000, reward total was -21.000000. running mean: -20.405710\n",
            "resetting env. episode 1270.000000, reward total was -20.000000. running mean: -20.401653\n",
            "resetting env. episode 1271.000000, reward total was -20.000000. running mean: -20.397636\n",
            "resetting env. episode 1272.000000, reward total was -21.000000. running mean: -20.403660\n",
            "resetting env. episode 1273.000000, reward total was -21.000000. running mean: -20.409623\n",
            "resetting env. episode 1274.000000, reward total was -21.000000. running mean: -20.415527\n",
            "resetting env. episode 1275.000000, reward total was -21.000000. running mean: -20.421372\n",
            "resetting env. episode 1276.000000, reward total was -20.000000. running mean: -20.417158\n",
            "resetting env. episode 1277.000000, reward total was -20.000000. running mean: -20.412986\n",
            "resetting env. episode 1278.000000, reward total was -21.000000. running mean: -20.418857\n",
            "resetting env. episode 1279.000000, reward total was -20.000000. running mean: -20.414668\n",
            "resetting env. episode 1280.000000, reward total was -20.000000. running mean: -20.410521\n",
            "resetting env. episode 1281.000000, reward total was -20.000000. running mean: -20.406416\n",
            "resetting env. episode 1282.000000, reward total was -21.000000. running mean: -20.412352\n",
            "resetting env. episode 1283.000000, reward total was -21.000000. running mean: -20.418228\n",
            "resetting env. episode 1284.000000, reward total was -19.000000. running mean: -20.404046\n",
            "resetting env. episode 1285.000000, reward total was -20.000000. running mean: -20.400006\n",
            "resetting env. episode 1286.000000, reward total was -20.000000. running mean: -20.396006\n",
            "resetting env. episode 1287.000000, reward total was -21.000000. running mean: -20.402046\n",
            "resetting env. episode 1288.000000, reward total was -19.000000. running mean: -20.388025\n",
            "resetting env. episode 1289.000000, reward total was -20.000000. running mean: -20.384145\n",
            "resetting env. episode 1290.000000, reward total was -20.000000. running mean: -20.380303\n",
            "resetting env. episode 1291.000000, reward total was -21.000000. running mean: -20.386500\n",
            "resetting env. episode 1292.000000, reward total was -21.000000. running mean: -20.392635\n",
            "resetting env. episode 1293.000000, reward total was -21.000000. running mean: -20.398709\n",
            "resetting env. episode 1294.000000, reward total was -21.000000. running mean: -20.404722\n",
            "resetting env. episode 1295.000000, reward total was -21.000000. running mean: -20.410675\n",
            "resetting env. episode 1296.000000, reward total was -21.000000. running mean: -20.416568\n",
            "resetting env. episode 1297.000000, reward total was -21.000000. running mean: -20.422402\n",
            "resetting env. episode 1298.000000, reward total was -21.000000. running mean: -20.428178\n",
            "resetting env. episode 1299.000000, reward total was -19.000000. running mean: -20.413897\n",
            "resetting env. episode 1300.000000, reward total was -20.000000. running mean: -20.409758\n",
            "resetting env. episode 1301.000000, reward total was -21.000000. running mean: -20.415660\n",
            "resetting env. episode 1302.000000, reward total was -21.000000. running mean: -20.421503\n",
            "resetting env. episode 1303.000000, reward total was -21.000000. running mean: -20.427288\n",
            "resetting env. episode 1304.000000, reward total was -21.000000. running mean: -20.433015\n",
            "resetting env. episode 1305.000000, reward total was -21.000000. running mean: -20.438685\n",
            "resetting env. episode 1306.000000, reward total was -21.000000. running mean: -20.444298\n",
            "resetting env. episode 1307.000000, reward total was -21.000000. running mean: -20.449855\n",
            "resetting env. episode 1308.000000, reward total was -19.000000. running mean: -20.435357\n",
            "resetting env. episode 1309.000000, reward total was -21.000000. running mean: -20.441003\n",
            "resetting env. episode 1310.000000, reward total was -21.000000. running mean: -20.446593\n",
            "resetting env. episode 1311.000000, reward total was -21.000000. running mean: -20.452127\n",
            "resetting env. episode 1312.000000, reward total was -21.000000. running mean: -20.457606\n",
            "resetting env. episode 1313.000000, reward total was -21.000000. running mean: -20.463030\n",
            "resetting env. episode 1314.000000, reward total was -21.000000. running mean: -20.468400\n",
            "resetting env. episode 1315.000000, reward total was -21.000000. running mean: -20.473716\n",
            "resetting env. episode 1316.000000, reward total was -21.000000. running mean: -20.478979\n",
            "resetting env. episode 1317.000000, reward total was -21.000000. running mean: -20.484189\n",
            "resetting env. episode 1318.000000, reward total was -20.000000. running mean: -20.479347\n",
            "resetting env. episode 1319.000000, reward total was -20.000000. running mean: -20.474553\n",
            "resetting env. episode 1320.000000, reward total was -21.000000. running mean: -20.479808\n",
            "resetting env. episode 1321.000000, reward total was -19.000000. running mean: -20.465010\n",
            "resetting env. episode 1322.000000, reward total was -21.000000. running mean: -20.470360\n",
            "resetting env. episode 1323.000000, reward total was -20.000000. running mean: -20.465656\n",
            "resetting env. episode 1324.000000, reward total was -20.000000. running mean: -20.461000\n",
            "resetting env. episode 1325.000000, reward total was -21.000000. running mean: -20.466390\n",
            "resetting env. episode 1326.000000, reward total was -21.000000. running mean: -20.471726\n",
            "resetting env. episode 1327.000000, reward total was -21.000000. running mean: -20.477008\n",
            "resetting env. episode 1328.000000, reward total was -21.000000. running mean: -20.482238\n",
            "resetting env. episode 1329.000000, reward total was -21.000000. running mean: -20.487416\n",
            "resetting env. episode 1330.000000, reward total was -21.000000. running mean: -20.492542\n",
            "resetting env. episode 1331.000000, reward total was -21.000000. running mean: -20.497616\n",
            "resetting env. episode 1332.000000, reward total was -20.000000. running mean: -20.492640\n",
            "resetting env. episode 1333.000000, reward total was -21.000000. running mean: -20.497714\n",
            "resetting env. episode 1334.000000, reward total was -21.000000. running mean: -20.502737\n",
            "resetting env. episode 1335.000000, reward total was -21.000000. running mean: -20.507709\n",
            "resetting env. episode 1336.000000, reward total was -20.000000. running mean: -20.502632\n",
            "resetting env. episode 1337.000000, reward total was -21.000000. running mean: -20.507606\n",
            "resetting env. episode 1338.000000, reward total was -21.000000. running mean: -20.512530\n",
            "resetting env. episode 1339.000000, reward total was -20.000000. running mean: -20.507405\n",
            "resetting env. episode 1340.000000, reward total was -21.000000. running mean: -20.512330\n",
            "resetting env. episode 1341.000000, reward total was -21.000000. running mean: -20.517207\n",
            "resetting env. episode 1342.000000, reward total was -18.000000. running mean: -20.492035\n",
            "resetting env. episode 1343.000000, reward total was -20.000000. running mean: -20.487115\n",
            "resetting env. episode 1344.000000, reward total was -20.000000. running mean: -20.482244\n",
            "resetting env. episode 1345.000000, reward total was -21.000000. running mean: -20.487421\n",
            "resetting env. episode 1346.000000, reward total was -19.000000. running mean: -20.472547\n",
            "resetting env. episode 1347.000000, reward total was -21.000000. running mean: -20.477821\n",
            "resetting env. episode 1348.000000, reward total was -20.000000. running mean: -20.473043\n",
            "resetting env. episode 1349.000000, reward total was -21.000000. running mean: -20.478313\n",
            "resetting env. episode 1350.000000, reward total was -21.000000. running mean: -20.483530\n",
            "resetting env. episode 1351.000000, reward total was -21.000000. running mean: -20.488694\n",
            "resetting env. episode 1352.000000, reward total was -21.000000. running mean: -20.493807\n",
            "resetting env. episode 1353.000000, reward total was -20.000000. running mean: -20.488869\n",
            "resetting env. episode 1354.000000, reward total was -20.000000. running mean: -20.483981\n",
            "resetting env. episode 1355.000000, reward total was -19.000000. running mean: -20.469141\n",
            "resetting env. episode 1356.000000, reward total was -21.000000. running mean: -20.474449\n",
            "resetting env. episode 1357.000000, reward total was -21.000000. running mean: -20.479705\n",
            "resetting env. episode 1358.000000, reward total was -20.000000. running mean: -20.474908\n",
            "resetting env. episode 1359.000000, reward total was -21.000000. running mean: -20.480159\n",
            "resetting env. episode 1360.000000, reward total was -21.000000. running mean: -20.485357\n",
            "resetting env. episode 1361.000000, reward total was -20.000000. running mean: -20.480504\n",
            "resetting env. episode 1362.000000, reward total was -20.000000. running mean: -20.475699\n",
            "resetting env. episode 1363.000000, reward total was -20.000000. running mean: -20.470942\n",
            "resetting env. episode 1364.000000, reward total was -21.000000. running mean: -20.476232\n",
            "resetting env. episode 1365.000000, reward total was -20.000000. running mean: -20.471470\n",
            "resetting env. episode 1366.000000, reward total was -21.000000. running mean: -20.476755\n",
            "resetting env. episode 1367.000000, reward total was -21.000000. running mean: -20.481988\n",
            "resetting env. episode 1368.000000, reward total was -20.000000. running mean: -20.477168\n",
            "resetting env. episode 1369.000000, reward total was -20.000000. running mean: -20.472396\n",
            "resetting env. episode 1370.000000, reward total was -19.000000. running mean: -20.457672\n",
            "resetting env. episode 1371.000000, reward total was -18.000000. running mean: -20.433095\n",
            "resetting env. episode 1372.000000, reward total was -20.000000. running mean: -20.428764\n",
            "resetting env. episode 1373.000000, reward total was -20.000000. running mean: -20.424477\n",
            "resetting env. episode 1374.000000, reward total was -21.000000. running mean: -20.430232\n",
            "resetting env. episode 1375.000000, reward total was -21.000000. running mean: -20.435930\n",
            "resetting env. episode 1376.000000, reward total was -21.000000. running mean: -20.441570\n",
            "resetting env. episode 1377.000000, reward total was -20.000000. running mean: -20.437155\n",
            "resetting env. episode 1378.000000, reward total was -20.000000. running mean: -20.432783\n",
            "resetting env. episode 1379.000000, reward total was -21.000000. running mean: -20.438455\n",
            "resetting env. episode 1380.000000, reward total was -20.000000. running mean: -20.434071\n",
            "resetting env. episode 1381.000000, reward total was -21.000000. running mean: -20.439730\n",
            "resetting env. episode 1382.000000, reward total was -21.000000. running mean: -20.445333\n",
            "resetting env. episode 1383.000000, reward total was -21.000000. running mean: -20.450879\n",
            "resetting env. episode 1384.000000, reward total was -21.000000. running mean: -20.456371\n",
            "resetting env. episode 1385.000000, reward total was -21.000000. running mean: -20.461807\n",
            "resetting env. episode 1386.000000, reward total was -21.000000. running mean: -20.467189\n",
            "resetting env. episode 1387.000000, reward total was -20.000000. running mean: -20.462517\n",
            "resetting env. episode 1388.000000, reward total was -21.000000. running mean: -20.467892\n",
            "resetting env. episode 1389.000000, reward total was -21.000000. running mean: -20.473213\n",
            "resetting env. episode 1390.000000, reward total was -21.000000. running mean: -20.478481\n",
            "resetting env. episode 1391.000000, reward total was -21.000000. running mean: -20.483696\n",
            "resetting env. episode 1392.000000, reward total was -21.000000. running mean: -20.488859\n",
            "resetting env. episode 1393.000000, reward total was -21.000000. running mean: -20.493970\n",
            "resetting env. episode 1394.000000, reward total was -21.000000. running mean: -20.499031\n",
            "resetting env. episode 1395.000000, reward total was -21.000000. running mean: -20.504040\n",
            "resetting env. episode 1396.000000, reward total was -21.000000. running mean: -20.509000\n",
            "resetting env. episode 1397.000000, reward total was -19.000000. running mean: -20.493910\n",
            "resetting env. episode 1398.000000, reward total was -20.000000. running mean: -20.488971\n",
            "resetting env. episode 1399.000000, reward total was -21.000000. running mean: -20.494081\n",
            "resetting env. episode 1400.000000, reward total was -21.000000. running mean: -20.499140\n",
            "resetting env. episode 1401.000000, reward total was -20.000000. running mean: -20.494149\n",
            "resetting env. episode 1402.000000, reward total was -20.000000. running mean: -20.489208\n",
            "resetting env. episode 1403.000000, reward total was -19.000000. running mean: -20.474315\n",
            "resetting env. episode 1404.000000, reward total was -21.000000. running mean: -20.479572\n",
            "resetting env. episode 1405.000000, reward total was -21.000000. running mean: -20.484777\n",
            "resetting env. episode 1406.000000, reward total was -20.000000. running mean: -20.479929\n",
            "resetting env. episode 1407.000000, reward total was -20.000000. running mean: -20.475130\n",
            "resetting env. episode 1408.000000, reward total was -20.000000. running mean: -20.470378\n",
            "resetting env. episode 1409.000000, reward total was -21.000000. running mean: -20.475674\n",
            "resetting env. episode 1410.000000, reward total was -21.000000. running mean: -20.480918\n",
            "resetting env. episode 1411.000000, reward total was -20.000000. running mean: -20.476109\n",
            "resetting env. episode 1412.000000, reward total was -21.000000. running mean: -20.481347\n",
            "resetting env. episode 1413.000000, reward total was -21.000000. running mean: -20.486534\n",
            "resetting env. episode 1414.000000, reward total was -21.000000. running mean: -20.491669\n",
            "resetting env. episode 1415.000000, reward total was -21.000000. running mean: -20.496752\n",
            "resetting env. episode 1416.000000, reward total was -21.000000. running mean: -20.501784\n",
            "resetting env. episode 1417.000000, reward total was -20.000000. running mean: -20.496767\n",
            "resetting env. episode 1418.000000, reward total was -19.000000. running mean: -20.481799\n",
            "resetting env. episode 1419.000000, reward total was -19.000000. running mean: -20.466981\n",
            "resetting env. episode 1420.000000, reward total was -17.000000. running mean: -20.432311\n",
            "resetting env. episode 1421.000000, reward total was -21.000000. running mean: -20.437988\n",
            "resetting env. episode 1422.000000, reward total was -21.000000. running mean: -20.443608\n",
            "resetting env. episode 1423.000000, reward total was -21.000000. running mean: -20.449172\n",
            "resetting env. episode 1424.000000, reward total was -21.000000. running mean: -20.454680\n",
            "resetting env. episode 1425.000000, reward total was -21.000000. running mean: -20.460134\n",
            "resetting env. episode 1426.000000, reward total was -21.000000. running mean: -20.465532\n",
            "resetting env. episode 1427.000000, reward total was -21.000000. running mean: -20.470877\n",
            "resetting env. episode 1428.000000, reward total was -21.000000. running mean: -20.476168\n",
            "resetting env. episode 1429.000000, reward total was -21.000000. running mean: -20.481406\n",
            "resetting env. episode 1430.000000, reward total was -21.000000. running mean: -20.486592\n",
            "resetting env. episode 1431.000000, reward total was -21.000000. running mean: -20.491726\n",
            "resetting env. episode 1432.000000, reward total was -20.000000. running mean: -20.486809\n",
            "resetting env. episode 1433.000000, reward total was -21.000000. running mean: -20.491941\n",
            "resetting env. episode 1434.000000, reward total was -21.000000. running mean: -20.497022\n",
            "resetting env. episode 1435.000000, reward total was -21.000000. running mean: -20.502051\n",
            "resetting env. episode 1436.000000, reward total was -21.000000. running mean: -20.507031\n",
            "resetting env. episode 1437.000000, reward total was -20.000000. running mean: -20.501961\n",
            "resetting env. episode 1438.000000, reward total was -21.000000. running mean: -20.506941\n",
            "resetting env. episode 1439.000000, reward total was -20.000000. running mean: -20.501872\n",
            "resetting env. episode 1440.000000, reward total was -20.000000. running mean: -20.496853\n",
            "resetting env. episode 1441.000000, reward total was -21.000000. running mean: -20.501884\n",
            "resetting env. episode 1442.000000, reward total was -21.000000. running mean: -20.506866\n",
            "resetting env. episode 1443.000000, reward total was -19.000000. running mean: -20.491797\n",
            "resetting env. episode 1444.000000, reward total was -20.000000. running mean: -20.486879\n",
            "resetting env. episode 1445.000000, reward total was -20.000000. running mean: -20.482010\n",
            "resetting env. episode 1446.000000, reward total was -21.000000. running mean: -20.487190\n",
            "resetting env. episode 1447.000000, reward total was -19.000000. running mean: -20.472318\n",
            "resetting env. episode 1448.000000, reward total was -21.000000. running mean: -20.477595\n",
            "resetting env. episode 1449.000000, reward total was -20.000000. running mean: -20.472819\n",
            "resetting env. episode 1450.000000, reward total was -20.000000. running mean: -20.468091\n",
            "resetting env. episode 1451.000000, reward total was -21.000000. running mean: -20.473410\n",
            "resetting env. episode 1452.000000, reward total was -21.000000. running mean: -20.478676\n",
            "resetting env. episode 1453.000000, reward total was -20.000000. running mean: -20.473889\n",
            "resetting env. episode 1454.000000, reward total was -21.000000. running mean: -20.479150\n",
            "resetting env. episode 1455.000000, reward total was -20.000000. running mean: -20.474359\n",
            "resetting env. episode 1456.000000, reward total was -21.000000. running mean: -20.479615\n",
            "resetting env. episode 1457.000000, reward total was -21.000000. running mean: -20.484819\n",
            "resetting env. episode 1458.000000, reward total was -18.000000. running mean: -20.459971\n",
            "resetting env. episode 1459.000000, reward total was -20.000000. running mean: -20.455371\n",
            "resetting env. episode 1460.000000, reward total was -21.000000. running mean: -20.460817\n",
            "resetting env. episode 1461.000000, reward total was -21.000000. running mean: -20.466209\n",
            "resetting env. episode 1462.000000, reward total was -20.000000. running mean: -20.461547\n",
            "resetting env. episode 1463.000000, reward total was -21.000000. running mean: -20.466932\n",
            "resetting env. episode 1464.000000, reward total was -21.000000. running mean: -20.472262\n",
            "resetting env. episode 1465.000000, reward total was -20.000000. running mean: -20.467540\n",
            "resetting env. episode 1466.000000, reward total was -20.000000. running mean: -20.462864\n",
            "resetting env. episode 1467.000000, reward total was -21.000000. running mean: -20.468236\n",
            "resetting env. episode 1468.000000, reward total was -20.000000. running mean: -20.463553\n",
            "resetting env. episode 1469.000000, reward total was -20.000000. running mean: -20.458918\n",
            "resetting env. episode 1470.000000, reward total was -21.000000. running mean: -20.464328\n",
            "resetting env. episode 1471.000000, reward total was -19.000000. running mean: -20.449685\n",
            "resetting env. episode 1472.000000, reward total was -21.000000. running mean: -20.455188\n",
            "resetting env. episode 1473.000000, reward total was -21.000000. running mean: -20.460636\n",
            "resetting env. episode 1474.000000, reward total was -19.000000. running mean: -20.446030\n",
            "resetting env. episode 1475.000000, reward total was -21.000000. running mean: -20.451570\n",
            "resetting env. episode 1476.000000, reward total was -21.000000. running mean: -20.457054\n",
            "resetting env. episode 1477.000000, reward total was -19.000000. running mean: -20.442484\n",
            "resetting env. episode 1478.000000, reward total was -19.000000. running mean: -20.428059\n",
            "resetting env. episode 1479.000000, reward total was -21.000000. running mean: -20.433778\n",
            "resetting env. episode 1480.000000, reward total was -21.000000. running mean: -20.439440\n",
            "resetting env. episode 1481.000000, reward total was -21.000000. running mean: -20.445046\n",
            "resetting env. episode 1482.000000, reward total was -21.000000. running mean: -20.450596\n",
            "resetting env. episode 1483.000000, reward total was -21.000000. running mean: -20.456090\n",
            "resetting env. episode 1484.000000, reward total was -21.000000. running mean: -20.461529\n",
            "resetting env. episode 1485.000000, reward total was -18.000000. running mean: -20.436913\n",
            "resetting env. episode 1486.000000, reward total was -21.000000. running mean: -20.442544\n",
            "resetting env. episode 1487.000000, reward total was -21.000000. running mean: -20.448119\n",
            "resetting env. episode 1488.000000, reward total was -20.000000. running mean: -20.443638\n",
            "resetting env. episode 1489.000000, reward total was -20.000000. running mean: -20.439201\n",
            "resetting env. episode 1490.000000, reward total was -21.000000. running mean: -20.444809\n",
            "resetting env. episode 1491.000000, reward total was -21.000000. running mean: -20.450361\n",
            "resetting env. episode 1492.000000, reward total was -21.000000. running mean: -20.455858\n",
            "resetting env. episode 1493.000000, reward total was -19.000000. running mean: -20.441299\n",
            "resetting env. episode 1494.000000, reward total was -21.000000. running mean: -20.446886\n",
            "resetting env. episode 1495.000000, reward total was -21.000000. running mean: -20.452417\n",
            "resetting env. episode 1496.000000, reward total was -17.000000. running mean: -20.417893\n",
            "resetting env. episode 1497.000000, reward total was -19.000000. running mean: -20.403714\n",
            "resetting env. episode 1498.000000, reward total was -21.000000. running mean: -20.409677\n",
            "resetting env. episode 1499.000000, reward total was -19.000000. running mean: -20.395580\n",
            "resetting env. episode 1500.000000, reward total was -21.000000. running mean: -20.401624\n",
            "CPU times: user 1h 8min 32s, sys: 30min 52s, total: 1h 39min 24s\n",
            "Wall time: 51min 11s\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w2NblmwDsL3y",
        "outputId": "dd7deee1-85ad-41b7-ceb8-f35824d1b44b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "play_game(env, model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  \"The argument mode in render method is deprecated; \"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished without success, accumulated reward = -16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 320x420 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}